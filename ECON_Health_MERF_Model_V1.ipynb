{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cfa03fe-eae9-48ca-a4b6-1fffa7c77805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liuc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\impute\\_base.py:598: UserWarning: Skipping features without any observed values: ['State']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "      <th>abs_coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>econ_hire_rate</td>\n",
       "      <td>12.445247</td>\n",
       "      <td>12.445247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>econ_emp_per_1k</td>\n",
       "      <td>10.715410</td>\n",
       "      <td>10.715410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STATE_% Children in Poverty</td>\n",
       "      <td>-0.513073</td>\n",
       "      <td>0.513073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>state_avg_earnings</td>\n",
       "      <td>-0.236653</td>\n",
       "      <td>0.236653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>STATE_% Low Birthweight</td>\n",
       "      <td>-0.208205</td>\n",
       "      <td>0.208205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>STATE_Food Environment Index</td>\n",
       "      <td>0.164597</td>\n",
       "      <td>0.164597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>state_hires_total</td>\n",
       "      <td>0.159182</td>\n",
       "      <td>0.159182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>growth_emp_qoq</td>\n",
       "      <td>0.153683</td>\n",
       "      <td>0.153683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>growth_emp_yoy</td>\n",
       "      <td>0.121825</td>\n",
       "      <td>0.121825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>STATE_Primary Care Physicians Rate</td>\n",
       "      <td>-0.099586</td>\n",
       "      <td>0.099586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>STATE_Social Association Rate</td>\n",
       "      <td>-0.092232</td>\n",
       "      <td>0.092232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>growth_earn_qoq</td>\n",
       "      <td>-0.058701</td>\n",
       "      <td>0.058701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>STATE_% Smokers</td>\n",
       "      <td>0.040334</td>\n",
       "      <td>0.040334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>STATE_Preventable Hospitalization Rate</td>\n",
       "      <td>-0.028303</td>\n",
       "      <td>0.028303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>STATE_Mentally Unhealthy Days</td>\n",
       "      <td>-0.016890</td>\n",
       "      <td>0.016890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>STATE_Income Ratio</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>STATE_Physically Unhealthy Days</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>STATE_Violent Crime Rate</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>STATE_Teen Birth Rate</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>STATE_% Uninsured</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>quarter_1</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>quarter_2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>quarter_3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>STATE_% With Access to Exercise Opportunities</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Year</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>STATE_% Unemployed</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>STATE_% Some College</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>STATE_% Severe Housing Problems</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>state_total_pop</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>STATE_% Long Commute - Drives Alone</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>STATE_% Fair or Poor Health</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>STATE_% Excessive Drinking</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>STATE_% Drive Alone to Work</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>STATE_% Children in Single-Parent Households</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>STATE_% Adults with Obesity</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>STATE_Premature Deaths</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>STATE_# Driving Deaths</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>STATE_# Alcohol-Impaired Driving Deaths</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>state_emp_total</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>quarter_4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          feature  coefficient  \\\n",
       "0                                  econ_hire_rate    12.445247   \n",
       "1                                 econ_emp_per_1k    10.715410   \n",
       "2                     STATE_% Children in Poverty    -0.513073   \n",
       "3                              state_avg_earnings    -0.236653   \n",
       "4                         STATE_% Low Birthweight    -0.208205   \n",
       "5                    STATE_Food Environment Index     0.164597   \n",
       "6                               state_hires_total     0.159182   \n",
       "7                                  growth_emp_qoq     0.153683   \n",
       "8                                  growth_emp_yoy     0.121825   \n",
       "9              STATE_Primary Care Physicians Rate    -0.099586   \n",
       "10                  STATE_Social Association Rate    -0.092232   \n",
       "11                                growth_earn_qoq    -0.058701   \n",
       "12                                STATE_% Smokers     0.040334   \n",
       "13         STATE_Preventable Hospitalization Rate    -0.028303   \n",
       "14                  STATE_Mentally Unhealthy Days    -0.016890   \n",
       "15                             STATE_Income Ratio    -0.000000   \n",
       "16                STATE_Physically Unhealthy Days    -0.000000   \n",
       "17                       STATE_Violent Crime Rate     0.000000   \n",
       "18                          STATE_Teen Birth Rate    -0.000000   \n",
       "19                              STATE_% Uninsured    -0.000000   \n",
       "20                                      quarter_1    -0.000000   \n",
       "21                                      quarter_2     0.000000   \n",
       "22                                      quarter_3     0.000000   \n",
       "23  STATE_% With Access to Exercise Opportunities    -0.000000   \n",
       "24                                           Year    -0.000000   \n",
       "25                             STATE_% Unemployed     0.000000   \n",
       "26                           STATE_% Some College    -0.000000   \n",
       "27                STATE_% Severe Housing Problems    -0.000000   \n",
       "28                                state_total_pop     0.000000   \n",
       "29            STATE_% Long Commute - Drives Alone    -0.000000   \n",
       "30                    STATE_% Fair or Poor Health    -0.000000   \n",
       "31                     STATE_% Excessive Drinking     0.000000   \n",
       "32                    STATE_% Drive Alone to Work     0.000000   \n",
       "33   STATE_% Children in Single-Parent Households    -0.000000   \n",
       "34                    STATE_% Adults with Obesity     0.000000   \n",
       "35                         STATE_Premature Deaths     0.000000   \n",
       "36                         STATE_# Driving Deaths     0.000000   \n",
       "37        STATE_# Alcohol-Impaired Driving Deaths     0.000000   \n",
       "38                                state_emp_total     0.000000   \n",
       "39                                      quarter_4     0.000000   \n",
       "\n",
       "    abs_coefficient  \n",
       "0         12.445247  \n",
       "1         10.715410  \n",
       "2          0.513073  \n",
       "3          0.236653  \n",
       "4          0.208205  \n",
       "5          0.164597  \n",
       "6          0.159182  \n",
       "7          0.153683  \n",
       "8          0.121825  \n",
       "9          0.099586  \n",
       "10         0.092232  \n",
       "11         0.058701  \n",
       "12         0.040334  \n",
       "13         0.028303  \n",
       "14         0.016890  \n",
       "15         0.000000  \n",
       "16         0.000000  \n",
       "17         0.000000  \n",
       "18         0.000000  \n",
       "19         0.000000  \n",
       "20         0.000000  \n",
       "21         0.000000  \n",
       "22         0.000000  \n",
       "23         0.000000  \n",
       "24         0.000000  \n",
       "25         0.000000  \n",
       "26         0.000000  \n",
       "27         0.000000  \n",
       "28         0.000000  \n",
       "29         0.000000  \n",
       "30         0.000000  \n",
       "31         0.000000  \n",
       "32         0.000000  \n",
       "33         0.000000  \n",
       "34         0.000000  \n",
       "35         0.000000  \n",
       "36         0.000000  \n",
       "37         0.000000  \n",
       "38         0.000000  \n",
       "39         0.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# -----------------------------\n",
    "# Load data\n",
    "# -----------------------------\n",
    "df = pd.read_csv(r\"C:\\Users\\liuc\\Downloads\\CH_ECON_V3.csv\")\n",
    "\n",
    "TARGET = \"econ_hires_per_1k\"\n",
    "y = pd.to_numeric(df[TARGET], errors=\"coerce\")\n",
    "\n",
    "mask = y.notna()\n",
    "df = df.loc[mask].reset_index(drop=True)\n",
    "y = y.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Time index (for TimeSeriesSplit)\n",
    "# -----------------------------\n",
    "if \"year\" in df.columns and \"quarter\" in df.columns:\n",
    "    q = df[\"quarter\"].astype(str).str.replace(\"Q\", \"\", regex=False)\n",
    "    df[\"_time_index\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\") * 4 + pd.to_numeric(q, errors=\"coerce\")\n",
    "else:\n",
    "    df[\"_time_index\"] = np.arange(len(df))\n",
    "\n",
    "df = df.sort_values(\"_time_index\").reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Feature selection\n",
    "# -----------------------------\n",
    "drop_cols = {TARGET, \"_time_index\"}\n",
    "categorical_cols = [c for c in [\"state\", \"county\", \"industry\", \"year\", \"quarter\"] if c in df.columns]\n",
    "numeric_cols = [c for c in df.columns if c not in drop_cols and c not in categorical_cols]\n",
    "\n",
    "for c in numeric_cols:\n",
    "    if df[c].dtype == \"object\":\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "X = df[numeric_cols + categorical_cols]\n",
    "\n",
    "# -----------------------------\n",
    "# Preprocessing + LASSO\n",
    "# -----------------------------\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), numeric_cols),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "        ]), categorical_cols)\n",
    "    ],\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"lasso\", LassoCV(\n",
    "        cv=TimeSeriesSplit(n_splits=5),\n",
    "        n_alphas=200,\n",
    "        max_iter=30000,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "# -----------------------------\n",
    "# Coefficient table (FINAL OUTPUT)\n",
    "# -----------------------------\n",
    "feature_names = model.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "coefs = model.named_steps[\"lasso\"].coef_\n",
    "\n",
    "coef_table = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"coefficient\": coefs\n",
    "    })\n",
    "    .assign(abs_coefficient=lambda d: d[\"coefficient\"].abs())\n",
    "    .sort_values(\"abs_coefficient\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "coef_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb5d77f3-5c12-44ac-9bf3-1de9fb0fe351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived # of state groups: 50\n",
      "First 10 group sizes:\n",
      " state_group\n",
      "0    44\n",
      "1    10\n",
      "2    44\n",
      "3    44\n",
      "4    44\n",
      "5    44\n",
      "6    44\n",
      "7    44\n",
      "8    44\n",
      "9    44\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     [merf.py:307] Training GLL is 6598.840007552962 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is 6543.059052563239 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is 6504.205413058526 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is 6476.706636652364 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is 6444.936594394965 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is 6430.6331927418105 at iteration 6.\n",
      "INFO     [merf.py:307] Training GLL is 6421.123105953389 at iteration 7.\n",
      "INFO     [merf.py:307] Training GLL is 6413.839452454632 at iteration 8.\n",
      "INFO     [merf.py:307] Training GLL is 6409.208651099883 at iteration 9.\n",
      "INFO     [merf.py:307] Training GLL is 6406.990171419416 at iteration 10.\n",
      "INFO     [merf.py:307] Training GLL is 6407.040857963105 at iteration 11.\n",
      "INFO     [merf.py:307] Training GLL is 6404.581430865687 at iteration 12.\n",
      "INFO     [merf.py:307] Training GLL is 6402.447439740721 at iteration 13.\n",
      "INFO     [merf.py:307] Training GLL is 6403.783230913285 at iteration 14.\n",
      "INFO     [merf.py:307] Training GLL is 6403.799705296823 at iteration 15.\n",
      "INFO     [merf.py:307] Training GLL is 6405.877775111201 at iteration 16.\n",
      "INFO     [merf.py:307] Training GLL is 6407.4163971490525 at iteration 17.\n",
      "INFO     [merf.py:307] Training GLL is 6408.294273918267 at iteration 18.\n",
      "INFO     [merf.py:307] Training GLL is 6403.976712552256 at iteration 19.\n",
      "INFO     [merf.py:307] Training GLL is 6403.079042556638 at iteration 20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FEATURES USED:\n",
      "['econ_emp_per_1k', 'state_avg_earnings', 'growth_emp_qoq', 'growth_emp_yoy', 'growth_earn_qoq', 'STATE_% Children in Poverty', 'STATE_% Low Birthweight', 'STATE_Food Environment Index', 'STATE_Primary Care Physicians Rate', 'STATE_Social Association Rate', 'STATE_% Smokers', 'STATE_Preventable Hospitalization Rate', 'STATE_Mentally Unhealthy Days']\n",
      "\n",
      "TRAIN:\n",
      "  R2  : 0.9320257712997496\n",
      "  RMSE: 3.690130444741052\n",
      "\n",
      "TEST:\n",
      "  R2  : 0.2546166989054003\n",
      "  RMSE: 11.641365630400133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from merf.merf import MERF\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load\n",
    "# -----------------------------\n",
    "\n",
    "TARGET = \"econ_hires_per_1k\"\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Ensure numeric types for Year/quarter/target/features\n",
    "# -----------------------------\n",
    "df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\")\n",
    "df[\"quarter\"] = pd.to_numeric(df[\"quarter\"], errors=\"coerce\")\n",
    "df[TARGET] = pd.to_numeric(df[TARGET], errors=\"coerce\")\n",
    "\n",
    "# LASSO-selected features (exclude leakage)\n",
    "X_cols = [\n",
    "    \"econ_emp_per_1k\",\n",
    "    \"state_avg_earnings\",\n",
    "    \"growth_emp_qoq\",\n",
    "    \"growth_emp_yoy\",\n",
    "    \"growth_earn_qoq\",\n",
    "    \"STATE_% Children in Poverty\",\n",
    "    \"STATE_% Low Birthweight\",\n",
    "    \"STATE_Food Environment Index\",\n",
    "    \"STATE_Primary Care Physicians Rate\",\n",
    "    \"STATE_Social Association Rate\",\n",
    "    \"STATE_% Smokers\",\n",
    "    \"STATE_Preventable Hospitalization Rate\",\n",
    "    \"STATE_Mentally Unhealthy Days\",\n",
    "]\n",
    "\n",
    "LEAKY = {\"econ_hire_rate\", \"state_hires_total\"}\n",
    "X_cols = [c for c in X_cols if c in df.columns and c not in LEAKY]\n",
    "\n",
    "for c in X_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Keep rows with essential time + target\n",
    "df = df.dropna(subset=[\"Year\", \"quarter\", TARGET]).copy()\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Reconstruct state grouping (state blocks)\n",
    "#    Assumption (matches your sheet): each state starts at minYear+minQuarter.\n",
    "# -----------------------------\n",
    "min_year = int(df[\"Year\"].min())\n",
    "min_q = int(df[\"quarter\"].min())\n",
    "\n",
    "# IMPORTANT: this uses the existing row order (state blocks are contiguous in your file)\n",
    "start_of_state = (df[\"Year\"] == min_year) & (df[\"quarter\"] == min_q)\n",
    "\n",
    "# Force the first row to start a group even if marker fails\n",
    "start_of_state.iloc[0] = True\n",
    "\n",
    "# state_group = 0,1,2,... per block\n",
    "df[\"state_group\"] = start_of_state.cumsum() - 1\n",
    "\n",
    "print(\"Derived # of state groups:\", df[\"state_group\"].nunique())\n",
    "print(\"First 10 group sizes:\\n\", df[\"state_group\"].value_counts().sort_index().head(10))\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Create time index for splitting\n",
    "# -----------------------------\n",
    "df[\"time_index\"] = df[\"Year\"] * 4 + df[\"quarter\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Build modeling frame\n",
    "#    (Do NOT drop rows with missing X; let SimpleImputer handle it.)\n",
    "# -----------------------------\n",
    "df_model = df[[TARGET, \"state_group\", \"time_index\"] + X_cols].copy()\n",
    "df_model = df_model.dropna(subset=[TARGET, \"state_group\", \"time_index\"]).reset_index(drop=True)\n",
    "\n",
    "# MERF inputs\n",
    "y = df_model[TARGET].values\n",
    "clusters = df_model[\"state_group\"].astype(str)  # MUST be pandas Series\n",
    "Z = np.ones((len(df_model), 1))                 # random intercept only\n",
    "\n",
    "X_raw = df_model[X_cols].values\n",
    "\n",
    "# Preprocess X\n",
    "x_pre = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "X = x_pre.fit_transform(X_raw)\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Time-based split (last 20% of time_index)\n",
    "# -----------------------------\n",
    "df_model = df_model.sort_values(\"time_index\").reset_index(drop=True)\n",
    "\n",
    "split = int(len(df_model) * 0.8)\n",
    "tr = np.arange(0, split)\n",
    "te = np.arange(split, len(df_model))\n",
    "\n",
    "X_tr, X_te = X[tr], X[te]\n",
    "y_tr, y_te = y[tr], y[te]\n",
    "Z_tr, Z_te = Z[tr], Z[te]\n",
    "cl_tr, cl_te = clusters.iloc[tr], clusters.iloc[te]\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Fit MERF\n",
    "# -----------------------------\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=600,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "merf = MERF(rf)\n",
    "merf.fit(X_tr, Z_tr, cl_tr, y_tr)\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Evaluate\n",
    "# -----------------------------\n",
    "pred_tr = merf.predict(X_tr, Z_tr, cl_tr)\n",
    "pred_te = merf.predict(X_te, Z_te, cl_te)\n",
    "\n",
    "print(\"\\nFEATURES USED:\")\n",
    "print(X_cols)\n",
    "\n",
    "print(\"\\nTRAIN:\")\n",
    "print(\"  R2  :\", r2_score(y_tr, pred_tr))\n",
    "print(\"  RMSE:\", mean_squared_error(y_tr, pred_tr, squared=False))\n",
    "\n",
    "print(\"\\nTEST:\")\n",
    "print(\"  R2  :\", r2_score(y_te, pred_te))\n",
    "print(\"  RMSE:\", mean_squared_error(y_te, pred_te, squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7314103",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\liuc\\Downloads\\CH_ECON_V3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022d4951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>state_total_pop</th>\n",
       "      <th>state_emp_total</th>\n",
       "      <th>state_hires_total</th>\n",
       "      <th>state_avg_earnings</th>\n",
       "      <th>econ_emp_per_1k</th>\n",
       "      <th>econ_hires_per_1k</th>\n",
       "      <th>econ_hire_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>STATE_% With Access to Exercise Opportunities</th>\n",
       "      <th>STATE_Food Environment Index</th>\n",
       "      <th>STATE_Income Ratio</th>\n",
       "      <th>STATE_Mentally Unhealthy Days</th>\n",
       "      <th>STATE_Physically Unhealthy Days</th>\n",
       "      <th>STATE_Preventable Hospitalization Rate</th>\n",
       "      <th>STATE_Primary Care Physicians Rate</th>\n",
       "      <th>STATE_Social Association Rate</th>\n",
       "      <th>STATE_Teen Birth Rate</th>\n",
       "      <th>STATE_Violent Crime Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>4843737.0</td>\n",
       "      <td>1484428.0</td>\n",
       "      <td>234866.0</td>\n",
       "      <td>3183.492990</td>\n",
       "      <td>306.463377</td>\n",
       "      <td>48.488595</td>\n",
       "      <td>0.158220</td>\n",
       "      <td>...</td>\n",
       "      <td>51.935239</td>\n",
       "      <td>6.932836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.254047</td>\n",
       "      <td>4.289236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.132991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.186843</td>\n",
       "      <td>410.745302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>4843737.0</td>\n",
       "      <td>1488384.0</td>\n",
       "      <td>296168.0</td>\n",
       "      <td>3186.477239</td>\n",
       "      <td>307.280102</td>\n",
       "      <td>61.144525</td>\n",
       "      <td>0.198986</td>\n",
       "      <td>...</td>\n",
       "      <td>51.935239</td>\n",
       "      <td>6.932836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.254047</td>\n",
       "      <td>4.289236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.132991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.186843</td>\n",
       "      <td>410.745302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>4843737.0</td>\n",
       "      <td>1510987.0</td>\n",
       "      <td>292385.0</td>\n",
       "      <td>3158.744593</td>\n",
       "      <td>311.946540</td>\n",
       "      <td>60.363517</td>\n",
       "      <td>0.193506</td>\n",
       "      <td>...</td>\n",
       "      <td>51.935239</td>\n",
       "      <td>6.932836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.254047</td>\n",
       "      <td>4.289236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.132991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.186843</td>\n",
       "      <td>410.745302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>4843737.0</td>\n",
       "      <td>1509475.0</td>\n",
       "      <td>273242.0</td>\n",
       "      <td>3383.957254</td>\n",
       "      <td>311.634385</td>\n",
       "      <td>56.411403</td>\n",
       "      <td>0.181018</td>\n",
       "      <td>...</td>\n",
       "      <td>51.935239</td>\n",
       "      <td>6.932836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.254047</td>\n",
       "      <td>4.289236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.132991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.186843</td>\n",
       "      <td>410.745302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>4854803.0</td>\n",
       "      <td>1510048.0</td>\n",
       "      <td>246062.0</td>\n",
       "      <td>3224.297608</td>\n",
       "      <td>311.042075</td>\n",
       "      <td>50.684240</td>\n",
       "      <td>0.162950</td>\n",
       "      <td>...</td>\n",
       "      <td>64.355654</td>\n",
       "      <td>6.694086</td>\n",
       "      <td>5.030240</td>\n",
       "      <td>4.252503</td>\n",
       "      <td>4.285823</td>\n",
       "      <td>69.088952</td>\n",
       "      <td>62.883135</td>\n",
       "      <td>12.483551</td>\n",
       "      <td>46.968524</td>\n",
       "      <td>410.093125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>578239.0</td>\n",
       "      <td>215710.0</td>\n",
       "      <td>45344.0</td>\n",
       "      <td>4717.370878</td>\n",
       "      <td>373.046439</td>\n",
       "      <td>78.417402</td>\n",
       "      <td>0.210208</td>\n",
       "      <td>...</td>\n",
       "      <td>77.638870</td>\n",
       "      <td>7.779140</td>\n",
       "      <td>4.235807</td>\n",
       "      <td>4.019662</td>\n",
       "      <td>2.849604</td>\n",
       "      <td>2324.557970</td>\n",
       "      <td>70.486996</td>\n",
       "      <td>11.973542</td>\n",
       "      <td>24.269712</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>580752.0</td>\n",
       "      <td>208388.0</td>\n",
       "      <td>36781.0</td>\n",
       "      <td>4665.328800</td>\n",
       "      <td>358.824421</td>\n",
       "      <td>63.333402</td>\n",
       "      <td>0.176502</td>\n",
       "      <td>...</td>\n",
       "      <td>77.761949</td>\n",
       "      <td>7.781328</td>\n",
       "      <td>4.259097</td>\n",
       "      <td>4.750646</td>\n",
       "      <td>3.437344</td>\n",
       "      <td>2178.144266</td>\n",
       "      <td>70.319097</td>\n",
       "      <td>12.253024</td>\n",
       "      <td>20.177949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>580752.0</td>\n",
       "      <td>208453.0</td>\n",
       "      <td>59642.0</td>\n",
       "      <td>4530.285609</td>\n",
       "      <td>358.936345</td>\n",
       "      <td>102.697881</td>\n",
       "      <td>0.286117</td>\n",
       "      <td>...</td>\n",
       "      <td>77.761949</td>\n",
       "      <td>7.781328</td>\n",
       "      <td>4.259097</td>\n",
       "      <td>4.750646</td>\n",
       "      <td>3.437344</td>\n",
       "      <td>2178.144266</td>\n",
       "      <td>70.319097</td>\n",
       "      <td>12.253024</td>\n",
       "      <td>20.177949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>580752.0</td>\n",
       "      <td>222565.0</td>\n",
       "      <td>51118.0</td>\n",
       "      <td>4574.083715</td>\n",
       "      <td>383.235873</td>\n",
       "      <td>88.020360</td>\n",
       "      <td>0.229677</td>\n",
       "      <td>...</td>\n",
       "      <td>77.761949</td>\n",
       "      <td>7.781328</td>\n",
       "      <td>4.259097</td>\n",
       "      <td>4.750646</td>\n",
       "      <td>3.437344</td>\n",
       "      <td>2178.144266</td>\n",
       "      <td>70.319097</td>\n",
       "      <td>12.253024</td>\n",
       "      <td>20.177949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2024</td>\n",
       "      <td>4</td>\n",
       "      <td>580752.0</td>\n",
       "      <td>217840.0</td>\n",
       "      <td>41963.0</td>\n",
       "      <td>4891.206367</td>\n",
       "      <td>375.099871</td>\n",
       "      <td>72.256316</td>\n",
       "      <td>0.192632</td>\n",
       "      <td>...</td>\n",
       "      <td>77.761949</td>\n",
       "      <td>7.781328</td>\n",
       "      <td>4.259097</td>\n",
       "      <td>4.750646</td>\n",
       "      <td>3.437344</td>\n",
       "      <td>2178.144266</td>\n",
       "      <td>70.319097</td>\n",
       "      <td>12.253024</td>\n",
       "      <td>20.177949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2151 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        State  Year  quarter  state_total_pop  state_emp_total  \\\n",
       "0     Alabama  2014        1        4843737.0        1484428.0   \n",
       "1     Alabama  2014        2        4843737.0        1488384.0   \n",
       "2     Alabama  2014        3        4843737.0        1510987.0   \n",
       "3     Alabama  2014        4        4843737.0        1509475.0   \n",
       "4     Alabama  2015        1        4854803.0        1510048.0   \n",
       "...       ...   ...      ...              ...              ...   \n",
       "2146  Wyoming  2023        4         578239.0         215710.0   \n",
       "2147  Wyoming  2024        1         580752.0         208388.0   \n",
       "2148  Wyoming  2024        2         580752.0         208453.0   \n",
       "2149  Wyoming  2024        3         580752.0         222565.0   \n",
       "2150  Wyoming  2024        4         580752.0         217840.0   \n",
       "\n",
       "      state_hires_total  state_avg_earnings  econ_emp_per_1k  \\\n",
       "0              234866.0         3183.492990       306.463377   \n",
       "1              296168.0         3186.477239       307.280102   \n",
       "2              292385.0         3158.744593       311.946540   \n",
       "3              273242.0         3383.957254       311.634385   \n",
       "4              246062.0         3224.297608       311.042075   \n",
       "...                 ...                 ...              ...   \n",
       "2146            45344.0         4717.370878       373.046439   \n",
       "2147            36781.0         4665.328800       358.824421   \n",
       "2148            59642.0         4530.285609       358.936345   \n",
       "2149            51118.0         4574.083715       383.235873   \n",
       "2150            41963.0         4891.206367       375.099871   \n",
       "\n",
       "      econ_hires_per_1k  econ_hire_rate  ...  \\\n",
       "0             48.488595        0.158220  ...   \n",
       "1             61.144525        0.198986  ...   \n",
       "2             60.363517        0.193506  ...   \n",
       "3             56.411403        0.181018  ...   \n",
       "4             50.684240        0.162950  ...   \n",
       "...                 ...             ...  ...   \n",
       "2146          78.417402        0.210208  ...   \n",
       "2147          63.333402        0.176502  ...   \n",
       "2148         102.697881        0.286117  ...   \n",
       "2149          88.020360        0.229677  ...   \n",
       "2150          72.256316        0.192632  ...   \n",
       "\n",
       "      STATE_% With Access to Exercise Opportunities  \\\n",
       "0                                         51.935239   \n",
       "1                                         51.935239   \n",
       "2                                         51.935239   \n",
       "3                                         51.935239   \n",
       "4                                         64.355654   \n",
       "...                                             ...   \n",
       "2146                                      77.638870   \n",
       "2147                                      77.761949   \n",
       "2148                                      77.761949   \n",
       "2149                                      77.761949   \n",
       "2150                                      77.761949   \n",
       "\n",
       "      STATE_Food Environment Index  STATE_Income Ratio  \\\n",
       "0                         6.932836            0.000000   \n",
       "1                         6.932836            0.000000   \n",
       "2                         6.932836            0.000000   \n",
       "3                         6.932836            0.000000   \n",
       "4                         6.694086            5.030240   \n",
       "...                            ...                 ...   \n",
       "2146                      7.779140            4.235807   \n",
       "2147                      7.781328            4.259097   \n",
       "2148                      7.781328            4.259097   \n",
       "2149                      7.781328            4.259097   \n",
       "2150                      7.781328            4.259097   \n",
       "\n",
       "      STATE_Mentally Unhealthy Days  STATE_Physically Unhealthy Days  \\\n",
       "0                          4.254047                         4.289236   \n",
       "1                          4.254047                         4.289236   \n",
       "2                          4.254047                         4.289236   \n",
       "3                          4.254047                         4.289236   \n",
       "4                          4.252503                         4.285823   \n",
       "...                             ...                              ...   \n",
       "2146                       4.019662                         2.849604   \n",
       "2147                       4.750646                         3.437344   \n",
       "2148                       4.750646                         3.437344   \n",
       "2149                       4.750646                         3.437344   \n",
       "2150                       4.750646                         3.437344   \n",
       "\n",
       "      STATE_Preventable Hospitalization Rate  \\\n",
       "0                                   0.000000   \n",
       "1                                   0.000000   \n",
       "2                                   0.000000   \n",
       "3                                   0.000000   \n",
       "4                                  69.088952   \n",
       "...                                      ...   \n",
       "2146                             2324.557970   \n",
       "2147                             2178.144266   \n",
       "2148                             2178.144266   \n",
       "2149                             2178.144266   \n",
       "2150                             2178.144266   \n",
       "\n",
       "      STATE_Primary Care Physicians Rate  STATE_Social Association Rate  \\\n",
       "0                              62.132991                       0.000000   \n",
       "1                              62.132991                       0.000000   \n",
       "2                              62.132991                       0.000000   \n",
       "3                              62.132991                       0.000000   \n",
       "4                              62.883135                      12.483551   \n",
       "...                                  ...                            ...   \n",
       "2146                           70.486996                      11.973542   \n",
       "2147                           70.319097                      12.253024   \n",
       "2148                           70.319097                      12.253024   \n",
       "2149                           70.319097                      12.253024   \n",
       "2150                           70.319097                      12.253024   \n",
       "\n",
       "      STATE_Teen Birth Rate  STATE_Violent Crime Rate  \n",
       "0                 48.186843                410.745302  \n",
       "1                 48.186843                410.745302  \n",
       "2                 48.186843                410.745302  \n",
       "3                 48.186843                410.745302  \n",
       "4                 46.968524                410.093125  \n",
       "...                     ...                       ...  \n",
       "2146              24.269712                  0.000000  \n",
       "2147              20.177949                  0.000000  \n",
       "2148              20.177949                  0.000000  \n",
       "2149              20.177949                  0.000000  \n",
       "2150              20.177949                  0.000000  \n",
       "\n",
       "[2151 rows x 39 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e66c8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Feats: 82, Valid Feats: 82\n",
      "\n",
      "Starting Grid Search (Output suppressed)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     [merf.py:307] Training GLL is -389.9590165547215 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -763.8024796282809 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1066.4370615738671 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1219.6380098424627 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1245.887071634718 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -802.7825568258776 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -1530.178423907873 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1926.851257422507 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1968.843972382961 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1974.692369936032 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1270.1531646334872 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2344.6629981083656 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2753.674875127493 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2747.285813116139 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2762.9650951663803 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1769.5950457623462 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3177.007421943216 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3509.513217234815 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3528.2278425766754 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3532.8646507413773 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -386.8263802307089 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -747.5752307119897 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1010.0116208586445 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1112.9729478780837 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1136.7993087000696 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -797.5195553207974 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -1494.5817160134684 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1821.2988706709777 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1840.960722005085 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1841.30491723513 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1262.4061587519773 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2284.4590787090524 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2575.344219136182 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2620.8171355324353 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2616.4218309888784 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1762.2143066602723 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3123.283908140541 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3411.109084029328 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3424.438578024798 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3416.9159921938804 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -392.0041702632045 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -775.0290458775875 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1117.8911575050993 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1340.2392924555843 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1411.6788712095797 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -808.4888802947956 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -1575.210528059228 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2120.4841313877723 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2277.125329455782 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2278.246548567879 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1279.9159592675212 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2457.021680150365 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3071.518347179866 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3138.525374861628 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3167.616775087591 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1791.6906983635258 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3387.6153533444717 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -4041.0503428667716 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -4091.55584442957 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -4071.9020969276116 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -390.58280266051685 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -766.5621689034188 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1081.2595284179488 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1254.9727410940584 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1304.2257808788866 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -804.770265986885 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -1548.3083108249045 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2008.4821904096586 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2112.798648566379 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2116.245924913475 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1275.3532601611712 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2410.3543797355296 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2918.714333551586 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2951.0359541019807 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2978.4855527864697 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1785.6876472054862 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3310.0202421597014 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3842.5039834810814 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3849.4096207748385 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3850.6530653207783 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -390.27427713428244 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -765.6612484095805 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1071.921634347125 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1226.6334844598907 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1261.3577210486787 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -804.5099458149678 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -1547.1120693108628 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1995.8100019704586 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2057.1816986393924 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2064.3005293350543 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1275.7208916979585 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2411.900556414882 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2923.1817336312624 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2929.157080651446 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2925.0571015886335 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1782.9775128383828 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3289.464188911705 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3796.1520240618406 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3779.9438492048985 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3787.4583413305395 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -386.83809414684094 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -747.5548225923112 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1010.2509702014396 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1111.1210877263159 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1136.7119543934994 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -798.5874258040453 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -1497.7795295663884 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1829.8300511405005 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1854.7558102083365 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1850.5051089311492 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1266.0467695924058 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2307.4812127639116 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2638.793339242746 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2653.144951465858 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2658.6149694546234 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1770.1423299999722 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3182.4806418271846 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3524.6279058059345 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3551.8842361394104 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3540.070971809079 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -392.12288497275284 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -775.4447146056779 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1118.0945846106144 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1341.7410411274934 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1416.5818609086195 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -809.3621416719784 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -1582.8343551164805 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2148.2343435769462 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2313.073768175174 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2321.72790752572 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1282.427172004943 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2476.6859691076634 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3157.7936135337964 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3249.3378026719006 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3264.659820305144 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1796.2634580180452 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3434.0958171469715 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -4216.338607842433 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -4238.250425549168 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -4235.855145401501 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -390.58257973465845 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -766.565451356063 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1080.9410414000415 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1254.4522579047314 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1304.7033436416705 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -804.8563888100625 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -1550.4092622114972 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2018.0840281102335 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2110.010278527771 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2114.987750820172 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1276.177523576826 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2414.8959410922425 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2933.7148426298754 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2957.3398512149743 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2986.0197122427944 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1786.9120141938852 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3329.9193021391393 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3859.524399386511 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3883.4963984638807 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3887.2718145197446 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -390.26641377969 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -765.6249632944026 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1073.3564125231376 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1226.2952157044554 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1263.6039655971683 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -805.3316271738274 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -1547.0256476533607 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1994.3856552440566 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2069.942824473442 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2073.910824661183 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1277.1986112833415 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2421.7313796344497 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2964.720324073684 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2994.1262351710175 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2983.2209035334295 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1786.8494801221004 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3333.8407711191153 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3914.59239250786 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3948.5120194646333 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3929.209246190848 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -386.83809414684083 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -747.5559575960764 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1010.2222311906136 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1109.3168148942661 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1135.6794252854197 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -798.6545138039137 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -1497.5307071221118 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1827.935316746009 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1854.2078942187973 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1850.9486237275494 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1267.1994691968969 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2319.7105287836184 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2648.9784228140784 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2662.4212422984333 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2668.5389238045523 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1771.3527520471507 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3197.020727617946 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3543.934184846357 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3564.0266134608764 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3551.508311957519 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -392.1233197214718 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -775.3765064951024 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1117.8037066451598 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1342.733324598512 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1417.1143011840397 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -809.427118847174 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -1582.2057839128363 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2149.5796901580165 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2309.2117398016403 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2322.9997351261045 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1282.8989471695425 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2482.9236659345297 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3177.3471771832715 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3266.517808600596 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3279.258128447062 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1796.5887950183428 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3436.740349118054 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -4207.986555341021 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -4263.8949755086705 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -4268.963577419436 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -390.58257973465845 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -766.563922108732 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1080.7936677849643 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1254.3100156432422 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1303.8409652807377 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -804.9251129333624 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -1550.8211063335684 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2018.5720656180008 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2113.132625960077 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2116.0339399468976 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1276.2413188007768 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2416.2596366952657 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2926.681407722224 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2959.2750693610437 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2983.300547335958 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1787.0843934499742 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3324.143688365272 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3868.319615472189 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3886.4221353177422 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3894.8656724507487 at iteration 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV RMSE (Transformed): 0.0497\n",
      "Best Params: {'max_depth': 8, 'max_features': 0.3, 'min_samples_leaf': 5}\n",
      "\n",
      "Fitting Final Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     [merf.py:307] Training GLL is -2340.865913413404 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -4432.421052293918 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -5232.642321370703 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -5270.968293472973 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -5260.1736364500985 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -5256.38253185449 at iteration 6.\n",
      "INFO     [merf.py:307] Training GLL is -5255.376030464955 at iteration 7.\n",
      "INFO     [merf.py:307] Training GLL is -5248.855765475695 at iteration 8.\n",
      "INFO     [merf.py:307] Training GLL is -5246.2706559197395 at iteration 9.\n",
      "INFO     [merf.py:307] Training GLL is -5235.7351235019505 at iteration 10.\n",
      "INFO     [merf.py:307] Training GLL is -5227.699393775909 at iteration 11.\n",
      "INFO     [merf.py:307] Training GLL is -5234.3327533759475 at iteration 12.\n",
      "INFO     [merf.py:307] Training GLL is -5238.424584501637 at iteration 13.\n",
      "INFO     [merf.py:307] Training GLL is -5232.590736964221 at iteration 14.\n",
      "INFO     [merf.py:307] Training GLL is -5240.952361128637 at iteration 15.\n",
      "INFO     [merf.py:307] Training GLL is -5238.823378684637 at iteration 16.\n",
      "INFO     [merf.py:307] Training GLL is -5251.0882002834105 at iteration 17.\n",
      "INFO     [merf.py:307] Training GLL is -5254.725189290438 at iteration 18.\n",
      "INFO     [merf.py:307] Training GLL is -5243.434159186222 at iteration 19.\n",
      "INFO     [merf.py:307] Training GLL is -5256.491982153235 at iteration 20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESULTS ---\n",
      "{'split': 'TRAIN', 'r2': 0.9786799013797017, 'rmse': 1.891661322788209, 'mae': 1.2850111471856491}\n",
      "{'split': 'TEST', 'r2': 0.5262158317967627, 'rmse': 9.76405394411975, 'mae': 7.307097495037219}\n",
      "\n",
      "Top 10 Drivers:\n",
      "                    feature  importance\n",
      "76   econ_hires_per_1k_lag4    0.447869\n",
      "15     growth_earn_qoq_lag1    0.101357\n",
      "10      growth_emp_qoq_lag4    0.101035\n",
      "79                      Q_2    0.063772\n",
      "77  econ_hires_per_1k_roll4    0.041697\n",
      "75   econ_hires_per_1k_lag1    0.034116\n",
      "9       growth_emp_qoq_lag1    0.032600\n",
      "78                  t_trend    0.017890\n",
      "8     state_total_pop_roll4    0.010428\n",
      "16     growth_earn_qoq_lag4    0.009857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from merf.merf import MERF\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================\n",
    "# CONFIG\n",
    "# =============================\n",
    "TARGET = \"econ_hires_per_1k\"\n",
    "USE_LOG_TARGET = True  # Transform target to log scale to handle outliers (NV, ND)\n",
    "\n",
    "# Expanded Candidate List based on your available columns\n",
    "# We exclude 'hire_rate' as requested, and exclude the target itself.\n",
    "CANDIDATE_X = [\n",
    "    # Econ / Structural\n",
    "    \"econ_emp_per_1k\",\n",
    "    \"state_avg_earnings\",\n",
    "    \"state_total_pop\", \n",
    "    \n",
    "    # Growth Metrics\n",
    "    \"growth_emp_qoq\",\n",
    "    \"growth_emp_yoy\",\n",
    "    \"growth_earn_qoq\",\n",
    "    \n",
    "    # Health / Social Determinants (Expanded)\n",
    "    \"STATE_% Adults with Obesity\",\n",
    "    \"STATE_% Children in Poverty\",\n",
    "    \"STATE_% Children in Single-Parent Households\",\n",
    "    \"STATE_% Excessive Drinking\",\n",
    "    \"STATE_% Fair or Poor Health\",\n",
    "    \"STATE_% Low Birthweight\",\n",
    "    \"STATE_% Severe Housing Problems\",\n",
    "    \"STATE_% Smokers\",\n",
    "    \"STATE_% Some College\",          # Added: Workforce skill proxy\n",
    "    \"STATE_% Unemployed\",            # Added: Critical labor market indicator\n",
    "    \"STATE_% Uninsured\",             # Added: Gig economy proxy\n",
    "    \"STATE_Food Environment Index\",\n",
    "    \"STATE_Income Ratio\",            # Added: Inequality proxy\n",
    "    \"STATE_Mentally Unhealthy Days\",\n",
    "    \"STATE_Physically Unhealthy Days\",\n",
    "    \"STATE_Preventable Hospitalization Rate\",\n",
    "    \"STATE_Primary Care Physicians Rate\",\n",
    "    \"STATE_Social Association Rate\",\n",
    "    \"STATE_Violent Crime Rate\"\n",
    "]\n",
    "\n",
    "EXCLUDE_STATES = {\"Alaska\"} \n",
    "TRAIN_END_YEAR = 2019\n",
    "N_FOLDS = 4\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# TUNING SETTINGS\n",
    "TUNE_N_ESTIMATORS = 100 \n",
    "FINAL_N_ESTIMATORS = 500 \n",
    "\n",
    "# =============================\n",
    "# HELPERS\n",
    "# =============================\n",
    "class HiddenPrints:\n",
    "    \"\"\"Context manager to suppress stdout\"\"\"\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def metrics_block(name, y_true, y_pred):\n",
    "    return {\n",
    "        \"split\": name,\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "        \"rmse\": rmse(y_true, y_pred),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "def normalize_quarter(q):\n",
    "    q = pd.to_numeric(q, errors=\"coerce\")\n",
    "    if pd.notna(q.min()) and q.min() == 0:\n",
    "        q = q + 1\n",
    "    return q\n",
    "\n",
    "def make_time_index(df):\n",
    "    return (df[\"Year\"].astype(int) * 4 + df[\"quarter\"].astype(int)).astype(int)\n",
    "\n",
    "# =============================\n",
    "# 1) DATA PREP & FEATURE ENGINEERING\n",
    "# =============================\n",
    "df_work = df.copy()\n",
    "df_work = df_work[~df_work[\"State\"].isin(EXCLUDE_STATES)].copy()\n",
    "\n",
    "# Basic Cleanup\n",
    "df_work[\"Year\"] = pd.to_numeric(df_work[\"Year\"], errors=\"coerce\")\n",
    "df_work[\"quarter\"] = normalize_quarter(df_work[\"quarter\"])\n",
    "df_work[TARGET] = pd.to_numeric(df_work[TARGET], errors=\"coerce\")\n",
    "\n",
    "# Filter Candidate X to what actually exists in df\n",
    "REAL_X = [c for c in CANDIDATE_X if c in df_work.columns]\n",
    "for c in REAL_X:\n",
    "    df_work[c] = pd.to_numeric(df_work[c], errors=\"coerce\")\n",
    "\n",
    "df_work = df_work.dropna(subset=[\"State\", \"Year\", \"quarter\", TARGET]).copy()\n",
    "df_work[\"Year\"] = df_work[\"Year\"].astype(int)\n",
    "df_work[\"quarter\"] = df_work[\"quarter\"].astype(int)\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "\n",
    "# 1. Time Index\n",
    "df_work[\"time_index\"] = make_time_index(df_work)\n",
    "df_work[\"t_trend\"] = df_work[\"time_index\"] - df_work[\"time_index\"].min()\n",
    "\n",
    "# 2. Seasonality (One Hot)\n",
    "q_dummies = pd.get_dummies(df_work['quarter'], prefix='Q', drop_first=True)\n",
    "df_work = pd.concat([df_work, q_dummies], axis=1)\n",
    "seasonal_cols = list(q_dummies.columns)\n",
    "\n",
    "# 3. Advanced Lags & Rolling Windows\n",
    "# We sort by State/Time to ensure shift/rolling works\n",
    "df_work = df_work.sort_values(['State', 'time_index'])\n",
    "\n",
    "# Features to apply Lag/Rolling to: All REAL_X + Target\n",
    "cols_to_transform = REAL_X + [TARGET]\n",
    "\n",
    "feat_cols = []\n",
    "\n",
    "for c in cols_to_transform:\n",
    "    # Lag 1 (Immediate past)\n",
    "    df_work[f\"{c}_lag1\"] = df_work.groupby(\"State\")[c].shift(1)\n",
    "    feat_cols.append(f\"{c}_lag1\")\n",
    "    \n",
    "    # Lag 4 (Year over Year reference)\n",
    "    df_work[f\"{c}_lag4\"] = df_work.groupby(\"State\")[c].shift(4)\n",
    "    feat_cols.append(f\"{c}_lag4\")\n",
    "    \n",
    "    # Rolling Mean 4 (Annual Trend Smoothing) - IMPROVEMENT\n",
    "    # This helps smooth out quarterly noise\n",
    "    df_work[f\"{c}_roll4\"] = df_work.groupby(\"State\")[c].transform(lambda x: x.shift(1).rolling(4).mean())\n",
    "    feat_cols.append(f\"{c}_roll4\")\n",
    "\n",
    "# Add Trend and Seasonality to features\n",
    "feat_cols += [\"t_trend\"] + seasonal_cols\n",
    "\n",
    "# Drop NaN caused by lags\n",
    "df_model = df_work.dropna(subset=feat_cols + [TARGET]).copy()\n",
    "\n",
    "# Log Transform Target if enabled (Helps with NV/ND outliers)\n",
    "if USE_LOG_TARGET:\n",
    "    df_model[f\"{TARGET}_log\"] = np.log1p(df_model[TARGET])\n",
    "    target_col_name = f\"{TARGET}_log\"\n",
    "else:\n",
    "    target_col_name = TARGET\n",
    "\n",
    "# =============================\n",
    "# 2) TRAIN/TEST SPLIT\n",
    "# =============================\n",
    "df_tr = df_model[df_model[\"Year\"] <= TRAIN_END_YEAR].copy()\n",
    "df_te = df_model[df_model[\"Year\"] > TRAIN_END_YEAR].copy()\n",
    "\n",
    "# Drop columns that are all-NaN in training\n",
    "valid_cols = [c for c in feat_cols if df_tr[c].notna().all()]\n",
    "print(f\"Original Feats: {len(feat_cols)}, Valid Feats: {len(valid_cols)}\")\n",
    "\n",
    "X_cols = valid_cols\n",
    "\n",
    "y_tr = df_tr[target_col_name].values.astype(float)\n",
    "y_te = df_te[target_col_name].values.astype(float)\n",
    "\n",
    "# Groups for MERF\n",
    "cl_tr = df_tr[\"State\"].astype(str)\n",
    "cl_te = df_te[\"State\"].astype(str)\n",
    "\n",
    "# Random Effect Design Matrix (Z) - Just a bias term (ones)\n",
    "Z_tr = np.ones((len(df_tr), 1), dtype=float)\n",
    "Z_te = np.ones((len(df_te), 1), dtype=float)\n",
    "\n",
    "X_tr_raw = df_tr[X_cols]\n",
    "X_te_raw = df_te[X_cols]\n",
    "\n",
    "# Impute\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_tr = imp.fit_transform(X_tr_raw)\n",
    "X_te = imp.transform(X_te_raw)\n",
    "\n",
    "# =============================\n",
    "# 3) TUNING (QUIET MODE)\n",
    "# =============================\n",
    "print(\"\\nStarting Grid Search (Output suppressed)...\")\n",
    "\n",
    "# Reduced complexity grid to prevent overfitting\n",
    "param_grid = {\n",
    "    \"max_depth\": [6, 8, 12],         # Cap depth to force generalization\n",
    "    \"min_samples_leaf\": [5, 10],     # Require more samples per leaf\n",
    "    \"max_features\": [\"sqrt\", 0.3],   # Force tree decorrelation\n",
    "}\n",
    "\n",
    "# Walk-Forward Splits\n",
    "times = sorted(df_tr[\"time_index\"].unique())\n",
    "blocks = np.array_split(times, N_FOLDS + 1)\n",
    "wf_splits = []\n",
    "curr_tr = blocks[0]\n",
    "for i in range(N_FOLDS):\n",
    "    if i > 0: curr_tr = np.concatenate([curr_tr, blocks[i]])\n",
    "    curr_va = blocks[i+1]\n",
    "    wf_splits.append((set(curr_tr), set(curr_va)))\n",
    "\n",
    "best_rmse = np.inf\n",
    "best_params = None\n",
    "\n",
    "# Using HiddenPrints to silence MERF iteration logs\n",
    "with HiddenPrints():\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        scores = []\n",
    "        for tr_t, va_t in wf_splits:\n",
    "            # Indices\n",
    "            idx_tr = df_tr[\"time_index\"].isin(tr_t)\n",
    "            idx_va = df_tr[\"time_index\"].isin(va_t)\n",
    "            \n",
    "            # Slice\n",
    "            X_f_tr, y_f_tr, Z_f_tr, cl_f_tr = X_tr[idx_tr], y_tr[idx_tr], Z_tr[idx_tr], cl_tr[idx_tr]\n",
    "            X_f_va, y_f_va, Z_f_va, cl_f_va = X_tr[idx_va], y_tr[idx_va], Z_tr[idx_va], cl_tr[idx_va]\n",
    "            \n",
    "            # Fit\n",
    "            rf = RandomForestRegressor(n_estimators=TUNE_N_ESTIMATORS, n_jobs=-1, random_state=RANDOM_STATE, **params)\n",
    "            m = MERF(rf, max_iterations=5) # Low iterations for tuning speed\n",
    "            m.fit(X_f_tr, Z_f_tr, cl_f_tr, y_f_tr)\n",
    "            \n",
    "            p_va = m.predict(X_f_va, Z_f_va, cl_f_va)\n",
    "            scores.append(np.sqrt(mean_squared_error(y_f_va, p_va)))\n",
    "        \n",
    "        avg_score = np.mean(scores)\n",
    "        if avg_score < best_rmse:\n",
    "            best_rmse = avg_score\n",
    "            best_params = params\n",
    "\n",
    "print(f\"Best CV RMSE (Transformed): {best_rmse:.4f}\")\n",
    "print(f\"Best Params: {best_params}\")\n",
    "\n",
    "# =============================\n",
    "# 4) FINAL FIT & EVAL\n",
    "# =============================\n",
    "print(\"\\nFitting Final Model...\")\n",
    "\n",
    "rf_final = RandomForestRegressor(n_estimators=FINAL_N_ESTIMATORS, n_jobs=-1, random_state=RANDOM_STATE, **best_params)\n",
    "\n",
    "# Silence the final fit as well\n",
    "with HiddenPrints():\n",
    "    merf_final = MERF(rf_final, max_iterations=20)\n",
    "    merf_final.fit(X_tr, Z_tr, cl_tr, y_tr)\n",
    "\n",
    "# Predict\n",
    "pred_tr_trans = merf_final.predict(X_tr, Z_tr, cl_tr)\n",
    "pred_te_trans = merf_final.predict(X_te, Z_te, cl_te)\n",
    "\n",
    "# Inverse Transform if Log used\n",
    "if USE_LOG_TARGET:\n",
    "    y_tr_orig = np.expm1(y_tr)\n",
    "    y_te_orig = np.expm1(y_te)\n",
    "    pred_tr_orig = np.expm1(pred_tr_trans)\n",
    "    pred_te_orig = np.expm1(pred_te_trans)\n",
    "else:\n",
    "    y_tr_orig = y_tr\n",
    "    y_te_orig = y_te\n",
    "    pred_tr_orig = pred_tr_trans\n",
    "    pred_te_orig = pred_te_trans\n",
    "\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "print(metrics_block(\"TRAIN\", y_tr_orig, pred_tr_orig))\n",
    "print(metrics_block(\"TEST\", y_te_orig, pred_te_orig))\n",
    "\n",
    "# Feature Importance (from the fixed effect RF)\n",
    "imp_df = pd.DataFrame({\n",
    "    'feature': X_cols,\n",
    "    'importance': merf_final.trained_fe_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "print(\"\\nTop 10 Drivers:\")\n",
    "print(imp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f5ad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     [merf.py:307] Training GLL is -515.5065244241347 at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1176 samples, Testing on 965 samples.\n",
      "Feature Count: 25\n",
      "\n",
      "Starting Grid Search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     [merf.py:307] Training GLL is -981.5883954604631 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1270.0948334693417 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1360.6272367426488 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1381.6622625057528 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1235.1368924525468 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2145.8203813757277 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2340.9001230339163 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2352.5856106325587 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2364.7000630776647 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -2054.2275645098343 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3273.5287210966317 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3499.511461248862 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3539.5035127421893 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3525.3774674348156 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -2941.9592288477766 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -4550.080355316541 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -4681.567124520803 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -4691.710826390192 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -4656.053562193562 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -509.5201291810846 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -946.6339844474736 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1179.2169620890127 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1238.891716502578 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1256.4281554674235 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1218.6705806517105 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2044.4391866150077 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2204.5242887482123 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2205.153029523266 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2206.5526005547363 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -2029.5372008520626 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3224.1806692882474 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3332.539923046122 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3368.6745511425993 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3400.836394191703 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -2912.0853480310575 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -4365.7371520696315 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -4473.360187941018 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -4495.911439586304 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -4618.006538341993 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -519.2012739694236 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -1005.862740823655 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1359.1194248370045 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1501.326110281814 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1528.7213073736034 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1262.3746157873848 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2335.828332105034 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2762.9929793116335 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2794.047261366778 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2847.1966784210213 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -2116.3291419770308 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3691.775716008797 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -4005.732395642147 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -4103.0852684697675 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -4153.773503402393 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -3035.9546298073114 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -5061.3512175819 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -5444.865885558697 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -5528.724940299081 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -5581.635354220673 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -514.4191511425719 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -977.114887012878 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1271.770082355546 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1386.2876225499356 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1424.5598389843626 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1253.0706754631492 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2263.9491683138367 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2586.5913995857263 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2617.4577318271245 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2619.860924341743 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -2095.6526913665416 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3587.371671613578 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3963.4293800187684 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -4024.1669089229354 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -4016.86840420484 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -3011.3299646505247 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -4960.689771058871 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -5264.44624225381 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -5290.651954537666 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -5351.402200828569 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -516.2529656296907 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -983.9227688471873 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1277.2916095877376 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1366.7077541037136 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1378.586190795204 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1247.728435070036 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2206.3127313709615 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2447.132817280688 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2474.2404332536544 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2484.256926663823 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -2089.969804730555 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3507.5216755431197 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3763.615565369942 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3779.292065400682 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3802.931481856426 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -3009.268859986208 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -4875.546172409931 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -5064.075242696805 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -5146.498963520808 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -5060.04933795363 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -509.5376718998237 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -946.8226467499186 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1179.7091499972037 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1240.9432498081792 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1257.6743294548953 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1221.826594225311 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2056.829654679796 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2226.0387013183185 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2205.8783251707596 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2222.9909543256176 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -2051.669214465145 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3301.9829530105735 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3463.4789063109374 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3448.353512136651 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3440.409559941215 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -2956.373966871334 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -4595.506372378757 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -4745.6541168039175 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -4737.649153262471 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -4763.032462437446 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -519.4716489717144 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -1007.100564793061 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1360.6664863833523 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1506.809145701531 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1531.9144668952656 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1268.3638379317692 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2366.458234231412 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2803.1299555089763 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2873.2467171630333 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2903.0804045934565 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -2133.611728852212 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3842.227946636424 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -4299.506182184197 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -4318.9715077452565 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -4378.09489109686 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -3073.6574204207177 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -5435.526290985948 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -5883.14423597898 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -5933.74207302736 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -5935.021273895796 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -514.4006320104634 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -977.1877739162768 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1271.833464864045 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1385.8982640572076 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1428.0395905572805 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1254.1516241825802 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2271.8511421284775 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2587.2808044813924 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2617.4551848124843 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2632.2759893195785 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -2106.9631825310435 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3660.2337061947374 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -4024.120147011957 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -4093.8011589734124 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -4108.367327527258 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -3036.8194697493286 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -5111.3488489968495 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -5459.352579593222 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -5490.714552126392 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -5520.0346136246535 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -516.2709153148504 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -983.9762041676719 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1277.60993685403 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1366.7417751755181 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1384.677557677206 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1247.7717113124716 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2212.2940243913094 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2458.66271139617 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2483.776614889827 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2478.1999166496626 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -2094.9929905247204 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3528.872042769791 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3801.927278017784 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3842.310992533685 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3839.4217249705825 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -3012.5038528440764 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -4962.138334449164 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -5134.42778336417 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -5209.1666405938995 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -5180.8822575970835 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -509.53767189982364 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -946.8226467499188 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1179.7091499972037 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1240.932706301643 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1257.6746368367924 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1221.8379657131927 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2057.092966405761 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2227.7194925928957 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2209.9391772475324 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2222.7455274687245 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -2053.2918600806906 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3312.257252738663 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -3468.6856415992197 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -3467.019581843591 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -3468.828527396256 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -2959.605158572123 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -4598.853797134278 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -4751.211567676753 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -4765.176972367888 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -4803.761656341376 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -519.4670852448046 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -1007.1042242169559 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1361.4432025388764 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1511.362529071591 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1531.2803430538563 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1268.5291431595713 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2369.423721590713 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2804.7699621719235 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2877.3151874350015 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2903.361625905815 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -2134.719005714646 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3854.2302336965836 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -4308.004824394574 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -4340.7828085086085 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -4371.294656129629 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -3074.615669515928 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -5436.576076118093 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -5913.643847459095 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -5951.720414666815 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -5969.798474112408 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -514.400632010463 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -977.1888046293766 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -1271.7644026846815 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -1386.3778923054174 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -1425.2105254342896 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -1254.163181546975 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -2271.7634712314534 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -2587.978970377305 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -2618.2969128902937 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -2632.652550426685 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -2106.9398698430236 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -3659.484441195765 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -4030.2017867176537 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -4097.247837375808 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -4108.326535344761 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -3038.151012661701 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -5111.553283013192 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -5468.422311438851 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -5484.855121043979 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -5523.792814987569 at iteration 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV RMSE: 0.0976\n",
      "Best Params: {'max_depth': 15, 'max_features': 0.4, 'min_samples_leaf': 5}\n",
      "\n",
      "Fitting Final Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     [merf.py:307] Training GLL is -3870.8675490942383 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -6746.383607277591 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -7197.555411067376 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -7244.178007972446 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -7278.204437394342 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -7309.340217718938 at iteration 6.\n",
      "INFO     [merf.py:307] Training GLL is -7317.451593306581 at iteration 7.\n",
      "INFO     [merf.py:307] Training GLL is -7317.884391084632 at iteration 8.\n",
      "INFO     [merf.py:307] Training GLL is -7323.84853448706 at iteration 9.\n",
      "INFO     [merf.py:307] Training GLL is -7330.384558296573 at iteration 10.\n",
      "INFO     [merf.py:307] Training GLL is -7346.358340526577 at iteration 11.\n",
      "INFO     [merf.py:307] Training GLL is -7355.729110285766 at iteration 12.\n",
      "INFO     [merf.py:307] Training GLL is -7362.199863056326 at iteration 13.\n",
      "INFO     [merf.py:307] Training GLL is -7351.443294337148 at iteration 14.\n",
      "INFO     [merf.py:307] Training GLL is -7341.433390764924 at iteration 15.\n",
      "INFO     [merf.py:307] Training GLL is -7338.4692783247965 at iteration 16.\n",
      "INFO     [merf.py:307] Training GLL is -7334.949506689077 at iteration 17.\n",
      "INFO     [merf.py:307] Training GLL is -7316.912380908081 at iteration 18.\n",
      "INFO     [merf.py:307] Training GLL is -7293.379876591528 at iteration 19.\n",
      "INFO     [merf.py:307] Training GLL is -7291.601717817075 at iteration 20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESULTS ---\n",
      "{'split': 'TRAIN', 'r2': 0.9706942927979709, 'rmse': 2.3563835747318023, 'mae': 1.5091733442876165}\n",
      "{'split': 'TEST', 'r2': 0.49087541076370655, 'rmse': 10.121664418518609, 'mae': 7.6325814881300245}\n",
      "\n",
      "Top 12 Drivers (Structural & Health):\n",
      "                          feature  importance\n",
      "22                            Q_2    0.271545\n",
      "23                            Q_3    0.191395\n",
      "24                            Q_4    0.139632\n",
      "21                        t_trend    0.130629\n",
      "4                 state_total_pop    0.032168\n",
      "3            STATE_% Some College    0.023925\n",
      "2               STATE_% Uninsured    0.023090\n",
      "1              STATE_% Unemployed    0.019408\n",
      "0              state_avg_earnings    0.017856\n",
      "6     STATE_% Children in Poverty    0.017375\n",
      "9     STATE_% Fair or Poor Health    0.015828\n",
      "19  STATE_Social Association Rate    0.013839\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from merf.merf import MERF\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================\n",
    "# CONFIG\n",
    "# =============================\n",
    "TARGET = \"econ_hires_per_1k\"\n",
    "USE_LOG_TARGET = True \n",
    "\n",
    "# --- CRITICAL CHANGE: STRUCTURAL FEATURE SET ---\n",
    "# We removed all \"Target Lags\" and \"QoQ Growth\". \n",
    "# The model must now rely on Health and Economic Fundamentals.\n",
    "\n",
    "CANDIDATE_X = [\n",
    "    # 1. Economic Fundamentals (Levels, not growth)\n",
    "    \"state_avg_earnings\",           # Wage theory: Lower wages = higher turnover?\n",
    "    \"STATE_% Unemployed\",           # Phillips curve: Labor leverage\n",
    "    \"STATE_% Uninsured\",            # Gig economy / Precarious work proxy\n",
    "    \"STATE_% Some College\",         # Skill level proxy\n",
    "    \"state_total_pop\",              # Scale factor\n",
    "\n",
    "    # 2. Health & Social Determinants (The Focus)\n",
    "    \"STATE_% Adults with Obesity\",\n",
    "    \"STATE_% Children in Poverty\",\n",
    "    \"STATE_% Children in Single-Parent Households\",\n",
    "    \"STATE_% Excessive Drinking\",\n",
    "    \"STATE_% Fair or Poor Health\",\n",
    "    \"STATE_% Low Birthweight\",\n",
    "    \"STATE_% Severe Housing Problems\",\n",
    "    \"STATE_% Smokers\",\n",
    "    \"STATE_Food Environment Index\",\n",
    "    \"STATE_Income Ratio\",\n",
    "    \"STATE_Mentally Unhealthy Days\",\n",
    "    \"STATE_Physically Unhealthy Days\",\n",
    "    \"STATE_Preventable Hospitalization Rate\",\n",
    "    \"STATE_Primary Care Physicians Rate\",\n",
    "    \"STATE_Social Association Rate\",\n",
    "    \"STATE_Violent Crime Rate\"\n",
    "]\n",
    "\n",
    "# We will add Seasonality (Q1-Q4) and Time Trend (t_trend) manually below.\n",
    "\n",
    "EXCLUDE_STATES = {\"Alaska\"} \n",
    "TRAIN_END_YEAR = 2019\n",
    "N_FOLDS = 4\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# TUNING SETTINGS\n",
    "TUNE_N_ESTIMATORS = 100 \n",
    "FINAL_N_ESTIMATORS = 500 \n",
    "\n",
    "# =============================\n",
    "# HELPERS\n",
    "# =============================\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "def metrics_block(name, y_true, y_pred):\n",
    "    return {\n",
    "        \"split\": name,\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "def normalize_quarter(q):\n",
    "    q = pd.to_numeric(q, errors=\"coerce\")\n",
    "    if pd.notna(q.min()) and q.min() == 0:\n",
    "        q = q + 1\n",
    "    return q\n",
    "\n",
    "def make_time_index(df):\n",
    "    return (df[\"Year\"].astype(int) * 4 + df[\"quarter\"].astype(int)).astype(int)\n",
    "\n",
    "# =============================\n",
    "# 1) DATA PREP\n",
    "# =============================\n",
    "df_work = df.copy()\n",
    "df_work = df_work[~df_work[\"State\"].isin(EXCLUDE_STATES)].copy()\n",
    "\n",
    "df_work[\"Year\"] = pd.to_numeric(df_work[\"Year\"], errors=\"coerce\")\n",
    "df_work[\"quarter\"] = normalize_quarter(df_work[\"quarter\"])\n",
    "df_work[TARGET] = pd.to_numeric(df_work[TARGET], errors=\"coerce\")\n",
    "\n",
    "REAL_X = [c for c in CANDIDATE_X if c in df_work.columns]\n",
    "for c in REAL_X:\n",
    "    df_work[c] = pd.to_numeric(df_work[c], errors=\"coerce\")\n",
    "\n",
    "df_work = df_work.dropna(subset=[\"State\", \"Year\", \"quarter\", TARGET]).copy()\n",
    "df_work[\"Year\"] = df_work[\"Year\"].astype(int)\n",
    "df_work[\"quarter\"] = df_work[\"quarter\"].astype(int)\n",
    "\n",
    "# Time Features\n",
    "df_work[\"time_index\"] = make_time_index(df_work)\n",
    "df_work[\"t_trend\"] = df_work[\"time_index\"] - df_work[\"time_index\"].min()\n",
    "\n",
    "# Seasonality (Crucial for Hires)\n",
    "q_dummies = pd.get_dummies(df_work['quarter'], prefix='Q', drop_first=True)\n",
    "df_work = pd.concat([df_work, q_dummies], axis=1)\n",
    "seasonal_cols = list(q_dummies.columns)\n",
    "\n",
    "# Combine Features\n",
    "# Note: We are NO LONGER generating lags for everything. \n",
    "# We are using the \"Current Status\" to predict \"Current Hires\".\n",
    "X_cols = REAL_X + [\"t_trend\"] + seasonal_cols\n",
    "\n",
    "# We still need to handle NaNs if any (though unlikely without lags)\n",
    "df_model = df_work.dropna(subset=X_cols + [TARGET]).copy()\n",
    "\n",
    "# Log Transform Target\n",
    "if USE_LOG_TARGET:\n",
    "    df_model[f\"{TARGET}_log\"] = np.log1p(df_model[TARGET])\n",
    "    target_col_name = f\"{TARGET}_log\"\n",
    "else:\n",
    "    target_col_name = TARGET\n",
    "\n",
    "# =============================\n",
    "# 2) TRAIN/TEST SPLIT\n",
    "# =============================\n",
    "df_tr = df_model[df_model[\"Year\"] <= TRAIN_END_YEAR].copy()\n",
    "df_te = df_model[df_model[\"Year\"] > TRAIN_END_YEAR].copy()\n",
    "\n",
    "y_tr = df_tr[target_col_name].values.astype(float)\n",
    "y_te = df_te[target_col_name].values.astype(float)\n",
    "\n",
    "cl_tr = df_tr[\"State\"].astype(str)\n",
    "cl_te = df_te[\"State\"].astype(str)\n",
    "\n",
    "Z_tr = np.ones((len(df_tr), 1), dtype=float)\n",
    "Z_te = np.ones((len(df_te), 1), dtype=float)\n",
    "\n",
    "X_tr_raw = df_tr[X_cols]\n",
    "X_te_raw = df_te[X_cols]\n",
    "\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_tr = imp.fit_transform(X_tr_raw)\n",
    "X_te = imp.transform(X_te_raw)\n",
    "\n",
    "print(f\"Training on {len(df_tr)} samples, Testing on {len(df_te)} samples.\")\n",
    "print(f\"Feature Count: {len(X_cols)}\")\n",
    "\n",
    "# =============================\n",
    "# 3) TUNING\n",
    "# =============================\n",
    "print(\"\\nStarting Grid Search...\")\n",
    "# Increased depth slightly because we removed the strong lag features. \n",
    "# The model needs more complexity to find the subtle health patterns.\n",
    "param_grid = {\n",
    "    \"max_depth\": [8, 12, 15],         \n",
    "    \"min_samples_leaf\": [5, 10],     \n",
    "    \"max_features\": [\"sqrt\", 0.4],   \n",
    "}\n",
    "\n",
    "times = sorted(df_tr[\"time_index\"].unique())\n",
    "blocks = np.array_split(times, N_FOLDS + 1)\n",
    "wf_splits = []\n",
    "curr_tr = blocks[0]\n",
    "for i in range(N_FOLDS):\n",
    "    if i > 0: curr_tr = np.concatenate([curr_tr, blocks[i]])\n",
    "    curr_va = blocks[i+1]\n",
    "    wf_splits.append((set(curr_tr), set(curr_va)))\n",
    "\n",
    "best_rmse = np.inf\n",
    "best_params = None\n",
    "\n",
    "with HiddenPrints():\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        scores = []\n",
    "        for tr_t, va_t in wf_splits:\n",
    "            idx_tr = df_tr[\"time_index\"].isin(tr_t)\n",
    "            idx_va = df_tr[\"time_index\"].isin(va_t)\n",
    "            \n",
    "            # Skip if split is empty\n",
    "            if not any(idx_tr) or not any(idx_va): continue\n",
    "\n",
    "            X_f_tr, y_f_tr, Z_f_tr, cl_f_tr = X_tr[idx_tr], y_tr[idx_tr], Z_tr[idx_tr], cl_tr[idx_tr]\n",
    "            X_f_va, y_f_va, Z_f_va, cl_f_va = X_tr[idx_va], y_tr[idx_va], Z_tr[idx_va], cl_tr[idx_va]\n",
    "            \n",
    "            rf = RandomForestRegressor(n_estimators=TUNE_N_ESTIMATORS, n_jobs=-1, random_state=RANDOM_STATE, **params)\n",
    "            m = MERF(rf, max_iterations=5)\n",
    "            m.fit(X_f_tr, Z_f_tr, cl_f_tr, y_f_tr)\n",
    "            \n",
    "            p_va = m.predict(X_f_va, Z_f_va, cl_f_va)\n",
    "            scores.append(np.sqrt(mean_squared_error(y_f_va, p_va)))\n",
    "        \n",
    "        if scores:\n",
    "            avg_score = np.mean(scores)\n",
    "            if avg_score < best_rmse:\n",
    "                best_rmse = avg_score\n",
    "                best_params = params\n",
    "\n",
    "print(f\"Best CV RMSE: {best_rmse:.4f}\")\n",
    "print(f\"Best Params: {best_params}\")\n",
    "\n",
    "# =============================\n",
    "# 4) FINAL FIT\n",
    "# =============================\n",
    "print(\"\\nFitting Final Model...\")\n",
    "rf_final = RandomForestRegressor(n_estimators=FINAL_N_ESTIMATORS, n_jobs=-1, random_state=RANDOM_STATE, **best_params)\n",
    "\n",
    "with HiddenPrints():\n",
    "    merf_final = MERF(rf_final, max_iterations=20)\n",
    "    merf_final.fit(X_tr, Z_tr, cl_tr, y_tr)\n",
    "\n",
    "pred_tr_trans = merf_final.predict(X_tr, Z_tr, cl_tr)\n",
    "pred_te_trans = merf_final.predict(X_te, Z_te, cl_te)\n",
    "\n",
    "if USE_LOG_TARGET:\n",
    "    y_tr_orig = np.expm1(y_tr)\n",
    "    y_te_orig = np.expm1(y_te)\n",
    "    pred_tr_orig = np.expm1(pred_tr_trans)\n",
    "    pred_te_orig = np.expm1(pred_te_trans)\n",
    "else:\n",
    "    y_tr_orig = y_tr\n",
    "    y_te_orig = y_te\n",
    "    pred_tr_orig = pred_tr_trans\n",
    "    pred_te_orig = pred_te_trans\n",
    "\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "print(metrics_block(\"TRAIN\", y_tr_orig, pred_tr_orig))\n",
    "print(metrics_block(\"TEST\", y_te_orig, pred_te_orig))\n",
    "\n",
    "imp_df = pd.DataFrame({\n",
    "    'feature': X_cols,\n",
    "    'importance': merf_final.trained_fe_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(12)\n",
    "\n",
    "print(\"\\nTop 12 Drivers (Structural & Health):\")\n",
    "print(imp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2a97684",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(r\"C:\\Users\\liuc\\Desktop\\CH_ECON_V4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f20ab45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'State', 'Year', 'state_total_pop', 'state_emp_avg',\n",
       "       'state_emp_sum', 'state_hires_total', 'state_avg_earnings',\n",
       "       'state_avg_earnings_meanq', 'econ_emp_per_1k', 'econ_hires_per_1k',\n",
       "       'econ_hire_rate_annual', 'growth_emp_yoy', 'growth_earn_yoy',\n",
       "       'growth_hires_yoy', 'STATE_# Alcohol-Impaired Driving Deaths',\n",
       "       'STATE_# Driving Deaths', 'STATE_Premature Deaths',\n",
       "       'STATE_% Adults with Obesity', 'STATE_% Children in Poverty',\n",
       "       'STATE_% Children in Single-Parent Households',\n",
       "       'STATE_% Drive Alone to Work', 'STATE_% Excessive Drinking',\n",
       "       'STATE_% Fair or Poor Health', 'STATE_% Long Commute - Drives Alone',\n",
       "       'STATE_% Low Birthweight', 'STATE_% Severe Housing Problems',\n",
       "       'STATE_% Smokers', 'STATE_% Some College', 'STATE_% Unemployed',\n",
       "       'STATE_% Uninsured', 'STATE_% With Access to Exercise Opportunities',\n",
       "       'STATE_Food Environment Index', 'STATE_Income Ratio',\n",
       "       'STATE_Mentally Unhealthy Days', 'STATE_Physically Unhealthy Days',\n",
       "       'STATE_Preventable Hospitalization Rate',\n",
       "       'STATE_Primary Care Physicians Rate', 'STATE_Social Association Rate',\n",
       "       'STATE_Teen Birth Rate', 'STATE_Violent Crime Rate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29ce960f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual panel rows: 536\n",
      "Years: [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
      "Feature count: 26\n",
      "Using US macro feature: True\n",
      "Train n=294 | Test n=242\n",
      "Train years: [2014, 2015, 2016, 2017, 2018, 2019]\n",
      "Test years : [2020, 2021, 2022, 2023, 2024]\n",
      "Walk-forward tuning folds: 3\n",
      "\n",
      "Starting MERF grid search (annual, walk-forward by year)...\n",
      "Best CV RMSE (model scale): 0.03365\n",
      "Best RF params: {'max_depth': 10, 'max_features': 0.6, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "\n",
      "Fitting final MERF model...\n",
      "\n",
      "--- RESULTS (original scale) ---\n",
      "{'split': 'TRAIN', 'r2': 0.9677903507348995, 'rmse': 7.556997226316377, 'mae': 4.1805998050578745, 'n': 294}\n",
      "{'split': 'TEST', 'r2': 0.569871640779752, 'rmse': 29.431456795827298, 'mae': 22.731281696586546, 'n': 242}\n",
      "\n",
      "Top 15 fixed-effect drivers:\n",
      "                                     feature  importance\n",
      "                                     log_pop    0.253215\n",
      "                             state_total_pop    0.246403\n",
      "                          STATE_% Unemployed    0.184992\n",
      "                             US_hires_per_1k    0.066242\n",
      "                                     t_trend    0.060208\n",
      "               STATE_Social Association Rate    0.029137\n",
      "                          STATE_Income Ratio    0.028247\n",
      "      STATE_Preventable Hospitalization Rate    0.026144\n",
      "                    STATE_Violent Crime Rate    0.021212\n",
      "                     STATE_% Low Birthweight    0.017461\n",
      "          STATE_Primary Care Physicians Rate    0.011935\n",
      "STATE_% Children in Single-Parent Households    0.008076\n",
      "             STATE_% Severe Housing Problems    0.007197\n",
      "                           STATE_% Uninsured    0.006064\n",
      "                        STATE_% Some College    0.005031\n",
      "\n",
      "--- ROLLING-ORIGIN BACKTEST (original scale) ---\n",
      " test_year        r2      rmse       mae  n\n",
      "      2020 -0.014195 32.982217 27.700214 49\n",
      "      2021  0.420095 31.506636 27.877904 49\n",
      "      2022  0.919231 12.633811  8.631140 48\n",
      "      2023  0.878868 13.363748  9.383434 48\n",
      "      2024  0.486526 27.390423 15.173610 48\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from merf.merf import MERF\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "TARGET = \"econ_hires_per_1k\"\n",
    "USE_LOG_TARGET = True\n",
    "\n",
    "START_YEAR = 2014\n",
    "END_YEAR   = 2024\n",
    "TRAIN_END_YEAR = 2019\n",
    "\n",
    "EXCLUDE_STATES = {\"Alaska\"}   # optional\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# MERF / RF tuning and final fit\n",
    "TUNE_N_ESTIMATORS  = 150\n",
    "FINAL_N_ESTIMATORS = 700\n",
    "MERF_TUNE_ITERS    = 5\n",
    "MERF_FINAL_ITERS   = 20\n",
    "\n",
    "# Walk-forward tuning folds within train window (<=2019)\n",
    "MIN_TRAIN_YEARS_TUNE = 3\n",
    "\n",
    "# Optional macro feature (requires state totals and population columns)\n",
    "ADD_US_MACRO = True\n",
    "STATE_HIRES_TOTAL_COL = \"state_hires_total\"\n",
    "STATE_POP_COL         = \"state_total_pop\"\n",
    "\n",
    "# Candidate feature set (annual, levels)\n",
    "CANDIDATE_X = [\n",
    "    # Economic fundamentals (levels)\n",
    "    \"state_avg_earnings\",\n",
    "    \"STATE_% Unemployed\",\n",
    "    \"STATE_% Uninsured\",\n",
    "    \"STATE_% Some College\",\n",
    "    \"state_total_pop\",\n",
    "\n",
    "    # Health & social determinants\n",
    "    \"STATE_% Adults with Obesity\",\n",
    "    \"STATE_% Children in Poverty\",\n",
    "    \"STATE_% Children in Single-Parent Households\",\n",
    "    \"STATE_% Excessive Drinking\",\n",
    "    \"STATE_% Fair or Poor Health\",\n",
    "    \"STATE_% Low Birthweight\",\n",
    "    \"STATE_% Severe Housing Problems\",\n",
    "    \"STATE_% Smokers\",\n",
    "    \"STATE_Food Environment Index\",\n",
    "    \"STATE_Income Ratio\",\n",
    "    \"STATE_Mentally Unhealthy Days\",\n",
    "    \"STATE_Physically Unhealthy Days\",\n",
    "    \"STATE_Preventable Hospitalization Rate\",\n",
    "    \"STATE_Primary Care Physicians Rate\",\n",
    "    \"STATE_Social Association Rate\",\n",
    "    \"STATE_Violent Crime Rate\",\n",
    "]\n",
    "\n",
    "# Leakage columns (drop if present)\n",
    "LEAKAGE_COLS = [\n",
    "    TARGET,\n",
    "    \"econ_hire_rate_annual\",\n",
    "    \"growth_hires_yoy\",\n",
    "    STATE_HIRES_TOTAL_COL,\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# HELPERS\n",
    "# ============================================================\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, \"w\")\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def metrics_block(name, y_true, y_pred):\n",
    "    return {\n",
    "        \"split\": name,\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "        \"rmse\": rmse(y_true, y_pred),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"n\": int(len(y_true)),\n",
    "    }\n",
    "\n",
    "def make_walk_forward_splits_by_year(years: np.ndarray, min_train_years: int = 3):\n",
    "    years = np.asarray(years).astype(int)\n",
    "    uniq = np.sort(np.unique(years))\n",
    "    if len(uniq) < min_train_years + 1:\n",
    "        raise ValueError(\"Not enough unique years for walk-forward folds.\")\n",
    "    splits = []\n",
    "    for i in range(min_train_years, len(uniq)):\n",
    "        tr_years = uniq[:i]\n",
    "        va_year  = uniq[i]\n",
    "        tr_idx = np.where(np.isin(years, tr_years))[0]\n",
    "        va_idx = np.where(years == va_year)[0]\n",
    "        if len(tr_idx) and len(va_idx):\n",
    "            splits.append((tr_idx, va_idx))\n",
    "    if not splits:\n",
    "        raise ValueError(\"No walk-forward splits created.\")\n",
    "    return splits\n",
    "\n",
    "def safe_intersection(cols, df_cols):\n",
    "    return [c for c in cols if c in df_cols]\n",
    "\n",
    "def backtransform_if_needed(y, use_log):\n",
    "    return np.expm1(y) if use_log else y\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) LOAD / CLEAN ANNUAL STATE-YEAR PANEL (df1 must exist)\n",
    "# ============================================================\n",
    "df0 = df1.copy()\n",
    "\n",
    "for c in [\"Unnamed: 0\", \"Unnamed: 0.1\", \"index\"]:\n",
    "    if c in df0.columns:\n",
    "        df0 = df0.drop(columns=[c])\n",
    "\n",
    "if \"State\" not in df0.columns or \"Year\" not in df0.columns:\n",
    "    raise ValueError(\"df1 must contain at least columns: ['State','Year'].\")\n",
    "\n",
    "df0[\"State\"] = df0[\"State\"].astype(str)\n",
    "df0[\"Year\"]  = pd.to_numeric(df0[\"Year\"], errors=\"coerce\")\n",
    "df0 = df0.dropna(subset=[\"State\", \"Year\"]).copy()\n",
    "df0[\"Year\"]  = df0[\"Year\"].astype(int)\n",
    "\n",
    "df0 = df0[(df0[\"Year\"] >= START_YEAR) & (df0[\"Year\"] <= END_YEAR)].copy()\n",
    "\n",
    "if EXCLUDE_STATES:\n",
    "    df0 = df0[~df0[\"State\"].isin(EXCLUDE_STATES)].copy()\n",
    "\n",
    "if TARGET not in df0.columns:\n",
    "    raise ValueError(f\"Target '{TARGET}' not found in df1 columns.\")\n",
    "\n",
    "df0[TARGET] = pd.to_numeric(df0[TARGET], errors=\"coerce\")\n",
    "df0 = df0.dropna(subset=[TARGET]).copy()\n",
    "\n",
    "# Optional US macro feature\n",
    "if ADD_US_MACRO and (STATE_HIRES_TOTAL_COL in df0.columns) and (STATE_POP_COL in df0.columns):\n",
    "    df0[STATE_HIRES_TOTAL_COL] = pd.to_numeric(df0[STATE_HIRES_TOTAL_COL], errors=\"coerce\")\n",
    "    df0[STATE_POP_COL] = pd.to_numeric(df0[STATE_POP_COL], errors=\"coerce\")\n",
    "    us_macro = (\n",
    "        df0.groupby(\"Year\", as_index=False)\n",
    "           .agg(US_hires_sum=(STATE_HIRES_TOTAL_COL, \"sum\"),\n",
    "                US_pop_sum=(STATE_POP_COL, \"sum\"))\n",
    "    )\n",
    "    us_macro[\"US_hires_per_1k\"] = 1000.0 * us_macro[\"US_hires_sum\"] / us_macro[\"US_pop_sum\"].replace({0: np.nan})\n",
    "    df0 = df0.merge(us_macro[[\"Year\", \"US_hires_per_1k\"]], on=\"Year\", how=\"left\")\n",
    "else:\n",
    "    ADD_US_MACRO = False\n",
    "\n",
    "# Time / structural break features\n",
    "df0[\"t_trend\"] = df0[\"Year\"] - df0[\"Year\"].min()\n",
    "df0[\"post_2020\"] = (df0[\"Year\"] >= 2020).astype(int)\n",
    "df0[\"t_post2020\"] = (df0[\"Year\"] - 2020).clip(lower=0)\n",
    "\n",
    "if \"state_total_pop\" in df0.columns:\n",
    "    df0[\"state_total_pop\"] = pd.to_numeric(df0[\"state_total_pop\"], errors=\"coerce\")\n",
    "    df0[\"log_pop\"] = np.log(df0[\"state_total_pop\"].replace({0: np.nan}))\n",
    "\n",
    "REAL_X = safe_intersection(CANDIDATE_X, df0.columns)\n",
    "REAL_X = [c for c in REAL_X if c not in set(LEAKAGE_COLS)]  # avoid accidental leakage\n",
    "\n",
    "extra_feats = [\"t_trend\", \"post_2020\", \"t_post2020\"]\n",
    "if \"log_pop\" in df0.columns and \"log_pop\" not in REAL_X:\n",
    "    extra_feats.append(\"log_pop\")\n",
    "if ADD_US_MACRO:\n",
    "    extra_feats.append(\"US_hires_per_1k\")\n",
    "\n",
    "X_cols = [c for c in (REAL_X + extra_feats) if c in df0.columns]\n",
    "if len(X_cols) == 0:\n",
    "    raise ValueError(\"No usable features found. Check CANDIDATE_X against df1 columns.\")\n",
    "\n",
    "df_model = df0.dropna(subset=[\"State\", \"Year\", TARGET]).copy()\n",
    "\n",
    "if USE_LOG_TARGET:\n",
    "    df_model[f\"{TARGET}_log\"] = np.log1p(df_model[TARGET].astype(float))\n",
    "    target_col = f\"{TARGET}_log\"\n",
    "else:\n",
    "    target_col = TARGET\n",
    "\n",
    "print(\"Annual panel rows:\", df_model.shape[0])\n",
    "print(\"Years:\", sorted(df_model[\"Year\"].unique()))\n",
    "print(\"Feature count:\", len(X_cols))\n",
    "print(\"Using US macro feature:\", ADD_US_MACRO)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) TRAIN/TEST SPLIT\n",
    "# ============================================================\n",
    "df_tr = df_model[df_model[\"Year\"] <= TRAIN_END_YEAR].copy()\n",
    "df_te = df_model[df_model[\"Year\"] >  TRAIN_END_YEAR].copy()\n",
    "\n",
    "# MERF needs test clusters to exist in train\n",
    "train_states = set(df_tr[\"State\"].unique())\n",
    "df_te = df_te[df_te[\"State\"].isin(train_states)].copy()\n",
    "\n",
    "y_tr = df_tr[target_col].to_numpy(dtype=float)\n",
    "y_te = df_te[target_col].to_numpy(dtype=float)\n",
    "\n",
    "# IMPORTANT: MERF requires clusters to be a pandas Series\n",
    "cl_tr = df_tr[\"State\"].astype(str)   # keep as Series (NOT .values)\n",
    "cl_te = df_te[\"State\"].astype(str)\n",
    "\n",
    "Z_tr = np.ones((len(df_tr), 1), dtype=float)\n",
    "Z_te = np.ones((len(df_te), 1), dtype=float)\n",
    "\n",
    "X_tr_raw = df_tr[X_cols].copy()\n",
    "X_te_raw = df_te[X_cols].copy()\n",
    "\n",
    "years_tr = df_tr[\"Year\"].astype(int).to_numpy()\n",
    "wf_splits = make_walk_forward_splits_by_year(years_tr, min_train_years=MIN_TRAIN_YEARS_TUNE)\n",
    "\n",
    "print(f\"Train n={len(df_tr)} | Test n={len(df_te)}\")\n",
    "print(\"Train years:\", sorted(df_tr[\"Year\"].unique()))\n",
    "print(\"Test years :\", sorted(df_te[\"Year\"].unique()))\n",
    "print(\"Walk-forward tuning folds:\", len(wf_splits))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) TUNING (WALK-FORWARD CV within train)\n",
    "# ============================================================\n",
    "print(\"\\nStarting MERF grid search (annual, walk-forward by year)...\")\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [4, 6, 8, 10],\n",
    "    \"min_samples_leaf\": [10, 20, 30],\n",
    "    \"min_samples_split\": [20, 40, 60],\n",
    "    \"max_features\": [0.4, 0.6, \"sqrt\"],\n",
    "}\n",
    "\n",
    "best_rmse = np.inf\n",
    "best_params = None\n",
    "\n",
    "with HiddenPrints():\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        fold_scores = []\n",
    "\n",
    "        for tr_idx, va_idx in wf_splits:\n",
    "            X_f_tr_raw = X_tr_raw.iloc[tr_idx]\n",
    "            y_f_tr     = y_tr[tr_idx]\n",
    "            Z_f_tr     = Z_tr[tr_idx]\n",
    "            cl_f_tr    = cl_tr.iloc[tr_idx]   # Series\n",
    "\n",
    "            X_f_va_raw = X_tr_raw.iloc[va_idx]\n",
    "            y_f_va     = y_tr[va_idx]\n",
    "            Z_f_va     = Z_tr[va_idx]\n",
    "            cl_f_va    = cl_tr.iloc[va_idx]   # Series\n",
    "\n",
    "            imp = SimpleImputer(strategy=\"median\")\n",
    "            X_f_tr = imp.fit_transform(X_f_tr_raw)\n",
    "            X_f_va = imp.transform(X_f_va_raw)\n",
    "\n",
    "            rf = RandomForestRegressor(\n",
    "                n_estimators=TUNE_N_ESTIMATORS,\n",
    "                n_jobs=-1,\n",
    "                random_state=RANDOM_STATE,\n",
    "                **params\n",
    "            )\n",
    "            m = MERF(rf, max_iterations=MERF_TUNE_ITERS)\n",
    "            m.fit(X_f_tr, Z_f_tr, cl_f_tr, y_f_tr)\n",
    "\n",
    "            p_va = m.predict(X_f_va, Z_f_va, cl_f_va)\n",
    "            fold_scores.append(rmse(y_f_va, p_va))\n",
    "\n",
    "        if fold_scores:\n",
    "            avg_score = float(np.mean(fold_scores))\n",
    "            if avg_score < best_rmse:\n",
    "                best_rmse = avg_score\n",
    "                best_params = params\n",
    "\n",
    "print(f\"Best CV RMSE (model scale): {best_rmse:.5f}\")\n",
    "print(f\"Best RF params: {best_params}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) FINAL FIT + EVAL\n",
    "# ============================================================\n",
    "print(\"\\nFitting final MERF model...\")\n",
    "\n",
    "imp_final = SimpleImputer(strategy=\"median\")\n",
    "X_tr = imp_final.fit_transform(X_tr_raw)\n",
    "X_te = imp_final.transform(X_te_raw)\n",
    "\n",
    "rf_final = RandomForestRegressor(\n",
    "    n_estimators=FINAL_N_ESTIMATORS,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    **(best_params if best_params is not None else {})\n",
    ")\n",
    "\n",
    "with HiddenPrints():\n",
    "    merf_final = MERF(rf_final, max_iterations=MERF_FINAL_ITERS)\n",
    "    merf_final.fit(X_tr, Z_tr, cl_tr, y_tr)\n",
    "\n",
    "pred_tr = merf_final.predict(X_tr, Z_tr, cl_tr)\n",
    "pred_te = merf_final.predict(X_te, Z_te, cl_te)\n",
    "\n",
    "y_tr_orig    = backtransform_if_needed(y_tr, USE_LOG_TARGET)\n",
    "y_te_orig    = backtransform_if_needed(y_te, USE_LOG_TARGET)\n",
    "pred_tr_orig = backtransform_if_needed(pred_tr, USE_LOG_TARGET)\n",
    "pred_te_orig = backtransform_if_needed(pred_te, USE_LOG_TARGET)\n",
    "\n",
    "print(\"\\n--- RESULTS (original scale) ---\")\n",
    "print(metrics_block(\"TRAIN\", y_tr_orig, pred_tr_orig))\n",
    "print(metrics_block(\"TEST\",  y_te_orig, pred_te_orig))\n",
    "\n",
    "# Feature importances (fixed effects RF)\n",
    "try:\n",
    "    imp_df = pd.DataFrame({\n",
    "        \"feature\": X_cols,\n",
    "        \"importance\": merf_final.trained_fe_model.feature_importances_\n",
    "    }).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    print(\"\\nTop 15 fixed-effect drivers:\")\n",
    "    print(imp_df.head(15).to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(\"\\nFeature importances not available:\", repr(e))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) OPTIONAL: Rolling-origin backtest (2020..2024)\n",
    "# ============================================================\n",
    "DO_ROLLING_ORIGIN = True\n",
    "ROLLING_WINDOW_YEARS = None   # e.g. 8 for rolling, None for expanding\n",
    "RETUNE_EACH_YEAR = False      # True = slow\n",
    "\n",
    "def rolling_origin_backtest(df_model, start_test_year=2020, end_test_year=2024,\n",
    "                            window_years=None, retune_each_year=False):\n",
    "    rows = []\n",
    "\n",
    "    for test_year in range(start_test_year, end_test_year + 1):\n",
    "        train_year_max = test_year - 1\n",
    "\n",
    "        if window_years is None:\n",
    "            df_tr_y = df_model[df_model[\"Year\"] <= train_year_max].copy()\n",
    "        else:\n",
    "            df_tr_y = df_model[(df_model[\"Year\"] <= train_year_max) &\n",
    "                               (df_model[\"Year\"] >= train_year_max - window_years + 1)].copy()\n",
    "\n",
    "        df_te_y = df_model[df_model[\"Year\"] == test_year].copy()\n",
    "\n",
    "        # Keep only states seen in train\n",
    "        seen = set(df_tr_y[\"State\"].unique())\n",
    "        df_te_y = df_te_y[df_te_y[\"State\"].isin(seen)].copy()\n",
    "\n",
    "        if df_tr_y.empty or df_te_y.empty:\n",
    "            continue\n",
    "\n",
    "        y_tr_y = df_tr_y[target_col].to_numpy(dtype=float)\n",
    "        y_te_y = df_te_y[target_col].to_numpy(dtype=float)\n",
    "\n",
    "        cl_tr_y = df_tr_y[\"State\"].astype(str)  # Series\n",
    "        cl_te_y = df_te_y[\"State\"].astype(str)  # Series\n",
    "\n",
    "        Z_tr_y = np.ones((len(df_tr_y), 1), dtype=float)\n",
    "        Z_te_y = np.ones((len(df_te_y), 1), dtype=float)\n",
    "\n",
    "        X_tr_y_raw = df_tr_y[X_cols].copy()\n",
    "        X_te_y_raw = df_te_y[X_cols].copy()\n",
    "\n",
    "        params_use = best_params\n",
    "\n",
    "        if retune_each_year:\n",
    "            years_tmp = df_tr_y[\"Year\"].astype(int).to_numpy()\n",
    "            uniq_years = np.unique(years_tmp)\n",
    "            if len(uniq_years) >= 4:\n",
    "                splits_tmp = make_walk_forward_splits_by_year(years_tmp, min_train_years=3)\n",
    "                best_rmse_tmp = np.inf\n",
    "                best_params_tmp = None\n",
    "\n",
    "                for params in ParameterGrid(param_grid):\n",
    "                    fold_scores = []\n",
    "                    for tr_idx, va_idx in splits_tmp:\n",
    "                        imp = SimpleImputer(strategy=\"median\")\n",
    "                        X_f_tr = imp.fit_transform(X_tr_y_raw.iloc[tr_idx])\n",
    "                        X_f_va = imp.transform(X_tr_y_raw.iloc[va_idx])\n",
    "\n",
    "                        rf = RandomForestRegressor(\n",
    "                            n_estimators=TUNE_N_ESTIMATORS,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=RANDOM_STATE,\n",
    "                            **params\n",
    "                        )\n",
    "                        m = MERF(rf, max_iterations=MERF_TUNE_ITERS)\n",
    "                        with HiddenPrints():\n",
    "                            m.fit(X_f_tr, Z_tr_y[tr_idx], cl_tr_y.iloc[tr_idx], y_tr_y[tr_idx])\n",
    "                        p_va = m.predict(X_f_va, Z_tr_y[va_idx], cl_tr_y.iloc[va_idx])\n",
    "                        fold_scores.append(rmse(y_tr_y[va_idx], p_va))\n",
    "                    if fold_scores:\n",
    "                        avg = float(np.mean(fold_scores))\n",
    "                        if avg < best_rmse_tmp:\n",
    "                            best_rmse_tmp = avg\n",
    "                            best_params_tmp = params\n",
    "\n",
    "                params_use = best_params_tmp if best_params_tmp is not None else best_params\n",
    "\n",
    "        imp = SimpleImputer(strategy=\"median\")\n",
    "        X_tr_y = imp.fit_transform(X_tr_y_raw)\n",
    "        X_te_y = imp.transform(X_te_y_raw)\n",
    "\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=FINAL_N_ESTIMATORS,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            **(params_use if params_use is not None else {})\n",
    "        )\n",
    "        m = MERF(rf, max_iterations=MERF_FINAL_ITERS)\n",
    "        with HiddenPrints():\n",
    "            m.fit(X_tr_y, Z_tr_y, cl_tr_y, y_tr_y)\n",
    "        p_te_y = m.predict(X_te_y, Z_te_y, cl_te_y)\n",
    "\n",
    "        y_true = backtransform_if_needed(y_te_y, USE_LOG_TARGET)\n",
    "        y_pred = backtransform_if_needed(p_te_y, USE_LOG_TARGET)\n",
    "\n",
    "        rows.append({\n",
    "            \"test_year\": test_year,\n",
    "            **metrics_block(f\"YEAR_{test_year}\", y_true, y_pred)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "if DO_ROLLING_ORIGIN:\n",
    "    bt = rolling_origin_backtest(\n",
    "        df_model=df_model,\n",
    "        start_test_year=2020,\n",
    "        end_test_year=2024,\n",
    "        window_years=ROLLING_WINDOW_YEARS,\n",
    "        retune_each_year=RETUNE_EACH_YEAR\n",
    "    )\n",
    "    print(\"\\n--- ROLLING-ORIGIN BACKTEST (original scale) ---\")\n",
    "    if bt.empty:\n",
    "        print(\"No backtest rows produced (check data coverage / states).\")\n",
    "    else:\n",
    "        print(bt[[\"test_year\", \"r2\", \"rmse\", \"mae\", \"n\"]].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2601e1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature count: 36\n",
      "Total rows for modeling: 539\n",
      "Train size: 297, Test size: 242\n",
      "Train years: [2014, 2015, 2016, 2017, 2018, 2019]\n",
      "Test years: [2020, 2021, 2022, 2023, 2024]\n",
      "folds_tune: 3 | folds_oof: 4\n",
      "\n",
      "====================\n",
      "STAGE 1: BASELINE\n",
      "====================\n",
      "   model  r2_train  rmse_train  mae_train     r2_test  rmse_test  mae_test\n",
      "1    xgb  0.986614    0.015550   0.009010    0.471374   0.109610  0.080481\n",
      "0  lasso  0.785818    0.062200   0.047760   -0.065036   0.155582  0.124222\n",
      "2    mlp -0.837078    0.182164   0.093459 -510.536862   3.409691  3.092839\n",
      "Stacking info (baseline): {'meta_train_kept': 197, 'meta_train_dropped': 100, 'meta_alpha': 0.001}\n",
      "\n",
      "BASELINE STACKED ENSEMBLE (walk-forward OOF Ridge)\n",
      "  Train: R2=0.711, RMSE=0.068, MAE=0.041   (n=197)\n",
      "  Test : R2=0.397, RMSE=0.117, MAE=0.086   (n=242)\n",
      "Baseline ensemble weights:\n",
      " pred_xgb      0.623365\n",
      "pred_lasso    0.310695\n",
      "pred_mlp     -0.000039\n",
      "dtype: float64\n",
      "\n",
      "====================\n",
      "STAGE 2: REGULARIZED / TUNED\n",
      "====================\n",
      "\n",
      "--- Tuning XGBoost (train-only, time-aware CV) ---\n",
      "Best CV RMSE: 0.0455\n",
      "Best params: {'xgb__subsample': 0.9, 'xgb__reg_lambda': 10.0, 'xgb__reg_alpha': 0.01, 'xgb__n_estimators': 500, 'xgb__min_child_weight': 15, 'xgb__max_depth': 5, 'xgb__learning_rate': 0.08, 'xgb__gamma': 0.0, 'xgb__colsample_bytree': 0.6}\n",
      "   model  r2_train  rmse_train  mae_train   r2_test  rmse_test  mae_test\n",
      "1    xgb  0.993555    0.010790   0.004067  0.410001   0.115798  0.084093\n",
      "2     rf  0.579964    0.087105   0.056729  0.236069   0.131766  0.098882\n",
      "0  lasso  0.785602    0.062231   0.047774 -0.064314   0.155529  0.124217\n",
      "Stacking info (regularized): {'meta_train_kept': 197, 'meta_train_dropped': 100, 'meta_alpha': 0.001}\n",
      "\n",
      "REGULARIZED STACKED ENSEMBLE (walk-forward OOF Ridge)\n",
      "  Train: R2=0.746, RMSE=0.064, MAE=0.038   (n=197)\n",
      "  Test : R2=0.409, RMSE=0.116, MAE=0.083   (n=242)\n",
      "Regularized ensemble weights:\n",
      " pred_xgb      0.722615\n",
      "pred_lasso    0.315811\n",
      "pred_rf      -0.243575\n",
      "dtype: float64\n",
      "\n",
      "--- Rolling-origin robustness (tuned XGB; no retuning) ---\n",
      "   test_year  train_n  test_n        r2      rmse       mae\n",
      "0       2018      199      49  0.868843  0.041693  0.033858\n",
      "1       2019      248      49  0.918383  0.034258  0.026385\n",
      "2       2020      297      49  0.662479  0.071660  0.047992\n",
      "3       2021      346      49  0.417358  0.095212  0.078090\n",
      "4       2022      395      48  0.842649  0.052175  0.039616\n",
      "5       2023      443      48  0.406269  0.094597  0.083490\n",
      "6       2024      491      48 -0.236129  0.164870  0.118800\n",
      "\n",
      "--- TEST RESULTS (original scale; back-transformed) ---\n",
      "{'r2': 0.4898966457437657, 'rmse': 32.05101668502368, 'mae': 23.9081915488895}\n",
      "\n",
      "=== FINAL XGB PARAMS TO REPORT ===\n",
      "{'subsample': 0.9, 'reg_lambda': 10.0, 'reg_alpha': 0.01, 'n_estimators': 500, 'min_child_weight': 15, 'max_depth': 5, 'learning_rate': 0.08, 'gamma': 0.0, 'colsample_bytree': 0.6}\n",
      "\n",
      "=== TUNING SUMMARY TO REPORT ===\n",
      "{'target': 'econ_hires_per_1k', 'target_transform': 'log1p', 'tuning_method': 'RandomizedSearchCV', 'cv_design': 'Walk-forward (expanding window) folds by year within training period (<=2019)', 'selection_metric': 'CV RMSE (model scale)', 'best_cv_rmse': 0.045520690346969844, 'n_iter': 50, 'stacking_note': 'Meta-learner trained on walk-forward OOF predictions; earliest year(s) have no prior data and are excluded.'}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STATE-YEAR HIRING MODEL (NON-MERF)\n",
    "#  - Target: econ_hires_per_1k (annual hires per 1k population)\n",
    "#  - Walk-forward CV within train (<=2019)\n",
    "#  - Baselines: LASSO / XGB / MLP\n",
    "#  - Regularized/Tuned: LASSO / tuned XGB / regularized RF\n",
    "#  - Stacking: OOF walk-forward Ridge meta-learner (leakage-safe)\n",
    "#  - Optional rolling-origin check\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LassoCV, Lasso, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helpers\n",
    "# ------------------------------------------------------------\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def eval_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "        \"rmse\": root_mean_squared_error(y_true, y_pred),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "def print_perf(title, y_train, pred_train, y_test, pred_test, train_mask=None):\n",
    "    \"\"\"\n",
    "    train_mask: optional boolean mask for selecting valid rows (e.g., for OOF-stacked train preds).\n",
    "    \"\"\"\n",
    "    if train_mask is None:\n",
    "        train_mask = np.ones(len(y_train), dtype=bool)\n",
    "\n",
    "    tr = eval_metrics(y_train[train_mask], pred_train[train_mask])\n",
    "    te = eval_metrics(y_test, pred_test)\n",
    "\n",
    "    print(f\"\\n{title}\")\n",
    "    print(f\"  Train: R2={tr['r2']:.3f}, RMSE={tr['rmse']:.3f}, MAE={tr['mae']:.3f}   (n={int(train_mask.sum())})\")\n",
    "    print(f\"  Test : R2={te['r2']:.3f}, RMSE={te['rmse']:.3f}, MAE={te['mae']:.3f}   (n={len(y_test)})\")\n",
    "\n",
    "    return tr, te\n",
    "\n",
    "def make_walk_forward_folds(years: np.ndarray, min_train_years: int = 1):\n",
    "    \"\"\"\n",
    "    Walk-forward folds by YEAR.\n",
    "\n",
    "    For years 2014..2019:\n",
    "      min_train_years=1 -> 2014->2015, 2014-15->2016, ..., 2014-18->2019\n",
    "      min_train_years=3 -> 2014-16->2017, 2014-17->2018, 2014-18->2019\n",
    "    \"\"\"\n",
    "    years = np.asarray(years).astype(int)\n",
    "    uniq = np.sort(np.unique(years))\n",
    "    if len(uniq) < (min_train_years + 1):\n",
    "        raise ValueError(\"Not enough unique years for walk-forward folds.\")\n",
    "\n",
    "    folds = []\n",
    "    for i in range(min_train_years, len(uniq)):\n",
    "        tr_years = uniq[:i]\n",
    "        va_year = uniq[i]\n",
    "        tr_idx = np.where(np.isin(years, tr_years))[0]\n",
    "        va_idx = np.where(years == va_year)[0]\n",
    "        if len(tr_idx) and len(va_idx):\n",
    "            folds.append((tr_idx, va_idx))\n",
    "\n",
    "    if not folds:\n",
    "        raise ValueError(\"No folds created.\")\n",
    "    return folds\n",
    "\n",
    "def oof_preds_walk_forward(model_fixed, X_train, y_train, folds):\n",
    "    \"\"\"\n",
    "    Returns OOF predictions length len(y_train), NaN for rows never in validation (usually first year).\n",
    "    model_fixed must NOT include internal CV with global folds.\n",
    "    \"\"\"\n",
    "    y_train = np.asarray(y_train)\n",
    "    oof = np.full(len(y_train), np.nan, dtype=float)\n",
    "\n",
    "    for tr_idx, va_idx in folds:\n",
    "        m = clone(model_fixed)\n",
    "        m.fit(X_train.iloc[tr_idx], y_train[tr_idx])\n",
    "        oof[va_idx] = m.predict(X_train.iloc[va_idx])\n",
    "\n",
    "    return oof\n",
    "\n",
    "def stack_with_oof_walk_forward(base_models_fixed, X_train, y_train, X_test, folds_oof, meta_alphas=None):\n",
    "    \"\"\"\n",
    "    Stacking:\n",
    "      - meta-train uses walk-forward OOF predictions (true out-of-sample)\n",
    "      - meta-learner fit only on rows where all OOF preds exist (drops earliest year)\n",
    "      - meta-test uses base predictions trained on full training\n",
    "    \"\"\"\n",
    "    if meta_alphas is None:\n",
    "        meta_alphas = np.logspace(-3, 3, 13)\n",
    "\n",
    "    y_train = np.asarray(y_train)\n",
    "\n",
    "    meta_train = np.column_stack([\n",
    "        oof_preds_walk_forward(m, X_train, y_train, folds_oof)\n",
    "        for m in base_models_fixed.values()\n",
    "    ])\n",
    "\n",
    "    valid = np.all(np.isfinite(meta_train), axis=1)\n",
    "    dropped = int((~valid).sum())\n",
    "    kept = int(valid.sum())\n",
    "\n",
    "    if kept < 30:\n",
    "        raise RuntimeError(f\"Too few valid OOF rows for meta-learner: kept={kept}, dropped={dropped}\")\n",
    "\n",
    "    # Fit base models on full training for meta-test\n",
    "    meta_test_cols = []\n",
    "    for m in base_models_fixed.values():\n",
    "        mf = clone(m)\n",
    "        mf.fit(X_train, y_train)\n",
    "        meta_test_cols.append(mf.predict(X_test))\n",
    "    meta_test = np.column_stack(meta_test_cols)\n",
    "\n",
    "    # Meta-learner (RidgeCV)\n",
    "    meta = RidgeCV(alphas=meta_alphas, cv=5)\n",
    "    meta.fit(meta_train[valid], y_train[valid])\n",
    "\n",
    "    ens_train_pred = np.full(len(y_train), np.nan, dtype=float)\n",
    "    ens_train_pred[valid] = meta.predict(meta_train[valid])\n",
    "    ens_test_pred = meta.predict(meta_test)\n",
    "\n",
    "    weights = pd.Series(meta.coef_, index=[f\"pred_{k}\" for k in base_models_fixed.keys()])\n",
    "    info = {\"meta_train_kept\": kept, \"meta_train_dropped\": dropped, \"meta_alpha\": float(meta.alpha_)}\n",
    "    return ens_train_pred, ens_test_pred, valid, weights, info\n",
    "\n",
    "def fit_eval_models(models, X_train, y_train, X_test, y_test):\n",
    "    rows = []\n",
    "    fitted = {}\n",
    "    for name, model in models.items():\n",
    "        m = clone(model)\n",
    "        m.fit(X_train, y_train)\n",
    "        fitted[name] = m\n",
    "\n",
    "        pred_tr = m.predict(X_train)\n",
    "        pred_te = m.predict(X_test)\n",
    "\n",
    "        tr = eval_metrics(y_train, pred_tr)\n",
    "        te = eval_metrics(y_test, pred_te)\n",
    "\n",
    "        rows.append({\n",
    "            \"model\": name,\n",
    "            \"r2_train\": tr[\"r2\"], \"rmse_train\": tr[\"rmse\"], \"mae_train\": tr[\"mae\"],\n",
    "            \"r2_test\": te[\"r2\"], \"rmse_test\": te[\"rmse\"], \"mae_test\": te[\"mae\"],\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values(\"r2_test\", ascending=False), fitted\n",
    "\n",
    "def rolling_origin_eval(model, df_model, X_all, y_all, start_train_end=2017, end_year=2024):\n",
    "    years = df_model[\"Year\"].astype(int).values\n",
    "    out = []\n",
    "    for test_year in range(start_train_end + 1, end_year + 1):\n",
    "        tr_mask = years <= (test_year - 1)\n",
    "        te_mask = years == test_year\n",
    "        if tr_mask.sum() < 30 or te_mask.sum() < 10:\n",
    "            continue\n",
    "        m = clone(model)\n",
    "        m.fit(X_all.loc[tr_mask], y_all[tr_mask])\n",
    "        pred = m.predict(X_all.loc[te_mask])\n",
    "        met = eval_metrics(y_all[te_mask], pred)\n",
    "        out.append({\"test_year\": test_year, \"train_n\": int(tr_mask.sum()), \"test_n\": int(te_mask.sum()), **met})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0) LOAD / CLEAN DATA (df1 already loaded by you)\n",
    "# ============================================================\n",
    "df0 = df1.copy()\n",
    "\n",
    "# drop index-like columns\n",
    "for c in [\"Unnamed: 0\", \"Unnamed: 0.1\"]:\n",
    "    if c in df0.columns:\n",
    "        df0 = df0.drop(columns=[c])\n",
    "\n",
    "df0[\"State\"] = df0[\"State\"].astype(str)\n",
    "df0[\"Year\"] = pd.to_numeric(df0[\"Year\"], errors=\"coerce\")\n",
    "df0 = df0.dropna(subset=[\"State\", \"Year\"]).copy()\n",
    "df0[\"Year\"] = df0[\"Year\"].astype(int)\n",
    "\n",
    "# Year window\n",
    "df0 = df0[(df0[\"Year\"] >= 2014) & (df0[\"Year\"] <= 2024)].copy()\n",
    "\n",
    "# Target (annual hires per 1k)\n",
    "target = \"econ_hires_per_1k\"\n",
    "if target not in df0.columns:\n",
    "    raise ValueError(f\"Target '{target}' not found in df1 columns.\")\n",
    "\n",
    "df0[target] = pd.to_numeric(df0[target], errors=\"coerce\")\n",
    "df_model = df0[np.isfinite(df0[target])].copy()\n",
    "\n",
    "# Optional: log target for heavy tails/outliers\n",
    "USE_LOG_TARGET = True\n",
    "y_raw = df_model[target].values.astype(float)\n",
    "y = np.log1p(y_raw) if USE_LOG_TARGET else y_raw\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) BUILD LEAKAGE-SAFE FEATURE MATRIX X\n",
    "# ============================================================\n",
    "id_cols = [\"State\", \"Year\"]\n",
    "\n",
    "# Anything that directly contains hires or is computed from hires should be excluded\n",
    "# (otherwise the model \"cheats\" and looks great in train)\n",
    "hire_leakage = [\n",
    "    \"state_hires_total\",        # target numerator in most constructions\n",
    "    \"econ_hire_rate_annual\",    # uses hires/employment\n",
    "    \"growth_hires_yoy\",         # computed from hires series\n",
    "]\n",
    "\n",
    "# Also exclude the raw target itself\n",
    "exclude_cols = set(id_cols + hire_leakage + [target])\n",
    "\n",
    "X = df_model.drop(columns=[c for c in exclude_cols if c in df_model.columns], errors=\"ignore\")\n",
    "\n",
    "# numeric only\n",
    "X = X.select_dtypes(include=[np.number]).copy()\n",
    "X = X.dropna(axis=1, how=\"all\")\n",
    "X = X.loc[:, X.nunique(dropna=True) > 1]\n",
    "\n",
    "# Small stabilizers that often help annual models:\n",
    "# (if these exist already, skip)\n",
    "if \"t_trend\" not in X.columns:\n",
    "    X[\"t_trend\"] = df_model[\"Year\"].astype(int) - df_model[\"Year\"].astype(int).min()\n",
    "if \"log_pop\" not in X.columns and \"state_total_pop\" in df_model.columns:\n",
    "    X[\"log_pop\"] = np.log(df_model[\"state_total_pop\"].replace({0: np.nan}))\n",
    "\n",
    "print(\"Final feature count:\", X.shape[1])\n",
    "print(\"Total rows for modeling:\", X.shape[0])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) TIME SPLIT + FOLDS\n",
    "# ============================================================\n",
    "split_year = 2019\n",
    "train_mask = df_model[\"Year\"] <= split_year\n",
    "test_mask  = df_model[\"Year\"] >  split_year\n",
    "\n",
    "X_train, X_test = X.loc[train_mask].copy(), X.loc[test_mask].copy()\n",
    "y_train, y_test = y[train_mask.values], y[test_mask.values]\n",
    "\n",
    "years_train = df_model.loc[train_mask, \"Year\"].astype(int).values\n",
    "\n",
    "folds_tune = make_walk_forward_folds(years_train, min_train_years=3)\n",
    "folds_oof  = make_walk_forward_folds(years_train, min_train_years=2)\n",
    "\n",
    "print(f\"Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}\")\n",
    "print(\"Train years:\", sorted(df_model.loc[train_mask, \"Year\"].unique()))\n",
    "print(\"Test years:\", sorted(df_model.loc[test_mask, \"Year\"].unique()))\n",
    "print(\"folds_tune:\", len(folds_tune), \"| folds_oof:\", len(folds_oof))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STAGE 1: BASELINE\n",
    "# ============================================================\n",
    "print(\"\\n====================\")\n",
    "print(\"STAGE 1: BASELINE\")\n",
    "print(\"====================\")\n",
    "\n",
    "lasso_cv_base = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lasso_cv\", LassoCV(cv=folds_tune, random_state=42, n_alphas=150, max_iter=100000)),\n",
    "])\n",
    "\n",
    "xgb_base = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"xgb\", xgb.XGBRegressor(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.04,\n",
    "        max_depth=3,                 # shallower for annual + generalization\n",
    "        min_child_weight=10,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=5.0,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"rmse\",\n",
    "    ))\n",
    "])\n",
    "\n",
    "mlp_base = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"mlp\", MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation=\"relu\",\n",
    "        alpha=5e-3,                  # more regularization for small annual sample\n",
    "        learning_rate_init=1e-3,\n",
    "        max_iter=3000,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.20,\n",
    "        n_iter_no_change=30,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "baseline_models = {\"lasso\": lasso_cv_base, \"xgb\": xgb_base, \"mlp\": mlp_base}\n",
    "base_perf, _ = fit_eval_models(baseline_models, X_train, y_train, X_test, y_test)\n",
    "print(base_perf)\n",
    "\n",
    "# --- Stacking baseline (need FIXED LASSO; no internal CV)\n",
    "lasso_cv_base.fit(X_train, y_train)\n",
    "alpha_base = lasso_cv_base.named_steps[\"lasso_cv\"].alpha_\n",
    "\n",
    "lasso_fixed_base = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lasso\", Lasso(alpha=alpha_base, max_iter=200000, random_state=42)),\n",
    "])\n",
    "\n",
    "baseline_stack_fixed = {\"lasso\": lasso_fixed_base, \"xgb\": xgb_base, \"mlp\": mlp_base}\n",
    "\n",
    "ens_tr, ens_te, valid_tr, w, info = stack_with_oof_walk_forward(\n",
    "    baseline_stack_fixed, X_train, y_train, X_test, folds_oof\n",
    ")\n",
    "\n",
    "print(\"Stacking info (baseline):\", info)\n",
    "print_perf(\"BASELINE STACKED ENSEMBLE (walk-forward OOF Ridge)\",\n",
    "           y_train, ens_tr, y_test, ens_te, train_mask=valid_tr)\n",
    "print(\"Baseline ensemble weights:\\n\", w.sort_values(ascending=False))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STAGE 2: REGULARIZED / TUNED\n",
    "# ============================================================\n",
    "print(\"\\n====================\")\n",
    "print(\"STAGE 2: REGULARIZED / TUNED\")\n",
    "print(\"====================\")\n",
    "\n",
    "lasso_cv_reg = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lasso_cv\", LassoCV(cv=folds_tune, random_state=42, n_alphas=250, max_iter=200000)),\n",
    "])\n",
    "\n",
    "rf_reg = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"rf\", RandomForestRegressor(\n",
    "        n_estimators=800,\n",
    "        max_depth=4,               # conservative depth to reduce overfit\n",
    "        min_samples_leaf=20,\n",
    "        min_samples_split=40,\n",
    "        max_features=0.5,\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "xgb_tune_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"xgb\", xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"rmse\",\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_dist = {\n",
    "    \"xgb__n_estimators\": np.arange(200, 1201, 100),\n",
    "    \"xgb__learning_rate\": np.array([0.01, 0.02, 0.03, 0.05, 0.08]),\n",
    "    \"xgb__max_depth\": np.arange(2, 6),\n",
    "    \"xgb__min_child_weight\": np.array([5, 10, 15, 20]),\n",
    "    \"xgb__gamma\": np.array([0.0, 0.1, 0.5, 1.0, 2.0]),\n",
    "    \"xgb__subsample\": np.array([0.6, 0.7, 0.8, 0.9]),\n",
    "    \"xgb__colsample_bytree\": np.array([0.6, 0.7, 0.8, 0.9]),\n",
    "    \"xgb__reg_alpha\": np.array([0.0, 0.01, 0.05, 0.1, 0.5, 1.0]),\n",
    "    \"xgb__reg_lambda\": np.array([1.0, 2.0, 5.0, 10.0, 20.0]),\n",
    "}\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    xgb_tune_pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=folds_tune,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n--- Tuning XGBoost (train-only, time-aware CV) ---\")\n",
    "xgb_search.fit(X_train, y_train)\n",
    "xgb_best = xgb_search.best_estimator_\n",
    "best_params = xgb_search.best_params_\n",
    "best_cv_rmse = -xgb_search.best_score_\n",
    "\n",
    "print(\"Best CV RMSE:\", round(best_cv_rmse, 4))\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "reg_models = {\"lasso\": lasso_cv_reg, \"xgb\": xgb_best, \"rf\": rf_reg}\n",
    "reg_perf, _ = fit_eval_models(reg_models, X_train, y_train, X_test, y_test)\n",
    "print(reg_perf)\n",
    "\n",
    "# --- Regularized stacking: fixed LASSO + tuned XGB + RF\n",
    "lasso_cv_reg.fit(X_train, y_train)\n",
    "alpha_reg = lasso_cv_reg.named_steps[\"lasso_cv\"].alpha_\n",
    "\n",
    "lasso_fixed_reg = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lasso\", Lasso(alpha=alpha_reg, max_iter=200000, random_state=42)),\n",
    "])\n",
    "\n",
    "reg_stack_fixed = {\"lasso\": lasso_fixed_reg, \"xgb\": xgb_best, \"rf\": rf_reg}\n",
    "\n",
    "ens_tr2, ens_te2, valid_tr2, w2, info2 = stack_with_oof_walk_forward(\n",
    "    reg_stack_fixed, X_train, y_train, X_test, folds_oof\n",
    ")\n",
    "\n",
    "print(\"Stacking info (regularized):\", info2)\n",
    "print_perf(\"REGULARIZED STACKED ENSEMBLE (walk-forward OOF Ridge)\",\n",
    "           y_train, ens_tr2, y_test, ens_te2, train_mask=valid_tr2)\n",
    "print(\"Regularized ensemble weights:\\n\", w2.sort_values(ascending=False))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# OPTIONAL: Rolling-origin robustness check (tuned XGB; no retuning)\n",
    "# ============================================================\n",
    "print(\"\\n--- Rolling-origin robustness (tuned XGB; no retuning) ---\")\n",
    "roll = rolling_origin_eval(\n",
    "    model=xgb_best,\n",
    "    df_model=df_model,\n",
    "    X_all=X,\n",
    "    y_all=y,\n",
    "    start_train_end=2017,\n",
    "    end_year=2024\n",
    ")\n",
    "print(roll)\n",
    "\n",
    "# ============================================================\n",
    "# Back-transform metrics to original scale if log1p was used\n",
    "# ============================================================\n",
    "if USE_LOG_TARGET:\n",
    "    # Use the regularized stacked model for reporting (common choice)\n",
    "    # Train preds are OOF only where valid_tr2, so back-transform those slices.\n",
    "    y_train_orig = np.expm1(y_train)\n",
    "    y_test_orig  = np.expm1(y_test)\n",
    "    pred_test_orig = np.expm1(ens_te2)\n",
    "\n",
    "    print(\"\\n--- TEST RESULTS (original scale; back-transformed) ---\")\n",
    "    print(eval_metrics(y_test_orig, pred_test_orig))\n",
    "\n",
    "# ============================================================\n",
    "# Print what to report\n",
    "# ============================================================\n",
    "final_xgb_params_to_report = {k.replace(\"xgb__\", \"\"): v for k, v in best_params.items()}\n",
    "print(\"\\n=== FINAL XGB PARAMS TO REPORT ===\")\n",
    "print(final_xgb_params_to_report)\n",
    "\n",
    "print(\"\\n=== TUNING SUMMARY TO REPORT ===\")\n",
    "print({\n",
    "    \"target\": target,\n",
    "    \"target_transform\": \"log1p\" if USE_LOG_TARGET else \"none\",\n",
    "    \"tuning_method\": \"RandomizedSearchCV\",\n",
    "    \"cv_design\": \"Walk-forward (expanding window) folds by year within training period (<=2019)\",\n",
    "    \"selection_metric\": \"CV RMSE (model scale)\",\n",
    "    \"best_cv_rmse\": float(best_cv_rmse),\n",
    "    \"n_iter\": 50,\n",
    "    \"stacking_note\": \"Meta-learner trained on walk-forward OOF predictions; earliest year(s) have no prior data and are excluded.\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63319eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after lag requirements: 487\n",
      "Years in model: [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
      "Feature count: 26\n",
      "Train n=245 | Test n=242\n",
      "Train years: [2015, 2016, 2017, 2018, 2019]\n",
      "Test years : [2020, 2021, 2022, 2023, 2024]\n",
      "Walk-forward tuning folds: 2\n",
      "\n",
      "====================\n",
      "BASELINES (original scale)\n",
      "====================\n",
      "{'split': 'TEST | Persistence hires(t-1)', 'r2': 0.42152767523947987, 'rmse': 34.131393024209736, 'mae': 26.911492517779255, 'n': 242}\n",
      "{'split': 'TEST | State pre-avg rel + US', 'r2': 0.8095372012016702, 'rmse': 19.58473960135387, 'mae': 13.466087818137826, 'n': 242}\n",
      "\n",
      "====================\n",
      "TUNING MERF (walk-forward CV on y_rel)\n",
      "====================\n",
      "Best CV RMSE (y_rel scale): 0.03827\n",
      "Best RF params: {'max_depth': 6, 'max_features': 0.6, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "\n",
      "====================\n",
      "FINAL MERF FIT + EVAL (report on original scale)\n",
      "====================\n",
      "{'split': 'TRAIN | MERF(rel->abs)', 'r2': 0.9912707683666939, 'rmse': 3.6881456056286805, 'mae': 2.319628862930542, 'n': 245}\n",
      "{'split': 'TEST  | MERF(rel->abs)', 'r2': 0.5558472638393466, 'rmse': 29.90741599493102, 'mae': 17.394599847846898, 'n': 242}\n",
      "\n",
      "Top 20 fixed-effect drivers (RF in MERF):\n",
      "                           feature  importance\n",
      "            econ_hires_per_1k_lag1    0.691450\n",
      "            econ_hires_per_1k_lag2    0.187176\n",
      "              econ_emp_per_1k_lag1    0.034773\n",
      "     STATE_Social Association Rate    0.012355\n",
      "            d_econ_emp_per_1k_lag1    0.009482\n",
      "         d_state_avg_earnings_lag1    0.009272\n",
      "              econ_emp_per_1k_lag2    0.007878\n",
      "                STATE_Income Ratio    0.005461\n",
      "      STATE_Food Environment Index    0.004108\n",
      "       STATE_% Fair or Poor Health    0.004032\n",
      "        STATE_% Excessive Drinking    0.003898\n",
      "           STATE_% Unemployed_lag1    0.003144\n",
      "STATE_Primary Care Physicians Rate    0.002944\n",
      "       STATE_% Children in Poverty    0.002877\n",
      "   STATE_Physically Unhealthy Days    0.002785\n",
      "          STATE_Violent Crime Rate    0.002368\n",
      "                   US_hires_per_1k    0.002302\n",
      "           STATE_% Unemployed_lag2    0.002213\n",
      "           state_avg_earnings_lag1    0.002072\n",
      "     STATE_Mentally Unhealthy Days    0.002059\n",
      "\n",
      "====================\n",
      "ROLLING-ORIGIN BACKTEST (2020..2024) | MERF(rel->abs)\n",
      "====================\n",
      " test_year        r2      rmse       mae  n\n",
      "      2020 -0.201794 35.903274 17.911505 49\n",
      "      2021  0.827093 17.203995 13.854050 49\n",
      "      2022  0.958521  9.053693  7.325059 48\n",
      "      2023  0.702231 20.952606 12.381674 48\n",
      "      2024  0.581424 24.730139 15.014089 48\n",
      "\n",
      "Case-study table (head):\n",
      "  State  Year  econ_hires_per_1k  US_hires_per_1k  y_rel_true  y_rel_pred  hires_pred_per_1k  excess_hires_per_1k_vs_US\n",
      "Alabama  2020         260.083956       261.982000   -0.007244   -0.102096         236.457831                 -25.524169\n",
      "Alabama  2021         306.208949       310.287935   -0.013190    0.015111         315.027385                   4.739449\n",
      "Alabama  2022         308.510276       314.272671   -0.018447    0.083842         341.845631                  27.572960\n",
      "Alabama  2023         287.250084       281.574271    0.019887    0.010391         284.525694                   2.951423\n",
      "Alabama  2024         260.303480       257.868318    0.009363   -0.065880         241.363773                 -16.504545\n",
      "Arizona  2020         272.395607       261.982000    0.038834   -0.055898         247.685232                 -14.296768\n",
      "Arizona  2021         307.123597       310.287935   -0.010217   -0.080543         286.198918                 -24.089017\n",
      "Arizona  2022         310.374858       314.272671   -0.012440   -0.041064         301.588516                 -12.684155\n",
      "Arizona  2023         282.451944       281.574271    0.003101   -0.029577         273.339021                  -8.235250\n",
      "Arizona  2024         271.679197       257.868318    0.051976   -0.053231         244.448934                 -13.419383\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STATE-YEAR HIRING MODEL (Aâ€“E IMPLEMENTATION)\n",
    "#  A) Add lag1/lag2 (incl. lagged target) - NOT leakage if built via groupby shift\n",
    "#  B) Model relative-to-US: y_rel = log1p(state_hires_per_1k) - log1p(US_hires_per_1k)\n",
    "#  C) MERF with random slopes (Z includes intercept + US macro + time trend)\n",
    "#  D) Bring in stronger economic predictors (lagged emp_per_1k, unemp, earnings, etc.)\n",
    "#  E) Stop chasing stacking; compare against strong baselines (persistence + state pre-avg rel)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from merf.merf import MERF\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "START_YEAR = 2014\n",
    "END_YEAR   = 2024\n",
    "TRAIN_END_YEAR = 2019\n",
    "\n",
    "EXCLUDE_STATES = {\"Alaska\"}   # optional\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Raw target (state hires per 1k)\n",
    "TARGET_RAW = \"econ_hires_per_1k\"\n",
    "\n",
    "# Needed to compute US_hires_per_1k\n",
    "STATE_HIRES_TOTAL_COL = \"state_hires_total\"\n",
    "STATE_POP_COL         = \"state_total_pop\"\n",
    "\n",
    "# Lags\n",
    "LAGS = [1, 2]  # \"at most lag 2\" per your request\n",
    "\n",
    "# MERF / RF tuning and final fit\n",
    "TUNE_N_ESTIMATORS  = 200\n",
    "FINAL_N_ESTIMATORS = 900\n",
    "MERF_TUNE_ITERS    = 6\n",
    "MERF_FINAL_ITERS   = 25\n",
    "\n",
    "# Walk-forward tuning folds within train window (<=2019)\n",
    "MIN_TRAIN_YEARS_TUNE = 3\n",
    "\n",
    "# If you want a stricter \"forecasting\" stance for slow variables,\n",
    "# set USE_LAGGED_SLOW_VARS=True to only use lagged versions of health/social vars.\n",
    "USE_LAGGED_SLOW_VARS = False\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# HELPERS\n",
    "# ============================================================\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, \"w\")\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        try:\n",
    "            sys.stdout.close()\n",
    "        finally:\n",
    "            sys.stdout = self._original_stdout\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def metrics_block(name, y_true, y_pred):\n",
    "    return {\n",
    "        \"split\": name,\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "        \"rmse\": rmse(y_true, y_pred),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"n\": int(len(y_true)),\n",
    "    }\n",
    "\n",
    "def make_walk_forward_splits_by_year(years: np.ndarray, min_train_years: int = 3):\n",
    "    years = np.asarray(years).astype(int)\n",
    "    uniq = np.sort(np.unique(years))\n",
    "    if len(uniq) < min_train_years + 1:\n",
    "        raise ValueError(\"Not enough unique years for walk-forward folds.\")\n",
    "    splits = []\n",
    "    for i in range(min_train_years, len(uniq)):\n",
    "        tr_years = uniq[:i]\n",
    "        va_year  = uniq[i]\n",
    "        tr_idx = np.where(np.isin(years, tr_years))[0]\n",
    "        va_idx = np.where(years == va_year)[0]\n",
    "        if len(tr_idx) and len(va_idx):\n",
    "            splits.append((tr_idx, va_idx))\n",
    "    if not splits:\n",
    "        raise ValueError(\"No walk-forward splits created.\")\n",
    "    return splits\n",
    "\n",
    "def safe_numeric(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def add_us_macro(df):\n",
    "    \"\"\"\n",
    "    Compute US_hires_per_1k by year from state totals.\n",
    "    \"\"\"\n",
    "    if (STATE_HIRES_TOTAL_COL not in df.columns) or (STATE_POP_COL not in df.columns):\n",
    "        raise ValueError(\"Missing columns for US macro: state_hires_total and/or state_total_pop.\")\n",
    "\n",
    "    df = safe_numeric(df, [STATE_HIRES_TOTAL_COL, STATE_POP_COL])\n",
    "    us_macro = (\n",
    "        df.groupby(\"Year\", as_index=False)\n",
    "          .agg(US_hires_sum=(STATE_HIRES_TOTAL_COL, \"sum\"),\n",
    "               US_pop_sum=(STATE_POP_COL, \"sum\"))\n",
    "    )\n",
    "    us_macro[\"US_hires_per_1k\"] = 1000.0 * us_macro[\"US_hires_sum\"] / us_macro[\"US_pop_sum\"].replace({0: np.nan})\n",
    "    return df.merge(us_macro[[\"Year\", \"US_hires_per_1k\"]], on=\"Year\", how=\"left\")\n",
    "\n",
    "def add_lags_by_state(df, cols, lags=(1,2)):\n",
    "    \"\"\"\n",
    "    Create lag features strictly within each State, by Year order.\n",
    "    \"\"\"\n",
    "    df = df.sort_values([\"State\", \"Year\"]).copy()\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        for L in lags:\n",
    "            df[f\"{col}_lag{L}\"] = df.groupby(\"State\")[col].shift(L)\n",
    "    return df\n",
    "\n",
    "def reconstruct_hires_from_rel(pred_y_rel, us_hires_per_1k):\n",
    "    \"\"\"\n",
    "    y_rel = log1p(state_hires_per_1k) - log1p(US_hires_per_1k)\n",
    "    => log1p(state_hires) = y_rel + log1p(US_hires)\n",
    "    => state_hires = expm1(...)\n",
    "    \"\"\"\n",
    "    pred_log1p_hires = pred_y_rel + np.log1p(us_hires_per_1k)\n",
    "    return np.expm1(pred_log1p_hires)\n",
    "\n",
    "def compute_y_rel(df):\n",
    "    \"\"\"\n",
    "    y_rel = log1p(state_hires_per_1k) - log1p(US_hires_per_1k)\n",
    "    \"\"\"\n",
    "    y_rel = np.log1p(df[TARGET_RAW].astype(float)) - np.log1p(df[\"US_hires_per_1k\"].astype(float))\n",
    "    return y_rel\n",
    "\n",
    "def baseline_persistence(df, kind=\"absolute\"):\n",
    "    \"\"\"\n",
    "    kind=\"absolute\": predict hires_t = hires_{t-1}\n",
    "    kind=\"relative\": predict y_rel_t = y_rel_{t-1} (using lagged y_rel)\n",
    "    \"\"\"\n",
    "    if kind == \"absolute\":\n",
    "        return df[f\"{TARGET_RAW}_lag1\"].to_numpy(dtype=float)\n",
    "    elif kind == \"relative\":\n",
    "        # build y_rel lag1 from its components:\n",
    "        # y_rel_lag1 = log1p(hires_lag1) - log1p(US_hires_lag1)\n",
    "        y_rel_lag1 = np.log1p(df[f\"{TARGET_RAW}_lag1\"].astype(float)) - np.log1p(df[\"US_hires_per_1k_lag1\"].astype(float))\n",
    "        return y_rel_lag1.to_numpy(dtype=float)\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'absolute' or 'relative'\")\n",
    "\n",
    "def baseline_state_preavg_rel(df_train, df_any):\n",
    "    \"\"\"\n",
    "    Baseline: for each state, use its average y_rel in training period (<= TRAIN_END_YEAR),\n",
    "    then reconstruct hires with that constant rel + current US_hires_per_1k.\n",
    "    \"\"\"\n",
    "    tmp = df_train.copy()\n",
    "    tmp[\"y_rel\"] = compute_y_rel(tmp)\n",
    "    state_rel_mean = tmp.groupby(\"State\")[\"y_rel\"].mean()\n",
    "\n",
    "    rel_hat = df_any[\"State\"].map(state_rel_mean).to_numpy(dtype=float)\n",
    "    # if any state missing (shouldn't), fall back to overall mean\n",
    "    if np.any(~np.isfinite(rel_hat)):\n",
    "        rel_hat[~np.isfinite(rel_hat)] = float(np.nanmean(rel_hat))\n",
    "    return rel_hat\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) LOAD / CLEAN\n",
    "# ============================================================\n",
    "df0 = df1.copy()\n",
    "\n",
    "# drop index-like columns\n",
    "for c in [\"Unnamed: 0\", \"Unnamed: 0.1\", \"index\"]:\n",
    "    if c in df0.columns:\n",
    "        df0 = df0.drop(columns=[c])\n",
    "\n",
    "# basic checks\n",
    "if \"State\" not in df0.columns or \"Year\" not in df0.columns:\n",
    "    raise ValueError(\"df1 must contain at least columns: ['State','Year'].\")\n",
    "\n",
    "df0[\"State\"] = df0[\"State\"].astype(str)\n",
    "df0[\"Year\"]  = pd.to_numeric(df0[\"Year\"], errors=\"coerce\")\n",
    "df0 = df0.dropna(subset=[\"State\", \"Year\"]).copy()\n",
    "df0[\"Year\"]  = df0[\"Year\"].astype(int)\n",
    "\n",
    "df0 = df0[(df0[\"Year\"] >= START_YEAR) & (df0[\"Year\"] <= END_YEAR)].copy()\n",
    "if EXCLUDE_STATES:\n",
    "    df0 = df0[~df0[\"State\"].isin(EXCLUDE_STATES)].copy()\n",
    "\n",
    "# numeric\n",
    "df0 = safe_numeric(df0, [TARGET_RAW, \"econ_emp_per_1k\", \"state_avg_earnings\", \"STATE_% Unemployed\", STATE_HIRES_TOTAL_COL, STATE_POP_COL])\n",
    "\n",
    "# drop missing target\n",
    "df0 = df0.dropna(subset=[TARGET_RAW]).copy()\n",
    "\n",
    "# US macro\n",
    "df0 = add_us_macro(df0)\n",
    "df0 = df0.dropna(subset=[\"US_hires_per_1k\"]).copy()\n",
    "\n",
    "# time features\n",
    "df0[\"t_trend\"] = df0[\"Year\"] - df0[\"Year\"].min()\n",
    "df0[\"post_2020\"] = (df0[\"Year\"] >= 2020).astype(int)\n",
    "\n",
    "# lag US macro too (needed for relative persistence baseline)\n",
    "df0 = add_lags_by_state(df0, cols=[\"US_hires_per_1k\"], lags=LAGS)  # US series is year-level, but we lag per state for alignment\n",
    "\n",
    "# ============================================================\n",
    "# 2) ADD LAGS (A) â€” including lagged target\n",
    "# ============================================================\n",
    "lag_cols_core = [\n",
    "    TARGET_RAW,\n",
    "    \"econ_emp_per_1k\",\n",
    "    \"STATE_% Unemployed\",\n",
    "    \"state_avg_earnings\",\n",
    "]\n",
    "df0 = add_lags_by_state(df0, cols=lag_cols_core, lags=LAGS)\n",
    "\n",
    "# Optional: lag some health/social vars (you can expand if desired)\n",
    "health_social = [\n",
    "    \"STATE_% Adults with Obesity\",\n",
    "    \"STATE_% Children in Poverty\",\n",
    "    \"STATE_% Excessive Drinking\",\n",
    "    \"STATE_% Fair or Poor Health\",\n",
    "    \"STATE_% Smokers\",\n",
    "    \"STATE_Food Environment Index\",\n",
    "    \"STATE_Income Ratio\",\n",
    "    \"STATE_Mentally Unhealthy Days\",\n",
    "    \"STATE_Physically Unhealthy Days\",\n",
    "    \"STATE_Primary Care Physicians Rate\",\n",
    "    \"STATE_Social Association Rate\",\n",
    "    \"STATE_Violent Crime Rate\",\n",
    "]\n",
    "# Ensure numeric\n",
    "df0 = safe_numeric(df0, health_social)\n",
    "\n",
    "# If you want strict forecasting with slow vars, use lagged versions; else keep levels\n",
    "if USE_LAGGED_SLOW_VARS:\n",
    "    df0 = add_lags_by_state(df0, cols=health_social, lags=LAGS)\n",
    "\n",
    "# Simple change features (often helps with regime shifts)\n",
    "for col in [\"STATE_% Unemployed\", \"state_avg_earnings\", \"econ_emp_per_1k\"]:\n",
    "    c1 = f\"{col}_lag1\"\n",
    "    c2 = f\"{col}_lag2\"\n",
    "    if c1 in df0.columns and c2 in df0.columns:\n",
    "        df0[f\"d_{col}_lag1\"] = df0[c1] - df0[c2]\n",
    "\n",
    "# Drop rows without lag1 for key series (2014 will drop; thatâ€™s expected)\n",
    "required_for_model = [f\"{TARGET_RAW}_lag1\", \"US_hires_per_1k\", \"US_hires_per_1k_lag1\"]\n",
    "df_model = df0.dropna(subset=required_for_model).copy()\n",
    "\n",
    "print(\"Rows after lag requirements:\", df_model.shape[0])\n",
    "print(\"Years in model:\", sorted(df_model[\"Year\"].unique()))\n",
    "\n",
    "# ============================================================\n",
    "# 3) TARGET (B) â€” relative to US (log-difference)\n",
    "# ============================================================\n",
    "df_model[\"y_rel\"] = compute_y_rel(df_model)\n",
    "\n",
    "# ============================================================\n",
    "# 4) FEATURE SET (D) â€” stronger econ + reasonable health/social\n",
    "# ============================================================\n",
    "X_cols = []\n",
    "\n",
    "# Strong dynamics\n",
    "for c in [\n",
    "    f\"{TARGET_RAW}_lag1\", f\"{TARGET_RAW}_lag2\",\n",
    "    \"econ_emp_per_1k_lag1\", \"econ_emp_per_1k_lag2\",\n",
    "    \"STATE_% Unemployed_lag1\", \"STATE_% Unemployed_lag2\",\n",
    "    \"state_avg_earnings_lag1\", \"state_avg_earnings_lag2\",\n",
    "    \"d_STATE_% Unemployed_lag1\", \"d_state_avg_earnings_lag1\", \"d_econ_emp_per_1k_lag1\",\n",
    "]:\n",
    "    if c in df_model.columns:\n",
    "        X_cols.append(c)\n",
    "\n",
    "# National macro and time structure (fixed effects)\n",
    "for c in [\"US_hires_per_1k\", \"t_trend\", \"post_2020\"]:\n",
    "    if c in df_model.columns:\n",
    "        X_cols.append(c)\n",
    "\n",
    "# Health/social\n",
    "if USE_LAGGED_SLOW_VARS:\n",
    "    for base in health_social:\n",
    "        for L in LAGS:\n",
    "            c = f\"{base}_lag{L}\"\n",
    "            if c in df_model.columns:\n",
    "                X_cols.append(c)\n",
    "else:\n",
    "    for c in health_social:\n",
    "        if c in df_model.columns:\n",
    "            X_cols.append(c)\n",
    "\n",
    "# Deduplicate\n",
    "X_cols = list(dict.fromkeys(X_cols))\n",
    "if not X_cols:\n",
    "    raise ValueError(\"No features found. Check column names / lag creation.\")\n",
    "\n",
    "print(\"Feature count:\", len(X_cols))\n",
    "\n",
    "# ============================================================\n",
    "# 5) SPLIT\n",
    "# ============================================================\n",
    "df_tr = df_model[df_model[\"Year\"] <= TRAIN_END_YEAR].copy()\n",
    "df_te = df_model[df_model[\"Year\"] >  TRAIN_END_YEAR].copy()\n",
    "\n",
    "# MERF needs clusters in test to exist in train\n",
    "train_states = set(df_tr[\"State\"].unique())\n",
    "df_te = df_te[df_te[\"State\"].isin(train_states)].copy()\n",
    "\n",
    "y_tr = df_tr[\"y_rel\"].to_numpy(dtype=float)\n",
    "y_te = df_te[\"y_rel\"].to_numpy(dtype=float)\n",
    "\n",
    "cl_tr = df_tr[\"State\"].astype(str)  # Series\n",
    "cl_te = df_te[\"State\"].astype(str)\n",
    "\n",
    "# (C) Random effects design matrix Z: intercept + US macro + time trend\n",
    "Z_tr = np.column_stack([\n",
    "    np.ones(len(df_tr), dtype=float),\n",
    "    df_tr[\"US_hires_per_1k\"].to_numpy(dtype=float),\n",
    "    df_tr[\"t_trend\"].to_numpy(dtype=float),\n",
    "])\n",
    "Z_te = np.column_stack([\n",
    "    np.ones(len(df_te), dtype=float),\n",
    "    df_te[\"US_hires_per_1k\"].to_numpy(dtype=float),\n",
    "    df_te[\"t_trend\"].to_numpy(dtype=float),\n",
    "])\n",
    "\n",
    "X_tr_raw = df_tr[X_cols].copy()\n",
    "X_te_raw = df_te[X_cols].copy()\n",
    "\n",
    "years_tr = df_tr[\"Year\"].astype(int).to_numpy()\n",
    "wf_splits = make_walk_forward_splits_by_year(years_tr, min_train_years=MIN_TRAIN_YEARS_TUNE)\n",
    "\n",
    "print(f\"Train n={len(df_tr)} | Test n={len(df_te)}\")\n",
    "print(\"Train years:\", sorted(df_tr[\"Year\"].unique()))\n",
    "print(\"Test years :\", sorted(df_te[\"Year\"].unique()))\n",
    "print(\"Walk-forward tuning folds:\", len(wf_splits))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) BASELINES (E)\n",
    "# ============================================================\n",
    "print(\"\\n====================\")\n",
    "print(\"BASELINES (original scale)\")\n",
    "print(\"====================\")\n",
    "\n",
    "# Persistence baseline: hires_t â‰ˆ hires_{t-1}\n",
    "pred_te_persist_abs = df_te[f\"{TARGET_RAW}_lag1\"].to_numpy(dtype=float)\n",
    "y_te_abs = df_te[TARGET_RAW].to_numpy(dtype=float)\n",
    "print(metrics_block(\"TEST | Persistence hires(t-1)\", y_te_abs, pred_te_persist_abs))\n",
    "\n",
    "# Relative-to-US state pre-avg baseline (constant per state, trained on <=2019)\n",
    "rel_hat_te = baseline_state_preavg_rel(df_tr, df_te)\n",
    "pred_te_preavg_abs = reconstruct_hires_from_rel(rel_hat_te, df_te[\"US_hires_per_1k\"].to_numpy(dtype=float))\n",
    "print(metrics_block(\"TEST | State pre-avg rel + US\", y_te_abs, pred_te_preavg_abs))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) TUNE MERF (RF fixed effects) with walk-forward CV\n",
    "# ============================================================\n",
    "print(\"\\n====================\")\n",
    "print(\"TUNING MERF (walk-forward CV on y_rel)\")\n",
    "print(\"====================\")\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 4, 5, 6],\n",
    "    \"min_samples_leaf\": [10, 20, 30],\n",
    "    \"min_samples_split\": [20, 40, 60],\n",
    "    \"max_features\": [0.4, 0.6, \"sqrt\"],\n",
    "}\n",
    "\n",
    "best_rmse = np.inf\n",
    "best_params = None\n",
    "\n",
    "with HiddenPrints():\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        fold_scores = []\n",
    "\n",
    "        for tr_idx, va_idx in wf_splits:\n",
    "            X_f_tr_raw = X_tr_raw.iloc[tr_idx]\n",
    "            y_f_tr     = y_tr[tr_idx]\n",
    "            Z_f_tr     = Z_tr[tr_idx]\n",
    "            cl_f_tr    = cl_tr.iloc[tr_idx]\n",
    "\n",
    "            X_f_va_raw = X_tr_raw.iloc[va_idx]\n",
    "            y_f_va     = y_tr[va_idx]\n",
    "            Z_f_va     = Z_tr[va_idx]\n",
    "            cl_f_va    = cl_tr.iloc[va_idx]\n",
    "\n",
    "            imp = SimpleImputer(strategy=\"median\")\n",
    "            X_f_tr = imp.fit_transform(X_f_tr_raw)\n",
    "            X_f_va = imp.transform(X_f_va_raw)\n",
    "\n",
    "            rf = RandomForestRegressor(\n",
    "                n_estimators=TUNE_N_ESTIMATORS,\n",
    "                n_jobs=-1,\n",
    "                random_state=RANDOM_STATE,\n",
    "                **params\n",
    "            )\n",
    "            m = MERF(rf, max_iterations=MERF_TUNE_ITERS)\n",
    "            m.fit(X_f_tr, Z_f_tr, cl_f_tr, y_f_tr)\n",
    "\n",
    "            p_va = m.predict(X_f_va, Z_f_va, cl_f_va)\n",
    "            fold_scores.append(rmse(y_f_va, p_va))\n",
    "\n",
    "        avg_score = float(np.mean(fold_scores))\n",
    "        if avg_score < best_rmse:\n",
    "            best_rmse = avg_score\n",
    "            best_params = params\n",
    "\n",
    "print(\"Best CV RMSE (y_rel scale):\", round(best_rmse, 5))\n",
    "print(\"Best RF params:\", best_params)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) FINAL MERF FIT + EVAL (y_rel -> hires per 1k)\n",
    "# ============================================================\n",
    "print(\"\\n====================\")\n",
    "print(\"FINAL MERF FIT + EVAL (report on original scale)\")\n",
    "print(\"====================\")\n",
    "\n",
    "imp_final = SimpleImputer(strategy=\"median\")\n",
    "X_tr = imp_final.fit_transform(X_tr_raw)\n",
    "X_te = imp_final.transform(X_te_raw)\n",
    "\n",
    "rf_final = RandomForestRegressor(\n",
    "    n_estimators=FINAL_N_ESTIMATORS,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    **(best_params if best_params is not None else {})\n",
    ")\n",
    "\n",
    "with HiddenPrints():\n",
    "    merf_final = MERF(rf_final, max_iterations=MERF_FINAL_ITERS)\n",
    "    merf_final.fit(X_tr, Z_tr, cl_tr, y_tr)\n",
    "\n",
    "pred_tr_rel = merf_final.predict(X_tr, Z_tr, cl_tr)\n",
    "pred_te_rel = merf_final.predict(X_te, Z_te, cl_te)\n",
    "\n",
    "# Convert back to hires per 1k for evaluation\n",
    "y_tr_abs = df_tr[TARGET_RAW].to_numpy(dtype=float)\n",
    "y_te_abs = df_te[TARGET_RAW].to_numpy(dtype=float)\n",
    "\n",
    "pred_tr_abs = reconstruct_hires_from_rel(pred_tr_rel, df_tr[\"US_hires_per_1k\"].to_numpy(dtype=float))\n",
    "pred_te_abs = reconstruct_hires_from_rel(pred_te_rel, df_te[\"US_hires_per_1k\"].to_numpy(dtype=float))\n",
    "\n",
    "print(metrics_block(\"TRAIN | MERF(rel->abs)\", y_tr_abs, pred_tr_abs))\n",
    "print(metrics_block(\"TEST  | MERF(rel->abs)\", y_te_abs, pred_te_abs))\n",
    "\n",
    "# Fixed-effect RF importances (interpret as \"within-year/state features\", not causal)\n",
    "try:\n",
    "    imp_df = pd.DataFrame({\n",
    "        \"feature\": X_cols,\n",
    "        \"importance\": merf_final.trained_fe_model.feature_importances_\n",
    "    }).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    print(\"\\nTop 20 fixed-effect drivers (RF in MERF):\")\n",
    "    print(imp_df.head(20).to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(\"\\nFeature importances not available:\", repr(e))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) ROLLING-ORIGIN BACKTEST 2020..2024 (EVALUATE IN ABS SCALE)\n",
    "# ============================================================\n",
    "print(\"\\n====================\")\n",
    "print(\"ROLLING-ORIGIN BACKTEST (2020..2024) | MERF(rel->abs)\")\n",
    "print(\"====================\")\n",
    "\n",
    "def rolling_origin_merf(df_model, start_test_year=2020, end_test_year=2024,\n",
    "                        x_cols=None, use_params=None):\n",
    "    rows = []\n",
    "\n",
    "    for test_year in range(start_test_year, end_test_year + 1):\n",
    "        df_tr_y = df_model[df_model[\"Year\"] <= (test_year - 1)].copy()\n",
    "        df_te_y = df_model[df_model[\"Year\"] == test_year].copy()\n",
    "\n",
    "        # keep only states seen in train\n",
    "        seen = set(df_tr_y[\"State\"].unique())\n",
    "        df_te_y = df_te_y[df_te_y[\"State\"].isin(seen)].copy()\n",
    "\n",
    "        if df_tr_y.empty or df_te_y.empty:\n",
    "            continue\n",
    "\n",
    "        y_tr_rel_y = compute_y_rel(df_tr_y).to_numpy(dtype=float)\n",
    "        y_te_rel_y = compute_y_rel(df_te_y).to_numpy(dtype=float)\n",
    "\n",
    "        cl_tr_y = df_tr_y[\"State\"].astype(str)\n",
    "        cl_te_y = df_te_y[\"State\"].astype(str)\n",
    "\n",
    "        Z_tr_y = np.column_stack([\n",
    "            np.ones(len(df_tr_y), dtype=float),\n",
    "            df_tr_y[\"US_hires_per_1k\"].to_numpy(dtype=float),\n",
    "            df_tr_y[\"t_trend\"].to_numpy(dtype=float),\n",
    "        ])\n",
    "        Z_te_y = np.column_stack([\n",
    "            np.ones(len(df_te_y), dtype=float),\n",
    "            df_te_y[\"US_hires_per_1k\"].to_numpy(dtype=float),\n",
    "            df_te_y[\"t_trend\"].to_numpy(dtype=float),\n",
    "        ])\n",
    "\n",
    "        X_tr_y_raw = df_tr_y[x_cols].copy()\n",
    "        X_te_y_raw = df_te_y[x_cols].copy()\n",
    "\n",
    "        imp = SimpleImputer(strategy=\"median\")\n",
    "        X_tr_y = imp.fit_transform(X_tr_y_raw)\n",
    "        X_te_y = imp.transform(X_te_y_raw)\n",
    "\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=FINAL_N_ESTIMATORS,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            **(use_params if use_params is not None else {})\n",
    "        )\n",
    "        m = MERF(rf, max_iterations=MERF_FINAL_ITERS)\n",
    "        with HiddenPrints():\n",
    "            m.fit(X_tr_y, Z_tr_y, cl_tr_y, y_tr_rel_y)\n",
    "\n",
    "        pred_te_rel_y = m.predict(X_te_y, Z_te_y, cl_te_y)\n",
    "\n",
    "        # evaluate on original hires per 1k\n",
    "        y_true_abs = df_te_y[TARGET_RAW].to_numpy(dtype=float)\n",
    "        y_pred_abs = reconstruct_hires_from_rel(pred_te_rel_y, df_te_y[\"US_hires_per_1k\"].to_numpy(dtype=float))\n",
    "\n",
    "        rows.append({\n",
    "            \"test_year\": test_year,\n",
    "            **metrics_block(f\"YEAR_{test_year}\", y_true_abs, y_pred_abs)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "bt = rolling_origin_merf(df_model=df_model, start_test_year=2020, end_test_year=2024,\n",
    "                         x_cols=X_cols, use_params=best_params)\n",
    "\n",
    "if bt.empty:\n",
    "    print(\"No backtest rows produced (check data coverage).\")\n",
    "else:\n",
    "    print(bt[[\"test_year\", \"r2\", \"rmse\", \"mae\", \"n\"]].to_string(index=False))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10) OPTIONAL: What to use for state-level case studies\n",
    "# ============================================================\n",
    "# For case studies, create a tidy frame with:\n",
    "#  - actual hires per 1k\n",
    "#  - predicted hires per 1k\n",
    "#  - predicted relative deviation (y_rel) and actual y_rel\n",
    "#\n",
    "# Example (on TEST set):\n",
    "case_te = df_te[[\"State\", \"Year\", TARGET_RAW, \"US_hires_per_1k\"]].copy()\n",
    "case_te[\"y_rel_true\"] = y_te\n",
    "case_te[\"y_rel_pred\"] = pred_te_rel\n",
    "case_te[\"hires_pred_per_1k\"] = pred_te_abs\n",
    "case_te[\"excess_hires_per_1k_vs_US\"] = case_te[\"hires_pred_per_1k\"] - case_te[\"US_hires_per_1k\"]\n",
    "\n",
    "print(\"\\nCase-study table (head):\")\n",
    "print(case_te.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b28fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after lag requirements: 487\n",
      "Years in model: [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
      "Train n=391 | Test n=96\n",
      "Train years: [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
      "Test years : [2023, 2024]\n",
      "\n",
      "====================\n",
      "BASELINES (original scale)\n",
      "====================\n",
      "{'split': 'TEST | Persistence hires(t-1)', 'r2': 0.3505226337496511, 'rmse': 32.337594145359, 'mae': 25.52804286373968, 'n': 96}\n",
      "{'split': 'TEST | State shrunk mean y_rel + US', 'r2': 0.7517381795579305, 'rmse': 19.99312418122563, 'mae': 12.787843618838101, 'n': 96}\n",
      "\n",
      "====================\n",
      "RESIDUAL RIDGE AR(2) around state baseline (report on original scale)\n",
      "====================\n",
      "{'split': 'TEST | Residual Ridge(rel->abs)', 'r2': 0.780223730292301, 'rmse': 18.811182401292037, 'mae': 11.52756520541269, 'n': 96}\n",
      "Ridge alpha: 38.56620421163472\n",
      "\n",
      "Top 15 Ridge coefficients by absolute magnitude:\n",
      "rel_resid_lag1                        0.025830\n",
      "econ_emp_per_1k_lag1                  0.007767\n",
      "STATE_Primary Care Physicians Rate   -0.005505\n",
      "state_avg_earnings_lag2              -0.003091\n",
      "STATE_% Smokers                      -0.002912\n",
      "d_state_avg_earnings_lag1             0.002634\n",
      "STATE_Violent Crime Rate              0.002136\n",
      "STATE_% Children in Poverty           0.001987\n",
      "STATE_Social Association Rate         0.001761\n",
      "t_trend                               0.001694\n",
      "STATE_Food Environment Index         -0.001478\n",
      "d_econ_emp_per_1k_lag1                0.001324\n",
      "d_STATE_% Unemployed_lag1            -0.001304\n",
      "STATE_% Unemployed_lag1               0.001163\n",
      "STATE_% Fair or Poor Health          -0.001082\n",
      "\n",
      "====================\n",
      "MERF (compare) using y_rel target + y_rel lags (report on original scale)\n",
      "====================\n",
      "MERF feature count: 26\n",
      "Walk-forward tuning folds: 5\n",
      "Best CV RMSE (y_rel scale): 0.03965\n",
      "Best RF params: {'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 35, 'min_samples_split': 30}\n",
      "{'split': 'TEST | MERF(rel->abs)', 'r2': 0.5117794861888013, 'rmse': 28.03713981585704, 'mae': 17.60006917665449, 'n': 96}\n",
      "\n",
      "Top 15 MERF fixed-effect importances:\n",
      "                        feature  importance\n",
      "    STATE_% Fair or Poor Health    0.180027\n",
      "         d_econ_emp_per_1k_lag1    0.109647\n",
      "      d_state_avg_earnings_lag1    0.104111\n",
      "  STATE_Mentally Unhealthy Days    0.069366\n",
      "                     y_rel_lag1    0.064777\n",
      "STATE_Physically Unhealthy Days    0.052396\n",
      "           econ_emp_per_1k_lag1    0.044562\n",
      "           econ_emp_per_1k_lag2    0.033672\n",
      "                STATE_% Smokers    0.032543\n",
      "  STATE_Social Association Rate    0.028873\n",
      "    STATE_% Adults with Obesity    0.025090\n",
      "        STATE_% Unemployed_lag2    0.022854\n",
      "        STATE_% Unemployed_lag1    0.021801\n",
      "       STATE_Violent Crime Rate    0.021055\n",
      "             STATE_Income Ratio    0.018837\n",
      "\n",
      "Case-study table (head):\n",
      "     State  Year  econ_hires_per_1k  US_hires_per_1k     y_rel  rel_base  excess_true_vs_US  excess_true_vs_statebaseline\n",
      "   Alabama  2023         287.250084       281.574271  0.019887 -0.052006           5.675813                      0.071893\n",
      "   Alabama  2024         260.303480       257.868318  0.009363 -0.052006           2.435162                      0.061369\n",
      "   Arizona  2023         282.451944       281.574271  0.003101 -0.025732           0.877672                      0.028834\n",
      "   Arizona  2024         271.679197       257.868318  0.051976 -0.025732          13.810880                      0.077709\n",
      "  Arkansas  2023         281.812486       281.574271  0.000843 -0.048056           0.238215                      0.048899\n",
      "  Arkansas  2024         262.878867       257.868318  0.019171 -0.048056           5.010549                      0.067227\n",
      "California  2023         267.036046       281.574271 -0.052820 -0.002513         -14.538225                     -0.050307\n",
      "California  2024         250.850195       257.868318 -0.027485 -0.002513          -7.018123                     -0.024972\n",
      "  Colorado  2023         319.454280       281.574271  0.125798  0.110519          37.880009                      0.015279\n",
      "  Colorado  2024         304.038460       257.868318  0.164118  0.110519          46.170143                      0.053599\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STATE-YEAR HIRING MODEL (Improved)\n",
    "# - Test only: 2023â€“2024; Train: 2015â€“2022\n",
    "# - Target: y_rel = log1p(state_hires_per_1k) - log1p(US_hires_per_1k)\n",
    "# - Strong baseline: (shrunken) state mean y_rel + current US\n",
    "# - Improvement: model residual with regularized AR(2) + econ controls (Ridge)\n",
    "# - Optional: MERF using y_rel lags (avoid absolute target lags)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from merf.merf import MERF\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "START_YEAR = 2014\n",
    "END_YEAR   = 2024\n",
    "TEST_YEARS = [2023, 2024]         # <<<<<<<<<<<< requested\n",
    "EXCLUDE_STATES = {\"Alaska\"}       # optional\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "TARGET_RAW = \"econ_hires_per_1k\"\n",
    "STATE_HIRES_TOTAL_COL = \"state_hires_total\"\n",
    "STATE_POP_COL         = \"state_total_pop\"\n",
    "\n",
    "LAGS = [1, 2]                     # at most lag 2\n",
    "\n",
    "# Baseline shrinkage (helps small-sample states; usually mild effect here)\n",
    "# shrunk_mean = (n/(n+K))*state_mean + (K/(n+K))*global_mean\n",
    "SHRINK_K = 2.0\n",
    "\n",
    "# RidgeCV grid\n",
    "RIDGE_ALPHAS = np.logspace(-3, 4, 30)\n",
    "\n",
    "# MERF settings (optional)\n",
    "RUN_MERF = True\n",
    "TUNE_N_ESTIMATORS  = 200\n",
    "FINAL_N_ESTIMATORS = 900\n",
    "MERF_TUNE_ITERS    = 6\n",
    "MERF_FINAL_ITERS   = 25\n",
    "MIN_TRAIN_YEARS_TUNE = 3\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, \"w\")\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        try:\n",
    "            sys.stdout.close()\n",
    "        finally:\n",
    "            sys.stdout = self._original_stdout\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def metrics_block(name, y_true, y_pred):\n",
    "    return {\n",
    "        \"split\": name,\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "        \"rmse\": rmse(y_true, y_pred),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"n\": int(len(y_true)),\n",
    "    }\n",
    "\n",
    "def safe_numeric(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def add_us_macro(df):\n",
    "    if (STATE_HIRES_TOTAL_COL not in df.columns) or (STATE_POP_COL not in df.columns):\n",
    "        raise ValueError(\"Missing columns for US macro: state_hires_total and/or state_total_pop.\")\n",
    "    df = safe_numeric(df, [STATE_HIRES_TOTAL_COL, STATE_POP_COL])\n",
    "\n",
    "    us_macro = (\n",
    "        df.groupby(\"Year\", as_index=False)\n",
    "          .agg(US_hires_sum=(STATE_HIRES_TOTAL_COL, \"sum\"),\n",
    "               US_pop_sum=(STATE_POP_COL, \"sum\"))\n",
    "    )\n",
    "    us_macro[\"US_hires_per_1k\"] = 1000.0 * us_macro[\"US_hires_sum\"] / us_macro[\"US_pop_sum\"].replace({0: np.nan})\n",
    "    return df.merge(us_macro[[\"Year\", \"US_hires_per_1k\"]], on=\"Year\", how=\"left\")\n",
    "\n",
    "def add_lags_by_state(df, cols, lags=(1,2)):\n",
    "    df = df.sort_values([\"State\", \"Year\"]).copy()\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        for L in lags:\n",
    "            df[f\"{col}_lag{L}\"] = df.groupby(\"State\")[col].shift(L)\n",
    "    return df\n",
    "\n",
    "def compute_y_rel(df):\n",
    "    return np.log1p(df[TARGET_RAW].astype(float)) - np.log1p(df[\"US_hires_per_1k\"].astype(float))\n",
    "\n",
    "def compute_y_rel_from_lags(df, L):\n",
    "    # y_rel_lagL = log1p(hires_lagL) - log1p(US_lagL)\n",
    "    return np.log1p(df[f\"{TARGET_RAW}_lag{L}\"].astype(float)) - np.log1p(df[f\"US_hires_per_1k_lag{L}\"].astype(float))\n",
    "\n",
    "def reconstruct_hires_from_rel(pred_y_rel, us_hires_per_1k):\n",
    "    pred_log1p = pred_y_rel + np.log1p(us_hires_per_1k)\n",
    "    return np.expm1(pred_log1p)\n",
    "\n",
    "def make_walk_forward_splits_by_year(years: np.ndarray, min_train_years: int = 3):\n",
    "    years = np.asarray(years).astype(int)\n",
    "    uniq = np.sort(np.unique(years))\n",
    "    if len(uniq) < min_train_years + 1:\n",
    "        raise ValueError(\"Not enough unique years for walk-forward folds.\")\n",
    "    splits = []\n",
    "    for i in range(min_train_years, len(uniq)):\n",
    "        tr_years = uniq[:i]\n",
    "        va_year  = uniq[i]\n",
    "        tr_idx = np.where(np.isin(years, tr_years))[0]\n",
    "        va_idx = np.where(years == va_year)[0]\n",
    "        if len(tr_idx) and len(va_idx):\n",
    "            splits.append((tr_idx, va_idx))\n",
    "    if not splits:\n",
    "        raise ValueError(\"No walk-forward splits created.\")\n",
    "    return splits\n",
    "\n",
    "def build_shrunken_state_means(df_train):\n",
    "    tmp = df_train.copy()\n",
    "    tmp[\"y_rel\"] = compute_y_rel(tmp)\n",
    "\n",
    "    gmean = float(tmp[\"y_rel\"].mean())\n",
    "    stats = tmp.groupby(\"State\")[\"y_rel\"].agg([\"mean\", \"count\"]).rename(columns={\"mean\":\"state_mean\", \"count\":\"n\"})\n",
    "    stats[\"shrunk_mean\"] = (stats[\"n\"]/(stats[\"n\"] + SHRINK_K))*stats[\"state_mean\"] + (SHRINK_K/(stats[\"n\"] + SHRINK_K))*gmean\n",
    "    return stats[\"shrunk_mean\"], gmean\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) LOAD / CLEAN\n",
    "# ============================================================\n",
    "df0 = df1.copy()\n",
    "\n",
    "for c in [\"Unnamed: 0\", \"Unnamed: 0.1\", \"index\"]:\n",
    "    if c in df0.columns:\n",
    "        df0 = df0.drop(columns=[c])\n",
    "\n",
    "if \"State\" not in df0.columns or \"Year\" not in df0.columns:\n",
    "    raise ValueError(\"df1 must contain at least columns: ['State','Year'].\")\n",
    "\n",
    "df0[\"State\"] = df0[\"State\"].astype(str)\n",
    "df0[\"Year\"]  = pd.to_numeric(df0[\"Year\"], errors=\"coerce\")\n",
    "df0 = df0.dropna(subset=[\"State\", \"Year\"]).copy()\n",
    "df0[\"Year\"]  = df0[\"Year\"].astype(int)\n",
    "\n",
    "df0 = df0[(df0[\"Year\"] >= START_YEAR) & (df0[\"Year\"] <= END_YEAR)].copy()\n",
    "if EXCLUDE_STATES:\n",
    "    df0 = df0[~df0[\"State\"].isin(EXCLUDE_STATES)].copy()\n",
    "\n",
    "df0 = safe_numeric(df0, [TARGET_RAW, \"econ_emp_per_1k\", \"state_avg_earnings\", \"STATE_% Unemployed\",\n",
    "                         STATE_HIRES_TOTAL_COL, STATE_POP_COL])\n",
    "\n",
    "df0 = df0.dropna(subset=[TARGET_RAW]).copy()\n",
    "\n",
    "# US macro + time features\n",
    "df0 = add_us_macro(df0)\n",
    "df0 = df0.dropna(subset=[\"US_hires_per_1k\"]).copy()\n",
    "\n",
    "df0[\"t_trend\"] = df0[\"Year\"] - df0[\"Year\"].min()\n",
    "df0[\"post_2020\"] = (df0[\"Year\"] >= 2020).astype(int)\n",
    "\n",
    "# Lags (target + econ + US macro)\n",
    "lag_cols = [TARGET_RAW, \"econ_emp_per_1k\", \"STATE_% Unemployed\", \"state_avg_earnings\", \"US_hires_per_1k\"]\n",
    "df0 = add_lags_by_state(df0, lag_cols, lags=LAGS)\n",
    "\n",
    "# Build y_rel + its lags\n",
    "df0[\"y_rel\"] = compute_y_rel(df0)\n",
    "for L in LAGS:\n",
    "    df0[f\"y_rel_lag{L}\"] = compute_y_rel_from_lags(df0, L)\n",
    "\n",
    "# Simple deltas on econ series (lag1 - lag2)\n",
    "for base in [\"econ_emp_per_1k\", \"STATE_% Unemployed\", \"state_avg_earnings\"]:\n",
    "    c1, c2 = f\"{base}_lag1\", f\"{base}_lag2\"\n",
    "    if c1 in df0.columns and c2 in df0.columns:\n",
    "        df0[f\"d_{base}_lag1\"] = df0[c1] - df0[c2]\n",
    "\n",
    "# Keep rows where lag1 exists (2014 drops)\n",
    "required = [\"y_rel\", \"y_rel_lag1\", \"US_hires_per_1k\"]\n",
    "df_model = df0.dropna(subset=required).copy()\n",
    "\n",
    "print(\"Rows after lag requirements:\", df_model.shape[0])\n",
    "print(\"Years in model:\", sorted(df_model[\"Year\"].unique()))\n",
    "\n",
    "# ============================================================\n",
    "# 2) TRAIN/TEST split (train = all except 2023/2024; test = 2023/2024)\n",
    "# ============================================================\n",
    "train_mask = ~df_model[\"Year\"].isin(TEST_YEARS)\n",
    "test_mask  =  df_model[\"Year\"].isin(TEST_YEARS)\n",
    "\n",
    "df_tr = df_model.loc[train_mask].copy()\n",
    "df_te = df_model.loc[test_mask].copy()\n",
    "\n",
    "# Make sure test states exist in train\n",
    "seen = set(df_tr[\"State\"].unique())\n",
    "df_te = df_te[df_te[\"State\"].isin(seen)].copy()\n",
    "\n",
    "print(f\"Train n={len(df_tr)} | Test n={len(df_te)}\")\n",
    "print(\"Train years:\", sorted(df_tr[\"Year\"].unique()))\n",
    "print(\"Test years :\", sorted(df_te[\"Year\"].unique()))\n",
    "\n",
    "# ============================================================\n",
    "# 3) BASELINES (original scale)\n",
    "# ============================================================\n",
    "print(\"\\n====================\")\n",
    "print(\"BASELINES (original scale)\")\n",
    "print(\"====================\")\n",
    "\n",
    "y_te_abs = df_te[TARGET_RAW].to_numpy(dtype=float)\n",
    "\n",
    "# Baseline 1: Persistence in absolute space: hires_t â‰ˆ hires_{t-1}\n",
    "pred_te_persist_abs = df_te[f\"{TARGET_RAW}_lag1\"].to_numpy(dtype=float)\n",
    "print(metrics_block(\"TEST | Persistence hires(t-1)\", y_te_abs, pred_te_persist_abs))\n",
    "\n",
    "# Baseline 2: State (shrunken) mean y_rel from TRAIN + current US\n",
    "state_rel_shrunk, global_rel_mean = build_shrunken_state_means(df_tr)\n",
    "rel_hat_te = df_te[\"State\"].map(state_rel_shrunk).to_numpy(dtype=float)\n",
    "missing = ~np.isfinite(rel_hat_te)\n",
    "if missing.any():\n",
    "    rel_hat_te[missing] = global_rel_mean\n",
    "\n",
    "pred_te_preavg_abs = reconstruct_hires_from_rel(rel_hat_te, df_te[\"US_hires_per_1k\"].to_numpy(dtype=float))\n",
    "print(metrics_block(\"TEST | State shrunk mean y_rel + US\", y_te_abs, pred_te_preavg_abs))\n",
    "\n",
    "# ============================================================\n",
    "# 4) RESIDUAL RIDGE MODEL (recommended)\n",
    "#    y_rel = state_baseline + residual\n",
    "#    residual ~ Ridge( y_rel_lag1_resid, y_rel_lag2_resid, econ lags/deltas, health/social )\n",
    "# ============================================================\n",
    "print(\"\\n====================\")\n",
    "print(\"RESIDUAL RIDGE AR(2) around state baseline (report on original scale)\")\n",
    "print(\"====================\")\n",
    "\n",
    "# Build baseline per row\n",
    "df_tr[\"rel_base\"] = df_tr[\"State\"].map(state_rel_shrunk).astype(float)\n",
    "df_te[\"rel_base\"] = df_te[\"State\"].map(state_rel_shrunk).astype(float)\n",
    "df_tr[\"rel_base\"] = df_tr[\"rel_base\"].fillna(global_rel_mean)\n",
    "df_te[\"rel_base\"] = df_te[\"rel_base\"].fillna(global_rel_mean)\n",
    "\n",
    "# Residual target\n",
    "df_tr[\"rel_resid_y\"] = df_tr[\"y_rel\"] - df_tr[\"rel_base\"]\n",
    "\n",
    "# Residual lag features (encourage mean reversion around baseline)\n",
    "df_tr[\"rel_resid_lag1\"] = df_tr[\"y_rel_lag1\"] - df_tr[\"rel_base\"]\n",
    "df_tr[\"rel_resid_lag2\"] = df_tr[\"y_rel_lag2\"] - df_tr[\"rel_base\"] if \"y_rel_lag2\" in df_tr.columns else np.nan\n",
    "\n",
    "df_te[\"rel_resid_lag1\"] = df_te[\"y_rel_lag1\"] - df_te[\"rel_base\"]\n",
    "df_te[\"rel_resid_lag2\"] = df_te[\"y_rel_lag2\"] - df_te[\"rel_base\"] if \"y_rel_lag2\" in df_te.columns else np.nan\n",
    "\n",
    "# Candidate predictors for residual\n",
    "health_social = [\n",
    "    \"STATE_% Adults with Obesity\",\n",
    "    \"STATE_% Children in Poverty\",\n",
    "    \"STATE_% Excessive Drinking\",\n",
    "    \"STATE_% Fair or Poor Health\",\n",
    "    \"STATE_% Smokers\",\n",
    "    \"STATE_Food Environment Index\",\n",
    "    \"STATE_Income Ratio\",\n",
    "    \"STATE_Mentally Unhealthy Days\",\n",
    "    \"STATE_Physically Unhealthy Days\",\n",
    "    \"STATE_Primary Care Physicians Rate\",\n",
    "    \"STATE_Social Association Rate\",\n",
    "    \"STATE_Violent Crime Rate\",\n",
    "]\n",
    "\n",
    "df_model = safe_numeric(df_model, health_social)\n",
    "\n",
    "ridge_feats = [\n",
    "    \"rel_resid_lag1\", \"rel_resid_lag2\",\n",
    "    \"econ_emp_per_1k_lag1\", \"econ_emp_per_1k_lag2\", \"d_econ_emp_per_1k_lag1\",\n",
    "    \"STATE_% Unemployed_lag1\", \"STATE_% Unemployed_lag2\", \"d_STATE_% Unemployed_lag1\",\n",
    "    \"state_avg_earnings_lag1\", \"state_avg_earnings_lag2\", \"d_state_avg_earnings_lag1\",\n",
    "    \"t_trend\", \"post_2020\",\n",
    "] + [c for c in health_social if c in df_tr.columns]\n",
    "\n",
    "ridge_feats = [c for c in ridge_feats if c in df_tr.columns and c in df_te.columns]\n",
    "\n",
    "X_tr_r = df_tr[ridge_feats].copy()\n",
    "X_te_r = df_te[ridge_feats].copy()\n",
    "\n",
    "y_tr_r = df_tr[\"rel_resid_y\"].to_numpy(dtype=float)\n",
    "\n",
    "ridge = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ridge\", RidgeCV(alphas=RIDGE_ALPHAS))\n",
    "])\n",
    "ridge.fit(X_tr_r, y_tr_r)\n",
    "\n",
    "pred_te_resid = ridge.predict(X_te_r)\n",
    "pred_te_rel = df_te[\"rel_base\"].to_numpy(dtype=float) + pred_te_resid\n",
    "pred_te_abs = reconstruct_hires_from_rel(pred_te_rel, df_te[\"US_hires_per_1k\"].to_numpy(dtype=float))\n",
    "\n",
    "print(metrics_block(\"TEST | Residual Ridge(rel->abs)\", y_te_abs, pred_te_abs))\n",
    "print(\"Ridge alpha:\", float(ridge.named_steps[\"ridge\"].alpha_))\n",
    "\n",
    "# Optional: coefficients for interpretation (signs are meaningful after scaling; for raw units, refit w/o scaler)\n",
    "coef = pd.Series(ridge.named_steps[\"ridge\"].coef_, index=ridge_feats).sort_values(key=np.abs, ascending=False)\n",
    "print(\"\\nTop 15 Ridge coefficients by absolute magnitude:\")\n",
    "print(coef.head(15).to_string())\n",
    "\n",
    "# ============================================================\n",
    "# 5) OPTIONAL: MERF (compare)\n",
    "# ============================================================\n",
    "if RUN_MERF:\n",
    "    print(\"\\n====================\")\n",
    "    print(\"MERF (compare) using y_rel target + y_rel lags (report on original scale)\")\n",
    "    print(\"====================\")\n",
    "\n",
    "    # Feature set for MERF (keep it smaller + aligned with y_rel)\n",
    "    X_cols = [\n",
    "        \"y_rel_lag1\", \"y_rel_lag2\",\n",
    "        \"econ_emp_per_1k_lag1\", \"econ_emp_per_1k_lag2\", \"d_econ_emp_per_1k_lag1\",\n",
    "        \"STATE_% Unemployed_lag1\", \"STATE_% Unemployed_lag2\", \"d_STATE_% Unemployed_lag1\",\n",
    "        \"state_avg_earnings_lag1\", \"state_avg_earnings_lag2\", \"d_state_avg_earnings_lag1\",\n",
    "        \"US_hires_per_1k\", \"t_trend\", \"post_2020\",\n",
    "    ] + [c for c in health_social if c in df_tr.columns]\n",
    "\n",
    "    X_cols = [c for c in X_cols if c in df_tr.columns and c in df_te.columns]\n",
    "\n",
    "    y_tr = df_tr[\"y_rel\"].to_numpy(dtype=float)\n",
    "    y_te = df_te[\"y_rel\"].to_numpy(dtype=float)\n",
    "\n",
    "    cl_tr = df_tr[\"State\"].astype(str)\n",
    "    cl_te = df_te[\"State\"].astype(str)\n",
    "\n",
    "    # Random effects: intercept + US macro + time trend\n",
    "    Z_tr = np.column_stack([\n",
    "        np.ones(len(df_tr), dtype=float),\n",
    "        df_tr[\"US_hires_per_1k\"].to_numpy(dtype=float),\n",
    "        df_tr[\"t_trend\"].to_numpy(dtype=float),\n",
    "    ])\n",
    "    Z_te = np.column_stack([\n",
    "        np.ones(len(df_te), dtype=float),\n",
    "        df_te[\"US_hires_per_1k\"].to_numpy(dtype=float),\n",
    "        df_te[\"t_trend\"].to_numpy(dtype=float),\n",
    "    ])\n",
    "\n",
    "    X_tr_raw = df_tr[X_cols].copy()\n",
    "    X_te_raw = df_te[X_cols].copy()\n",
    "\n",
    "    # Walk-forward folds inside TRAIN years\n",
    "    years_tr = df_tr[\"Year\"].astype(int).to_numpy()\n",
    "    wf_splits = make_walk_forward_splits_by_year(years_tr, min_train_years=MIN_TRAIN_YEARS_TUNE)\n",
    "    print(\"MERF feature count:\", len(X_cols))\n",
    "    print(\"Walk-forward tuning folds:\", len(wf_splits))\n",
    "\n",
    "    param_grid = {\n",
    "        \"max_depth\": [3, 4, 5],                # keep conservative\n",
    "        \"min_samples_leaf\": [15, 25, 35],\n",
    "        \"min_samples_split\": [30, 50, 80],\n",
    "        \"max_features\": [0.4, 0.6, \"sqrt\"],\n",
    "    }\n",
    "\n",
    "    best_rmse = np.inf\n",
    "    best_params = None\n",
    "\n",
    "    with HiddenPrints():\n",
    "        for params in ParameterGrid(param_grid):\n",
    "            fold_scores = []\n",
    "            for tr_idx, va_idx in wf_splits:\n",
    "                imp = SimpleImputer(strategy=\"median\")\n",
    "                X_f_tr = imp.fit_transform(X_tr_raw.iloc[tr_idx])\n",
    "                X_f_va = imp.transform(X_tr_raw.iloc[va_idx])\n",
    "\n",
    "                rf = RandomForestRegressor(\n",
    "                    n_estimators=TUNE_N_ESTIMATORS,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=RANDOM_STATE,\n",
    "                    **params\n",
    "                )\n",
    "                m = MERF(rf, max_iterations=MERF_TUNE_ITERS)\n",
    "                m.fit(X_f_tr, Z_tr[tr_idx], cl_tr.iloc[tr_idx], y_tr[tr_idx])\n",
    "                p_va = m.predict(X_f_va, Z_tr[va_idx], cl_tr.iloc[va_idx])\n",
    "                fold_scores.append(rmse(y_tr[va_idx], p_va))\n",
    "\n",
    "            avg = float(np.mean(fold_scores))\n",
    "            if avg < best_rmse:\n",
    "                best_rmse = avg\n",
    "                best_params = params\n",
    "\n",
    "    print(\"Best CV RMSE (y_rel scale):\", round(best_rmse, 5))\n",
    "    print(\"Best RF params:\", best_params)\n",
    "\n",
    "    imp_final = SimpleImputer(strategy=\"median\")\n",
    "    X_tr = imp_final.fit_transform(X_tr_raw)\n",
    "    X_te = imp_final.transform(X_te_raw)\n",
    "\n",
    "    rf_final = RandomForestRegressor(\n",
    "        n_estimators=FINAL_N_ESTIMATORS,\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        **(best_params if best_params is not None else {})\n",
    "    )\n",
    "\n",
    "    with HiddenPrints():\n",
    "        merf = MERF(rf_final, max_iterations=MERF_FINAL_ITERS)\n",
    "        merf.fit(X_tr, Z_tr, cl_tr, y_tr)\n",
    "\n",
    "    pred_te_rel = merf.predict(X_te, Z_te, cl_te)\n",
    "    pred_te_abs = reconstruct_hires_from_rel(pred_te_rel, df_te[\"US_hires_per_1k\"].to_numpy(dtype=float))\n",
    "\n",
    "    print(metrics_block(\"TEST | MERF(rel->abs)\", y_te_abs, pred_te_abs))\n",
    "\n",
    "    try:\n",
    "        imp_df = pd.DataFrame({\n",
    "            \"feature\": X_cols,\n",
    "            \"importance\": merf.trained_fe_model.feature_importances_\n",
    "        }).sort_values(\"importance\", ascending=False)\n",
    "        print(\"\\nTop 15 MERF fixed-effect importances:\")\n",
    "        print(imp_df.head(15).to_string(index=False))\n",
    "    except Exception as e:\n",
    "        print(\"MERF importances unavailable:\", repr(e))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) Case-study table for 2023â€“2024\n",
    "# ============================================================\n",
    "case = df_te[[\"State\",\"Year\",TARGET_RAW,\"US_hires_per_1k\",\"y_rel\",\"rel_base\"]].copy()\n",
    "case[\"excess_true_vs_US\"] = case[TARGET_RAW] - case[\"US_hires_per_1k\"]\n",
    "case[\"excess_true_vs_statebaseline\"] = case[\"y_rel\"] - case[\"rel_base\"]\n",
    "print(\"\\nCase-study table (head):\")\n",
    "print(case.head(10).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
