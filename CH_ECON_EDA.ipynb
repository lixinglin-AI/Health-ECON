{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1f8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b209c7d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 197\u001b[0m\n\u001b[1;32m    194\u001b[0m     standardized_dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Combine all dataframes\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(standardized_dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Sort by Year, State, and County\u001b[39;00m\n\u001b[1;32m    200\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m combined_df\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCounty\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m    385\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[1;32m    386\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[1;32m    387\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[1;32m    388\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[1;32m    389\u001b[0m     names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[1;32m    390\u001b[0m     verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[1;32m    391\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_keys_and_objs(objs, keys)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# Define the folder path\n",
    "folder_path = r\"/Users/david/Desktop/github/Health-ECON/data\"\n",
    "\n",
    "# Get all Excel files\n",
    "excel_files = sorted([f for f in os.listdir(folder_path) if f.endswith(('.xlsx', '.xls'))])\n",
    "\n",
    "# Column name mapping dictionary to standardize names\n",
    "column_mapping = {\n",
    "    # FIPS, State, County (consistent)\n",
    "    'FIPS': 'FIPS',\n",
    "    'State': 'State',\n",
    "    'County': 'County',\n",
    "    \n",
    "    # Deaths/Premature Deaths variations\n",
    "    'pre mature Deaths': 'Premature Deaths',\n",
    "    'premature Deaths': 'Premature Deaths',\n",
    "    'Premature death': 'Premature Deaths',\n",
    "    '# Deaths': 'Premature Deaths',\n",
    "    'Deaths': 'Premature Deaths',\n",
    "    \n",
    "    # Fair/Poor Health variations\n",
    "    '% Fair or Poor Health': '% Fair or Poor Health',\n",
    "    '% Fair/Poor': '% Fair or Poor Health',\n",
    "    \n",
    "    # Physically Unhealthy Days\n",
    "    'Physically Unhealthy Days': 'Physically Unhealthy Days',\n",
    "    'Average Number of Physically Unhealthy Days': 'Physically Unhealthy Days',\n",
    "    \n",
    "    # Mentally Unhealthy Days\n",
    "    'Mentally Unhealthy Days': 'Mentally Unhealthy Days',\n",
    "    'Average Number of Mentally Unhealthy Days': 'Mentally Unhealthy Days',\n",
    "    \n",
    "    # Low Birth Weight\n",
    "    '% low birth weight': '% Low Birthweight',\n",
    "    '% Low birthweight': '% Low Birthweight',\n",
    "    '% Low Birthweight': '% Low Birthweight',\n",
    "    '% LBW': '% Low Birthweight',\n",
    "    \n",
    "    # Smoking\n",
    "    '% Smokers': '% Smokers',\n",
    "    '% Adults Reporting Currently Smoking': '% Smokers',\n",
    "    \n",
    "    # Obesity\n",
    "    '% Obese': '% Adults with Obesity',\n",
    "    '% Adults with Obesity': '% Adults with Obesity',\n",
    "    \n",
    "    # Food Environment Index (consistent)\n",
    "    'Food Environment Index': 'Food Environment Index',\n",
    "    \n",
    "    # Physical Activity/Exercise Access\n",
    "    '% With Access': '% With Access to Exercise Opportunities',\n",
    "    '% With Access exercise': '% With Access to Exercise Opportunities',\n",
    "    '% With Access to Exercise Opportunities': '% With Access to Exercise Opportunities',\n",
    "    '% Physically Inactive': '% Physically Inactive',\n",
    "    \n",
    "    # Excessive Drinking (consistent)\n",
    "    '% Excessive Drinking': '% Excessive Drinking',\n",
    "    \n",
    "    # Driving Deaths\n",
    "    '# Alcohol-Impaired Driving Deaths': '# Alcohol-Impaired Driving Deaths',\n",
    "    '# Driving Deaths': '# Driving Deaths',\n",
    "    \n",
    "    # Teen Birth Rate (consistent)\n",
    "    'Teen Birth Rate': 'Teen Birth Rate',\n",
    "    \n",
    "    # Uninsured\n",
    "    '# Uninsured': '# Uninsured',\n",
    "    '% Uninsured': '% Uninsured',\n",
    "    \n",
    "    # Primary Care Physicians\n",
    "    'PCP Rate': 'Primary Care Physicians Rate',\n",
    "    'Primary Care Physicians Ratio': 'Primary Care Physicians Rate',\n",
    "    'Primary Care Physicians Rate': 'Primary Care Physicians Rate',\n",
    "    \n",
    "    # Medicare\n",
    "    '# Medicare enrollees': '# Medicare Enrollees',\n",
    "    '# Medicare Enrollees': '# Medicare Enrollees',\n",
    "    \n",
    "    # Preventable Hospitalizations\n",
    "    'Preventable Hosp. Rate': 'Preventable Hospitalization Rate',\n",
    "    'Preventable Hospitalization Rate': 'Preventable Hospitalization Rate',\n",
    "    \n",
    "    # Education\n",
    "    '% Some College': '% Some College',\n",
    "    \n",
    "    # Unemployment\n",
    "    '% Unemployed': '% Unemployed',\n",
    "    \n",
    "    # Child Poverty\n",
    "    '% Children in Poverty': '% Children in Poverty',\n",
    "    \n",
    "    # Income Ratio\n",
    "    'Income Ratio': 'Income Ratio',\n",
    "    \n",
    "    # Single-Parent Households\n",
    "    '% Single-Parent Households': '% Children in Single-Parent Households',\n",
    "    '% Children in Single-Parent Households': '% Children in Single-Parent Households',\n",
    "    \n",
    "    # Social Association\n",
    "    'Association Rate': 'Social Association Rate',\n",
    "    'Social Association Rate': 'Social Association Rate',\n",
    "    \n",
    "    # Crime\n",
    "    'Violent Crime Rate': 'Violent Crime Rate',\n",
    "    \n",
    "    # Housing\n",
    "    '% Severe Housing Problems': '% Severe Housing Problems',\n",
    "    \n",
    "    # Commuting\n",
    "    '% Drive Alone': '% Drive Alone to Work',\n",
    "    '% Drive Alone to Work': '% Drive Alone to Work',\n",
    "    'Long Commute - Drives Alone': '% Long Commute - Drives Alone',\n",
    "    '% Long Commute - Drives Alone': '% Long Commute - Drives Alone',\n",
    "    \n",
    "    # Dentist columns (2023 specific)\n",
    "    'Quartile': 'Quartile',\n",
    "    '# Dentists': '# Dentists',\n",
    "    'Dentist Rate': 'Dentist Rate',\n",
    "    'Dentist Ratio': 'Dentist Ratio'\n",
    "}\n",
    "\n",
    "# List to store all dataframes\n",
    "all_dfs = []\n",
    "\n",
    "# Process each file\n",
    "for file in excel_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    \n",
    "    try:\n",
    "        # Extract year from filename\n",
    "        if '2014' in file:\n",
    "            year = 2014\n",
    "        elif '2015' in file:\n",
    "            year = 2015\n",
    "        elif '2016' in file:\n",
    "            year = 2016\n",
    "        elif '2017' in file:\n",
    "            year = 2017\n",
    "        elif '2018' in file:\n",
    "            year = 2018\n",
    "        elif '2019' in file:\n",
    "            year = 2019\n",
    "        elif '2020' in file:\n",
    "            year = 2020\n",
    "        elif '2021' in file:\n",
    "            year = 2021\n",
    "        elif '2022' in file:\n",
    "            year = 2022\n",
    "        elif '2023' in file:\n",
    "            year = 2023\n",
    "        elif '2024' in file:\n",
    "            year = 2024\n",
    "        else:\n",
    "            year = 'Unknown'\n",
    "        \n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(file_path, sheet_name='Ranked Measure Data')\n",
    "        \n",
    "        # Add year column\n",
    "        df['Year'] = year\n",
    "        \n",
    "        # Rename columns using the mapping\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Add to list\n",
    "        all_dfs.append(df)\n",
    "        \n",
    "        print(f\"Processed {file}: {len(df)} rows, Year: {year}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {str(e)}\")\n",
    "\n",
    "# Get all unique columns across all dataframes\n",
    "all_columns = set()\n",
    "for df in all_dfs:\n",
    "    all_columns.update(df.columns)\n",
    "\n",
    "# Remove 'Year' from all_columns as we'll add it at a specific position\n",
    "all_columns.discard('Year')\n",
    "\n",
    "# Define the order of columns (put most important ones first)\n",
    "column_order = ['Year', 'FIPS', 'State', 'County'] + sorted(list(all_columns - {'FIPS', 'State', 'County'}))\n",
    "\n",
    "# Standardize all dataframes to have the same columns\n",
    "standardized_dfs = []\n",
    "for df in all_dfs:\n",
    "    # Add missing columns with NaN\n",
    "    for col in column_order:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "    \n",
    "    # Reorder columns\n",
    "    df = df[column_order]\n",
    "    standardized_dfs.append(df)\n",
    "\n",
    "# Combine all dataframes\n",
    "combined_df = pd.concat(standardized_dfs, ignore_index=True)\n",
    "\n",
    "# Sort by Year, State, and County\n",
    "combined_df = combined_df.sort_values(['Year', 'State', 'County'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50003fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where County is null\n",
    "print(f\"Original shape: {combined_df.shape}\")\n",
    "print(f\"Rows with null County: {combined_df['County'].isna().sum()}\")\n",
    "\n",
    "# Remove rows where County is null\n",
    "combined_df_cleaned = combined_df[combined_df['County'].notna()].copy()\n",
    "\n",
    "print(f\"After removing null counties: {combined_df_cleaned.shape}\")\n",
    "\n",
    "# Calculate missing percentage for each column\n",
    "missing_percent = (combined_df_cleaned.isna().sum() / len(combined_df_cleaned)) * 100\n",
    "\n",
    "# Create a summary of missing data\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': missing_percent.index,\n",
    "    'Missing_Count': combined_df_cleaned.isna().sum().values,\n",
    "    'Total_Rows': len(combined_df_cleaned),\n",
    "    'Missing_Percentage': missing_percent.values\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Columns with missing data:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(missing_summary[missing_summary['Missing_Percentage'] > 0])\n",
    "\n",
    "# Identify columns to keep (less than or equal to 50% missing)\n",
    "columns_to_keep = missing_percent[missing_percent <= 50].index.tolist()\n",
    "columns_to_remove = missing_percent[missing_percent > 50].index.tolist()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Columns to remove (>50% missing): {len(columns_to_remove)}\")\n",
    "print(f\"{'='*60}\")\n",
    "for col in columns_to_remove:\n",
    "    print(f\"  - {col}: {missing_percent[col]:.1f}% missing\")\n",
    "\n",
    "# Keep only columns with <=50% missing data\n",
    "combined_df_cleaned = combined_df_cleaned[columns_to_keep]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Final cleaned dataset summary:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Shape: {combined_df_cleaned.shape}\")\n",
    "print(f\"Rows: {len(combined_df_cleaned)}\")\n",
    "print(f\"Columns: {len(combined_df_cleaned.columns)}\")\n",
    "print(f\"\\nRemaining columns ({len(combined_df_cleaned.columns)}):\")\n",
    "for i, col in enumerate(combined_df_cleaned.columns, 1):\n",
    "    non_missing = combined_df_cleaned[col].notna().sum()\n",
    "    pct_available = (non_missing / len(combined_df_cleaned)) * 100\n",
    "    print(f\"  {i:2d}. {col:<50} ({pct_available:.1f}% data available)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1657e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the QWI data\n",
    "qwi_path = r\"C:\\Users\\liuc\\Downloads\\qwi_47eb6eafd6f449ccbab042fc81879bc1.csv\"\n",
    "qwi_df = pd.read_csv(qwi_path)\n",
    "\n",
    "print(\"QWI Data Shape:\", qwi_df.shape)\n",
    "print(f\"\\nUnique years: {sorted(qwi_df['year'].unique())}\")\n",
    "print(f\"Number of unique counties (FIPS): {qwi_df['geography'].nunique()}\")\n",
    "print(f\"Number of unique industries: {qwi_df['industry'].nunique()}\")\n",
    "\n",
    "# Handle FIPS codes\n",
    "qwi_df['FIPS'] = qwi_df['geography'].astype(str).str.zfill(5)\n",
    "\n",
    "# Handle suppressed data\n",
    "value_cols = ['EarnBeg', 'Emp', 'HirA']\n",
    "suppression_cols = ['sEarnBeg', 'sEmp', 'sHirA']\n",
    "\n",
    "for val_col, supp_col in zip(value_cols, suppression_cols):\n",
    "    qwi_df.loc[qwi_df[supp_col] == 5, val_col] = np.nan\n",
    "    print(f\"Suppressed {val_col}: {(qwi_df[supp_col] == 5).sum()} records\")\n",
    "\n",
    "# Create quarterly version of health data\n",
    "quarters = [1, 2, 3, 4]\n",
    "health_quarterly_list = []\n",
    "\n",
    "for quarter in quarters:\n",
    "    health_q = combined_df_cleaned.copy()\n",
    "    health_q['quarter'] = quarter\n",
    "    health_quarterly_list.append(health_q)\n",
    "\n",
    "health_quarterly = pd.concat(health_quarterly_list, ignore_index=True)\n",
    "\n",
    "print(f\"\\nOriginal health data shape: {combined_df_cleaned.shape}\")\n",
    "print(f\"Quarterly health data shape: {health_quarterly.shape}\")\n",
    "\n",
    "# Ensure FIPS formatting in health data\n",
    "health_quarterly['FIPS'] = health_quarterly['FIPS'].astype(str).str.zfill(5)\n",
    "\n",
    "# Merge - note the lowercase 'year' in QWI data\n",
    "merged_df = pd.merge(\n",
    "    health_quarterly,\n",
    "    qwi_df,\n",
    "    left_on=['FIPS', 'Year', 'quarter'],\n",
    "    right_on=['FIPS', 'year', 'quarter'],\n",
    "    how='inner',\n",
    "    suffixes=('_health', '_qwi')\n",
    ")\n",
    "\n",
    "print(f\"\\nMerged data shape: {merged_df.shape}\")\n",
    "print(f\"Number of columns: {len(merged_df.columns)}\")\n",
    "\n",
    "# Check merge quality\n",
    "print(\"\\n=== MERGE QUALITY CHECK ===\")\n",
    "print(f\"Unique FIPS in health data: {health_quarterly['FIPS'].nunique()}\")\n",
    "print(f\"Unique FIPS in QWI data: {qwi_df['FIPS'].nunique()}\")\n",
    "print(f\"Unique FIPS in merged data: {merged_df['FIPS'].nunique()}\")\n",
    "\n",
    "# Check industries in merged data\n",
    "print(f\"\\nRecords per industry in merged data:\")\n",
    "print(merged_df['industry'].value_counts().head())\n",
    "\n",
    "# Check year coverage\n",
    "print(f\"\\nYear coverage in merged data:\")\n",
    "print(merged_df['Year'].value_counts().sort_index())\n",
    "\n",
    "# Basic statistics on workforce metrics\n",
    "print(\"\\n=== WORKFORCE METRICS SUMMARY ===\")\n",
    "workforce_metrics = merged_df.groupby('industry')[['Emp', 'EarnBeg', 'HirA']].agg(['mean', 'median', 'count'])\n",
    "print(workforce_metrics.head())\n",
    "\n",
    "# Check how much data we retained\n",
    "retention_rate = (merged_df['FIPS'].nunique() / health_quarterly['FIPS'].nunique()) * 100\n",
    "print(f\"\\nCounty retention rate: {retention_rate:.1f}%\")\n",
    "\n",
    "print(f\"\\nFinal dataset: {len(merged_df):,} records\")\n",
    "print(f\"Unique county-year-quarter-industry combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57232fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_columns = [col for col in combined_df_cleaned.columns if col != 'quarter']\n",
    "\n",
    "# From QWI data: only the specific columns you mentioned\n",
    "qwi_columns = ['quarter', 'EarnBeg', 'Emp', 'HirA', 'sEarnBeg', 'sEmp', 'sHirA', 'industry']\n",
    "\n",
    "# Combine the column lists\n",
    "columns_to_keep = health_columns + qwi_columns\n",
    "\n",
    "# Filter the merged dataframe\n",
    "merged_df_filtered = merged_df[columns_to_keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec3070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mapping\n",
    "industry_mapping = {\n",
    "    '00': 'Total, All Industries',\n",
    "    '11': 'Agriculture, Forestry, Fishing, and Hunting',\n",
    "    '21': 'Mining, Quarrying, and Oil and Gas Extraction',\n",
    "    '22': 'Utilities',\n",
    "    '23': 'Construction',\n",
    "    '31-33': 'Manufacturing',\n",
    "    '42': 'Wholesale Trade',\n",
    "    '44-45': 'Retail Trade',\n",
    "    '48-49': 'Transportation and Warehousing',\n",
    "    '51': 'Information',\n",
    "    '52': 'Finance and Insurance',\n",
    "    '53': 'Real Estate and Rental and Leasing',\n",
    "    '54': 'Professional, Scientific, and Technical Services',\n",
    "    '55': 'Management of Companies and Enterprises',\n",
    "    '56': 'Administrative and Support and Waste Management Services',\n",
    "    '61': 'Educational Services',\n",
    "    '62': 'Health Care and Social Assistance',\n",
    "    '71': 'Arts, Entertainment, and Recreation',\n",
    "    '72': 'Accommodation and Food Services',\n",
    "    '81': 'Other Services (except Public Administration)',\n",
    "    '92': 'Public Administration'\n",
    "}\n",
    "\n",
    "# Add readable industry name\n",
    "merged_df_filtered['industry_name'] = merged_df_filtered['industry'].map(industry_mapping)\n",
    "\n",
    "merged_df_filtered = merged_df_filtered.rename(columns={\n",
    "    'EarnBeg': 'Earnings_Beginning_Qtr',\n",
    "    'Emp': 'Employment_Count',\n",
    "    'HirA': 'New_Hires',\n",
    "    'sEarnBeg': 'Earnings_Suppression_Flag',\n",
    "    'sEmp': 'Employment_Suppression_Flag',\n",
    "    'sHirA': 'Hires_Suppression_Flag'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a2b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_path = r\"C:\\Users\\liuc\\Downloads\\Population by Age and Sex - US, States, Counties.csv\"\n",
    "df_population = pd.read_csv(population_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b143f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Year is int\n",
    "merged_df_filtered['Year'] = merged_df_filtered['Year'].astype(int)\n",
    "df_population_unique = (\n",
    "    df_population\n",
    "    .sort_values(['Statefips','Countyfips','Year'])\n",
    "    .drop_duplicates(subset=['Statefips','Countyfips','Year'], keep='first')\n",
    ")\n",
    "\n",
    "\n",
    "df_population_unique['FIPS'] = (\n",
    "    df_population_unique['Statefips'].astype(str).str.zfill(2) +\n",
    "    df_population_unique['Countyfips'].astype(str).str.zfill(3)\n",
    ")\n",
    "\n",
    "df_pop_small = df_population_unique[['FIPS','Year','Total Population']].rename(\n",
    "    columns={'Total Population':'Population'}\n",
    ")\n",
    "\n",
    "merged_full = merged_df_filtered.merge(\n",
    "    df_pop_small, on=['FIPS','Year'], how='left'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ea032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CONFIG\n",
    "# ---------------------------------------------------------\n",
    "health_count_cols = [\n",
    "    '# Alcohol-Impaired Driving Deaths',\n",
    "    '# Driving Deaths',\n",
    "    'Premature Deaths'\n",
    "]\n",
    "\n",
    "health_rate_cols = [\n",
    "    '% Adults with Obesity',\n",
    "    '% Children in Poverty',\n",
    "    '% Children in Single-Parent Households',\n",
    "    '% Drive Alone to Work',\n",
    "    '% Excessive Drinking',\n",
    "    '% Fair or Poor Health',\n",
    "    '% Long Commute - Drives Alone',\n",
    "    '% Low Birthweight',\n",
    "    '% Severe Housing Problems',\n",
    "    '% Smokers',\n",
    "    '% Some College',\n",
    "    '% Unemployed',\n",
    "    '% Uninsured',\n",
    "    '% With Access to Exercise Opportunities',\n",
    "    'Food Environment Index',\n",
    "    'Income Ratio',\n",
    "    'Mentally Unhealthy Days',\n",
    "    'Physically Unhealthy Days',\n",
    "    'Preventable Hospitalization Rate',\n",
    "    'Primary Care Physicians Rate',\n",
    "    'Social Association Rate',\n",
    "    'Teen Birth Rate',\n",
    "    'Violent Crime Rate'\n",
    "]\n",
    "\n",
    "econ_cols = ['Employment_Count', 'New_Hires', 'Earnings_Beginning_Qtr']\n",
    "flag_map = {\n",
    "    'Employment_Count': 'Employment_Suppression_Flag',\n",
    "    'New_Hires': 'Hires_Suppression_Flag',\n",
    "    'Earnings_Beginning_Qtr': 'Earnings_Suppression_Flag'\n",
    "}\n",
    "\n",
    "# Common \"total industry\" signals (adjust if your data differs)\n",
    "TOTAL_CODES = {'00', '0', '000', '10'}\n",
    "TOTAL_NAME_PAT = r'\\b(total|all)\\b'\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------------------------------\n",
    "def to_num_clean(s):\n",
    "    # robust numeric conversion: removes commas and common suppression tokens\n",
    "    s = s.astype(str).str.replace(\",\", \"\", regex=False).str.strip()\n",
    "    s = s.replace({\n",
    "        \"\": np.nan, \"nan\": np.nan, \"None\": np.nan, \"NA\": np.nan, \"N/A\": np.nan,\n",
    "        \"*\": np.nan, \"S\": np.nan, \"s\": np.nan\n",
    "    })\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def sum_mc1(x):\n",
    "    return x.sum(min_count=1)\n",
    "\n",
    "def safe_div(n, d):\n",
    "    n = np.asarray(n, dtype=float)\n",
    "    d = np.asarray(d, dtype=float)\n",
    "    return np.where((d == 0) | np.isnan(d), np.nan, n / d)\n",
    "\n",
    "def weighted_mean(x, w):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    w = np.asarray(w, dtype=float)\n",
    "    num = np.nansum(x * w)\n",
    "    den = np.nansum(w)\n",
    "    return np.nan if den == 0 else num / den\n",
    "\n",
    "def infer_suppressed_value(df, val_col, flag_col):\n",
    "    \"\"\"\n",
    "    Tries to infer which flag value indicates suppression by checking which flag value\n",
    "    is most associated with missing/low coverage in val_col.\n",
    "    Returns a set of values to treat as 'suppressed'. If ambiguous, returns empty set.\n",
    "    \"\"\"\n",
    "    if flag_col not in df.columns or val_col not in df.columns:\n",
    "        return set()\n",
    "\n",
    "    f = df[flag_col].copy()\n",
    "    # normalize flag to strings so 1/\"1\"/True collapse\n",
    "    f = f.astype(str).str.strip().replace({\"nan\": np.nan})\n",
    "    tmp = pd.DataFrame({\"flag\": f, \"val\": df[val_col]})\n",
    "\n",
    "    # only keep flags that appear enough\n",
    "    vc = tmp[\"flag\"].value_counts(dropna=True)\n",
    "    cand = vc[vc >= 100].index.tolist()\n",
    "    if not cand:\n",
    "        return set()\n",
    "\n",
    "    stats = []\n",
    "    for v in cand:\n",
    "        sub = tmp[tmp[\"flag\"] == v]\n",
    "        miss = sub[\"val\"].isna().mean()\n",
    "        nonmiss = 1 - miss\n",
    "        stats.append((v, nonmiss, miss, len(sub)))\n",
    "\n",
    "    # pick the flag value with the *lowest* non-missing rate as \"suppressed\"\n",
    "    stats_sorted = sorted(stats, key=lambda t: t[1])\n",
    "    best = stats_sorted[0]\n",
    "    second = stats_sorted[1] if len(stats_sorted) > 1 else None\n",
    "\n",
    "    # require separation to avoid guessing wrong\n",
    "    # (e.g., suppressed has 5% non-missing and next has 95%)\n",
    "    if second is None or (second[1] - best[1] >= 0.20):\n",
    "        return {best[0]}\n",
    "    return set()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 0: Pre-processing\n",
    "# ---------------------------------------------------------\n",
    "df = merged_full.copy()\n",
    "\n",
    "# Keep FIPS as string with leading zeros (do NOT numeric-coerce it)\n",
    "df[\"FIPS\"] = df[\"FIPS\"].astype(str).str.extract(r\"(\\d+)\", expand=False)\n",
    "df[\"FIPS\"] = df[\"FIPS\"].str.zfill(5)\n",
    "\n",
    "# Industry as string\n",
    "df[\"industry\"] = df[\"industry\"].astype(str).str.strip()\n",
    "df[\"industry_name\"] = df[\"industry_name\"].astype(str)\n",
    "\n",
    "# Numeric coercion for relevant cols\n",
    "for c in [\"Year\", \"quarter\", \"Population\"] + econ_cols + health_count_cols + health_rate_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = to_num_clean(df[c])\n",
    "\n",
    "# Filter year window\n",
    "df = df[(df[\"Year\"] >= 2014) & (df[\"Year\"] <= 2024)].copy()\n",
    "\n",
    "# Quarter normalization 0..3 -> 1..4\n",
    "if \"quarter\" in df.columns:\n",
    "    qmin, qmax = df[\"quarter\"].min(), df[\"quarter\"].max()\n",
    "    if pd.notna(qmin) and pd.notna(qmax) and qmin >= 0 and qmax <= 3:\n",
    "        df[\"quarter\"] = df[\"quarter\"] + 1\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 1: HEALTH (County-Year -> State-Year), de-dup first\n",
    "# ---------------------------------------------------------\n",
    "health_keep = [\"State\", \"Year\", \"FIPS\", \"Population\"] + health_count_cols + health_rate_cols\n",
    "county_health_raw = df[health_keep].copy()\n",
    "\n",
    "county_health = (\n",
    "    county_health_raw\n",
    "    .sort_values([\"State\", \"FIPS\", \"Year\"])\n",
    "    .groupby([\"State\", \"Year\", \"FIPS\"], as_index=False)\n",
    "    .agg({**{\"Population\": \"first\"},\n",
    "          **{c: \"first\" for c in health_count_cols},\n",
    "          **{c: \"first\" for c in health_rate_cols}})\n",
    ")\n",
    "\n",
    "# Create numerators for weighted averages\n",
    "for col in health_rate_cols:\n",
    "    county_health[f\"{col}_num\"] = county_health[col] * county_health[\"Population\"]\n",
    "\n",
    "# Aggregate to state-year\n",
    "agg_dict = {\"state_total_pop\": (\"Population\", \"sum\")}\n",
    "for col in health_count_cols:\n",
    "    agg_dict[f\"STATE_{col}\"] = (col, sum_mc1)\n",
    "for col in health_rate_cols:\n",
    "    agg_dict[f\"{col}_num_sum\"] = (f\"{col}_num\", sum_mc1)\n",
    "\n",
    "state_health = county_health.groupby([\"State\", \"Year\"], as_index=False).agg(**agg_dict)\n",
    "\n",
    "# finalize weighted rates\n",
    "for col in health_rate_cols:\n",
    "    state_health[f\"STATE_{col}\"] = state_health[f\"{col}_num_sum\"] / state_health[\"state_total_pop\"].replace({0: np.nan})\n",
    "    state_health.drop(columns=[f\"{col}_num_sum\"], inplace=True)\n",
    "\n",
    "# optional: convert health counts to per-100k at state level\n",
    "for col in health_count_cols:\n",
    "    state_health[f\"STATE_{col}_per_100k\"] = (\n",
    "        100000 * state_health[f\"STATE_{col}\"] / state_health[\"state_total_pop\"].replace({0: np.nan})\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 2: ECON (County-Quarter-Industry -> State-Year)\n",
    "# Key: avoid double counting totals vs components + handle suppression flags safely\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_econ = df[[\"State\", \"Year\", \"quarter\", \"FIPS\", \"industry\", \"industry_name\", \"Population\"] + econ_cols +\n",
    "            [v for v in flag_map.values() if v in df.columns]].copy()\n",
    "\n",
    "# --- 2A) Infer which flag value means \"suppressed\" (so we don't accidentally wipe all data)\n",
    "suppressed_values = {}\n",
    "for col, flag in flag_map.items():\n",
    "    if flag in df_econ.columns:\n",
    "        suppressed_values[col] = infer_suppressed_value(df_econ, col, flag)\n",
    "    else:\n",
    "        suppressed_values[col] = set()\n",
    "\n",
    "# Apply suppression -> NaN ONLY if we inferred a value confidently\n",
    "for col, flag in flag_map.items():\n",
    "    vals = suppressed_values.get(col, set())\n",
    "    if flag in df_econ.columns and vals:\n",
    "        f = df_econ[flag].astype(str).str.strip()\n",
    "        df_econ.loc[f.isin(vals), col] = np.nan\n",
    "\n",
    "# --- 2B) Decide how to handle industry totals\n",
    "# Strategy:\n",
    "#   If \"total\" rows exist and have decent coverage -> use total only (no double count)\n",
    "#   Else -> sum across non-total industries, excluding totals if present\n",
    "\n",
    "is_total = (\n",
    "    df_econ[\"industry\"].isin(TOTAL_CODES)\n",
    "    | df_econ[\"industry_name\"].str.lower().str.contains(TOTAL_NAME_PAT, regex=True, na=False)\n",
    ")\n",
    "\n",
    "econ_total = df_econ[is_total].copy()\n",
    "econ_parts = df_econ[~is_total].copy()\n",
    "\n",
    "# Coverage check: how many State-Year-Quarter groups have any non-missing employment in total rows?\n",
    "tot_grp = econ_total.groupby([\"State\", \"Year\", \"quarter\"])[\"Employment_Count\"].apply(lambda s: s.notna().any())\n",
    "total_coverage = tot_grp.mean() if len(tot_grp) else 0.0\n",
    "\n",
    "MODE = \"total_only\" if (len(econ_total) > 0 and total_coverage >= 0.60) else \"sum_parts\"\n",
    "\n",
    "base_econ = econ_total if MODE == \"total_only\" else econ_parts\n",
    "\n",
    "# Wage bill (use employment-weighted earnings; if Employment_Count missing, wage_bill missing)\n",
    "base_econ[\"wage_bill\"] = base_econ[\"Earnings_Beginning_Qtr\"] * base_econ[\"Employment_Count\"]\n",
    "\n",
    "# --- 2C) State-quarter totals (sum across counties and (if parts) industries)\n",
    "state_qtr = (\n",
    "    base_econ.groupby([\"State\", \"Year\", \"quarter\"], as_index=False)\n",
    "    .agg(\n",
    "        state_emp_qtr=(\"Employment_Count\", sum_mc1),\n",
    "        state_hires_qtr=(\"New_Hires\", sum_mc1),\n",
    "        state_wage_bill_qtr=(\"wage_bill\", sum_mc1),\n",
    "    )\n",
    ")\n",
    "\n",
    "state_qtr[\"state_avg_earnings_qtr\"] = safe_div(state_qtr[\"state_wage_bill_qtr\"], state_qtr[\"state_emp_qtr\"])\n",
    "\n",
    "# --- 2D) State-year aggregation\n",
    "state_annual = (\n",
    "    state_qtr.groupby([\"State\", \"Year\"], as_index=False)\n",
    "    .agg(\n",
    "        state_emp_avg=(\"state_emp_qtr\", \"mean\"),   # stock -> average across quarters\n",
    "        state_emp_sum=(\"state_emp_qtr\", sum_mc1),  # person-quarters\n",
    "        state_hires_total=(\"state_hires_qtr\", sum_mc1),  # flow -> sum across quarters\n",
    "        state_wage_bill_total=(\"state_wage_bill_qtr\", sum_mc1),\n",
    "        state_avg_earnings_meanq=(\"state_avg_earnings_qtr\", \"mean\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Employment-weighted earnings across quarters (recommended)\n",
    "state_annual[\"state_avg_earnings\"] = safe_div(state_annual[\"state_wage_bill_total\"], state_annual[\"state_emp_sum\"])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 3: Merge Econ + Health (annual)\n",
    "# ---------------------------------------------------------\n",
    "state_df = state_annual.merge(state_health, on=[\"State\", \"Year\"], how=\"left\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 4: Core annual rates + YoY (your existing features)\n",
    "# ---------------------------------------------------------\n",
    "pop = state_df[\"state_total_pop\"].replace({0: np.nan})\n",
    "emp_avg = state_df[\"state_emp_avg\"].replace({0: np.nan})\n",
    "\n",
    "state_df[\"econ_emp_per_1k\"] = 1000 * safe_div(state_df[\"state_emp_avg\"], pop)\n",
    "state_df[\"econ_hires_per_1k\"] = 1000 * safe_div(state_df[\"state_hires_total\"], pop)\n",
    "state_df[\"econ_hire_rate_annual\"] = safe_div(state_df[\"state_hires_total\"], emp_avg)\n",
    "\n",
    "state_df = state_df.sort_values([\"State\", \"Year\"]).copy()\n",
    "\n",
    "def yoy_growth(s):\n",
    "    prev = s.shift(1)\n",
    "    return safe_div(s - prev, prev)\n",
    "\n",
    "state_df[\"growth_emp_yoy\"] = state_df.groupby(\"State\")[\"state_emp_avg\"].transform(yoy_growth)\n",
    "state_df[\"growth_earn_yoy\"] = state_df.groupby(\"State\")[\"state_avg_earnings\"].transform(yoy_growth)\n",
    "state_df[\"growth_hires_yoy\"] = state_df.groupby(\"State\")[\"state_hires_total\"].transform(yoy_growth)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 5: NEW TOPIC: Relativity + Divergence from US trends\n",
    "# ---------------------------------------------------------\n",
    "# Level rates\n",
    "state_df[\"emp_rate\"]   = safe_div(state_df[\"state_emp_avg\"], pop)\n",
    "state_df[\"hires_rate\"] = safe_div(state_df[\"state_hires_total\"], pop)\n",
    "state_df[\"earn_level\"] = state_df[\"state_avg_earnings\"]  # rename for clarity\n",
    "\n",
    "# US aggregates by year (consistent definitions)\n",
    "us = (\n",
    "    state_df.groupby(\"Year\", as_index=False)\n",
    "    .apply(lambda s: pd.Series({\n",
    "        \"us_pop\": np.nansum(s[\"state_total_pop\"]),\n",
    "        \"us_emp_avg\": np.nansum(s[\"state_emp_avg\"]),\n",
    "        \"us_hires_total\": np.nansum(s[\"state_hires_total\"]),\n",
    "        \"us_earn_level\": weighted_mean(s[\"earn_level\"].to_numpy(), s[\"state_emp_sum\"].to_numpy())\n",
    "    }))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "us[\"us_emp_rate\"] = safe_div(us[\"us_emp_avg\"], us[\"us_pop\"])\n",
    "us[\"us_hires_rate\"] = safe_div(us[\"us_hires_total\"], us[\"us_pop\"])\n",
    "\n",
    "state_df = state_df.merge(us[[\"Year\", \"us_emp_rate\", \"us_hires_rate\", \"us_earn_level\"]], on=\"Year\", how=\"left\")\n",
    "\n",
    "# Relativity indices (ratio + log ratio)\n",
    "state_df[\"rel_emp_idx\"] = safe_div(state_df[\"emp_rate\"], state_df[\"us_emp_rate\"])\n",
    "state_df[\"rel_hires_idx\"] = safe_div(state_df[\"hires_rate\"], state_df[\"us_hires_rate\"])\n",
    "state_df[\"rel_earn_idx\"] = safe_div(state_df[\"earn_level\"], state_df[\"us_earn_level\"])\n",
    "\n",
    "state_df[\"log_rel_emp\"] = np.log(state_df[\"rel_emp_idx\"])\n",
    "state_df[\"log_rel_hires\"] = np.log(state_df[\"rel_hires_idx\"])\n",
    "state_df[\"log_rel_earn\"] = np.log(state_df[\"rel_earn_idx\"])\n",
    "\n",
    "# Divergence (growth gap vs US)\n",
    "state_df[\"emp_rate_yoy\"] = state_df.groupby(\"State\")[\"emp_rate\"].pct_change()\n",
    "state_df[\"hires_rate_yoy\"] = state_df.groupby(\"State\")[\"hires_rate\"].pct_change()\n",
    "state_df[\"earn_level_yoy\"] = state_df.groupby(\"State\")[\"earn_level\"].pct_change()\n",
    "\n",
    "us_tr = us.sort_values(\"Year\").copy()\n",
    "us_tr[\"us_emp_rate_yoy\"] = us_tr[\"us_emp_rate\"].pct_change()\n",
    "us_tr[\"us_hires_rate_yoy\"] = us_tr[\"us_hires_rate\"].pct_change()\n",
    "us_tr[\"us_earn_level_yoy\"] = us_tr[\"us_earn_level\"].pct_change()\n",
    "\n",
    "state_df = state_df.merge(us_tr[[\"Year\",\"us_emp_rate_yoy\",\"us_hires_rate_yoy\",\"us_earn_level_yoy\"]],\n",
    "                          on=\"Year\", how=\"left\")\n",
    "\n",
    "state_df[\"div_emp_rate_yoy\"] = state_df[\"emp_rate_yoy\"] - state_df[\"us_emp_rate_yoy\"]\n",
    "state_df[\"div_hires_rate_yoy\"] = state_df[\"hires_rate_yoy\"] - state_df[\"us_hires_rate_yoy\"]\n",
    "state_df[\"div_earnings_yoy\"] = state_df[\"earn_level_yoy\"] - state_df[\"us_earn_level_yoy\"]\n",
    "\n",
    "# Optional composite (z within year to remove scale)\n",
    "for c in [\"log_rel_emp\",\"log_rel_hires\",\"log_rel_earn\",\"div_emp_rate_yoy\",\"div_hires_rate_yoy\",\"div_earnings_yoy\"]:\n",
    "    mu = state_df.groupby(\"Year\")[c].transform(\"mean\")\n",
    "    sd = state_df.groupby(\"Year\")[c].transform(\"std\")\n",
    "    state_df[c + \"_z\"] = (state_df[c] - mu) / sd\n",
    "\n",
    "state_df[\"labor_rel_div_index\"] = (\n",
    "    state_df[\"log_rel_emp_z\"] + state_df[\"log_rel_hires_z\"] + state_df[\"log_rel_earn_z\"]\n",
    "    + state_df[\"div_emp_rate_yoy_z\"]\n",
    ")\n",
    "\n",
    "# Final cleanup\n",
    "state_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 6: Quick diagnostics (crucial)\n",
    "# ---------------------------------------------------------\n",
    "print(\"ECON MODE:\", MODE, \"| total_coverage =\", round(float(total_coverage), 3))\n",
    "print(\"Inferred suppressed flag values used (empty = not applied):\")\n",
    "for k,v in suppressed_values.items():\n",
    "    print(\" \", k, \":\", v)\n",
    "\n",
    "print(\"\\nNon-null rates in annual econ:\")\n",
    "print(state_df[[\"state_emp_avg\",\"state_hires_total\",\"state_avg_earnings\"]].notna().mean().to_string())\n",
    "\n",
    "print(\"\\nShare of zeros among non-missing annual econ (should NOT be ~1.0):\")\n",
    "for c in [\"state_emp_avg\",\"state_hires_total\"]:\n",
    "    s = state_df[c]\n",
    "    print(c, \"zero_share=\", float((s.fillna(np.nan) == 0).mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL UPDATED PIPELINE (Lasso + RandomForest + LightGBM)\n",
    "#  - Target: rel_emp_idx\n",
    "#  - Test years: 2022-2024 only\n",
    "#  - Time-aware tuning: walk-forward CV on TRAIN only\n",
    "#  - Honest evaluation:\n",
    "#      * Walk-forward OOF on TRAIN (for sanity / robustness)\n",
    "#      * True holdout TEST (2022-2024)\n",
    "#  - Robustness:\n",
    "#      * Mean ensemble\n",
    "#      * Ridge stacking with walk-forward OOF meta-features (leakage-safe)\n",
    "#      * Rolling-origin (per-year) diagnostics\n",
    "#  - X constraints enforced:\n",
    "#      * NO emp-related predictors\n",
    "#      * NO yoy/log/z/us/rel variables in X\n",
    "#      * X includes hires + earnings + health\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, Lasso, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import root_mean_squared_error\n",
    "except Exception:\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# ---- LightGBM ----\n",
    "# If you get ImportError: install first:\n",
    "#   pip install lightgbm\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        \"LightGBM is not installed. Install it via: pip install lightgbm\\n\"\n",
    "        \"On some Windows setups you may need Visual C++ Build Tools.\"\n",
    "    ) from e\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helpers\n",
    "# ------------------------------------------------------------\n",
    "def eval_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "        \"rmse\": float(root_mean_squared_error(y_true, y_pred)),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "def make_walk_forward_folds(years: np.ndarray, min_train_years: int = 1):\n",
    "    years = np.asarray(years).astype(int)\n",
    "    uniq = np.sort(np.unique(years))\n",
    "    if len(uniq) < (min_train_years + 1):\n",
    "        raise ValueError(\"Not enough unique years for walk-forward folds.\")\n",
    "\n",
    "    folds = []\n",
    "    for i in range(min_train_years, len(uniq)):\n",
    "        tr_years = uniq[:i]\n",
    "        va_year = uniq[i]\n",
    "        tr_idx = np.where(np.isin(years, tr_years))[0]\n",
    "        va_idx = np.where(years == va_year)[0]\n",
    "        if len(tr_idx) and len(va_idx):\n",
    "            folds.append((tr_idx, va_idx))\n",
    "\n",
    "    if not folds:\n",
    "        raise ValueError(\"No folds created.\")\n",
    "    return folds\n",
    "\n",
    "def oof_preds_walk_forward(model_fixed, X_train, y_train, folds):\n",
    "    y_train = np.asarray(y_train)\n",
    "    oof = np.full(len(y_train), np.nan, dtype=float)\n",
    "    for tr_idx, va_idx in folds:\n",
    "        m = clone(model_fixed)\n",
    "        m.fit(X_train.iloc[tr_idx], y_train[tr_idx])\n",
    "        oof[va_idx] = m.predict(X_train.iloc[va_idx])\n",
    "    return oof\n",
    "\n",
    "def pick_alpha_1se_from_lassocv(lasso_cv: LassoCV):\n",
    "    \"\"\"\n",
    "    1SE rule: pick the MOST regularized alpha whose mean CV MSE is within\n",
    "    1 std error of the minimum mean MSE.\n",
    "    \"\"\"\n",
    "    mse_path = lasso_cv.mse_path_\n",
    "    if mse_path.shape[0] == len(lasso_cv.alphas_):\n",
    "        mean_mse = mse_path.mean(axis=1)\n",
    "        std_mse  = mse_path.std(axis=1)\n",
    "    else:\n",
    "        mean_mse = mse_path.mean(axis=0)\n",
    "        std_mse  = mse_path.std(axis=0)\n",
    "\n",
    "    alphas = np.asarray(lasso_cv.alphas_).astype(float)\n",
    "    i_min = int(np.argmin(mean_mse))\n",
    "    thresh = mean_mse[i_min] + std_mse[i_min]\n",
    "    candidates = alphas[mean_mse <= thresh]\n",
    "    return float(np.max(candidates)) if len(candidates) else float(lasso_cv.alpha_)\n",
    "\n",
    "def fit_eval_one(model_fixed, X_train, y_train, X_test, y_test, folds_oof, name=\"model\"):\n",
    "    # Walk-forward OOF on TRAIN\n",
    "    oof = oof_preds_walk_forward(model_fixed, X_train, y_train, folds_oof)\n",
    "    valid = np.isfinite(oof)\n",
    "\n",
    "    # Fit on full TRAIN\n",
    "    m = clone(model_fixed)\n",
    "    m.fit(X_train, y_train)\n",
    "\n",
    "    pred_tr_in = m.predict(X_train)\n",
    "    pred_te = m.predict(X_test)\n",
    "\n",
    "    out = {\n",
    "        \"model\": name,\n",
    "        \"train_in\": eval_metrics(y_train, pred_tr_in),\n",
    "        \"train_oof\": eval_metrics(y_train[valid], oof[valid]) if valid.sum() else None,\n",
    "        \"test\": eval_metrics(y_test, pred_te),\n",
    "        \"oof_kept\": int(valid.sum()),\n",
    "        \"oof_dropped\": int((~valid).sum()),\n",
    "        \"fitted\": m\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def stack_ridge_oof(base_models_fixed, X_train, y_train, X_test, folds_oof, meta_alphas=None):\n",
    "    \"\"\"\n",
    "    Ridge stacking using walk-forward OOF features (leakage-safe).\n",
    "    - meta-train features are true OOF predictions by year\n",
    "    - meta-test uses base models fit on full TRAIN\n",
    "    \"\"\"\n",
    "    if meta_alphas is None:\n",
    "        meta_alphas = np.logspace(-3, 3, 19)\n",
    "\n",
    "    y_train = np.asarray(y_train)\n",
    "\n",
    "    meta_train = np.column_stack([\n",
    "        oof_preds_walk_forward(m, X_train, y_train, folds_oof)\n",
    "        for m in base_models_fixed.values()\n",
    "    ])\n",
    "    valid = np.all(np.isfinite(meta_train), axis=1)\n",
    "    if valid.sum() < 60:\n",
    "        raise RuntimeError(f\"Too few OOF rows for stacking meta-learner: kept={int(valid.sum())}\")\n",
    "\n",
    "    meta_test = np.column_stack([\n",
    "        clone(m).fit(X_train, y_train).predict(X_test)\n",
    "        for m in base_models_fixed.values()\n",
    "    ])\n",
    "\n",
    "    meta = RidgeCV(alphas=meta_alphas, cv=5)\n",
    "    meta.fit(meta_train[valid], y_train[valid])\n",
    "\n",
    "    ens_train_oof = np.full(len(y_train), np.nan, dtype=float)\n",
    "    ens_train_oof[valid] = meta.predict(meta_train[valid])\n",
    "    ens_test = meta.predict(meta_test)\n",
    "\n",
    "    weights = pd.Series(meta.coef_, index=list(base_models_fixed.keys()))\n",
    "    info = {\"meta_kept\": int(valid.sum()), \"meta_dropped\": int((~valid).sum())}\n",
    "    return ens_train_oof, ens_test, valid, weights, info\n",
    "\n",
    "def rolling_origin_eval(model_fixed, df_model, X_all, y_all, start_year=2017, end_year=2024):\n",
    "    years = df_model[\"Year\"].astype(int).values\n",
    "    out = []\n",
    "    for t in range(start_year, end_year + 1):\n",
    "        tr_mask = years <= (t - 1)\n",
    "        te_mask = years == t\n",
    "        if tr_mask.sum() < 80 or te_mask.sum() < 30:\n",
    "            continue\n",
    "        m = clone(model_fixed)\n",
    "        m.fit(X_all.loc[tr_mask], y_all[tr_mask])\n",
    "        pred = m.predict(X_all.loc[te_mask])\n",
    "        met = eval_metrics(y_all[te_mask], pred)\n",
    "        out.append({\"test_year\": t, \"train_n\": int(tr_mask.sum()), \"test_n\": int(te_mask.sum()), **met})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0) BUILD MODELING FRAME FROM state_df\n",
    "# ============================================================\n",
    "df_model = state_df.copy()\n",
    "df_model[\"Year\"] = pd.to_numeric(df_model[\"Year\"], errors=\"coerce\")\n",
    "df_model = df_model[(df_model[\"Year\"] >= 2014) & (df_model[\"Year\"] <= 2024)].copy()\n",
    "df_model[\"State\"] = df_model[\"State\"].astype(str)\n",
    "df_model = df_model.dropna(subset=[\"Year\"]).copy()\n",
    "df_model[\"Year\"] = df_model[\"Year\"].astype(int)\n",
    "\n",
    "target = \"rel_emp_idx\"\n",
    "df_model[target] = pd.to_numeric(df_model[target], errors=\"coerce\")\n",
    "df_model = df_model[np.isfinite(df_model[target])].copy()\n",
    "y_all = df_model[target].values\n",
    "\n",
    "econ_x = [\n",
    "    \"state_hires_total\",\n",
    "    \"econ_hires_per_1k\",\n",
    "    \"state_avg_earnings\",\n",
    "    \"state_avg_earnings_meanq\",\n",
    "    \"state_total_pop\",\n",
    "]\n",
    "health_x = [\n",
    "    \"STATE_% Children in Poverty\",\n",
    "    \"STATE_% Uninsured\",\n",
    "    \"STATE_% Adults with Obesity\",\n",
    "    \"STATE_% Smokers\",\n",
    "    \"STATE_% Fair or Poor Health\",\n",
    "    \"STATE_Mentally Unhealthy Days\",\n",
    "    \"STATE_Physically Unhealthy Days\",\n",
    "    \"STATE_Food Environment Index\",\n",
    "    \"STATE_Income Ratio\",\n",
    "    \"STATE_Primary Care Physicians Rate\",\n",
    "    \"STATE_Preventable Hospitalization Rate\",\n",
    "    \"STATE_Violent Crime Rate\",\n",
    "    \"STATE_Teen Birth Rate\",\n",
    "    \"STATE_% With Access to Exercise Opportunities\",\n",
    "    \"STATE_% Some College\",\n",
    "    \"STATE_Premature Deaths_per_100k\",\n",
    "    \"STATE_# Driving Deaths_per_100k\",\n",
    "    \"STATE_# Alcohol-Impaired Driving Deaths_per_100k\",\n",
    "]\n",
    "\n",
    "X_cols = [c for c in (econ_x + health_x) if c in df_model.columns]\n",
    "X_all = df_model[X_cols].select_dtypes(include=[np.number]).copy()\n",
    "X_all = X_all.dropna(axis=1, how=\"all\")\n",
    "X_all = X_all.loc[:, X_all.nunique(dropna=True) > 1]\n",
    "\n",
    "# ---- HARD GUARDS: enforce your constraints ----\n",
    "bad_patterns = [\n",
    "    \"state_emp\", \"Employment\", \"econ_emp\", \"emp_rate\", \"emp_per\", \"hire_to_emp\",\n",
    "    \"growth_\", \"log_\", \"_yoy\", \"_z\", \"us_\", \"rel_\"  # allow rel_* only as Y, not X\n",
    "]\n",
    "bad_in_X = [c for c in X_all.columns if any(p.lower() in c.lower() for p in bad_patterns)]\n",
    "if bad_in_X:\n",
    "    raise ValueError(f\"Disallowed columns ended up in X: {bad_in_X}\")\n",
    "\n",
    "print(\"Target:\", target)\n",
    "print(\"Rows:\", len(df_model), \"| Features:\", X_all.shape[1])\n",
    "print(\"X columns:\", X_all.columns.tolist())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) TIME SPLIT: Test = 2022-2024 only\n",
    "# ============================================================\n",
    "test_years = {2022, 2023, 2024}\n",
    "train_mask = ~df_model[\"Year\"].isin(test_years)\n",
    "test_mask  =  df_model[\"Year\"].isin(test_years)\n",
    "\n",
    "X_train, X_test = X_all.loc[train_mask].copy(), X_all.loc[test_mask].copy()\n",
    "y_train, y_test = y_all[train_mask.values], y_all[test_mask.values]\n",
    "years_train = df_model.loc[train_mask, \"Year\"].astype(int).values\n",
    "\n",
    "print(\"\\nTrain years:\", sorted(df_model.loc[train_mask, \"Year\"].unique()))\n",
    "print(\"Test years :\", sorted(df_model.loc[test_mask, \"Year\"].unique()))\n",
    "print(\"Train n:\", X_train.shape[0], \"| Test n:\", X_test.shape[0])\n",
    "\n",
    "# Walk-forward folds on TRAIN only:\n",
    "# - folds_tune used for hyperparam selection (validate later years in train window)\n",
    "# - folds_oof used for honest OOF estimation (drops early years with no prior train history)\n",
    "folds_tune = make_walk_forward_folds(years_train, min_train_years=4)  # validate 2018-2021\n",
    "folds_oof  = make_walk_forward_folds(years_train, min_train_years=3)  # validate 2017-2021\n",
    "print(\"folds_tune:\", len(folds_tune), \"| folds_oof:\", len(folds_oof))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) MODEL A: LASSO (main, interpretable) + choose alpha by OOF sanity\n",
    "# ============================================================\n",
    "lasso_cv = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lasso_cv\", LassoCV(cv=folds_tune, random_state=42, n_alphas=300, max_iter=400000))\n",
    "])\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "alpha_min = float(lasso_cv.named_steps[\"lasso_cv\"].alpha_)\n",
    "alpha_1se = pick_alpha_1se_from_lassocv(lasso_cv.named_steps[\"lasso_cv\"])\n",
    "\n",
    "print(\"\\nLasso alpha_min:\", alpha_min, \"| alpha_1se:\", alpha_1se)\n",
    "\n",
    "lasso_min = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lasso\", Lasso(alpha=alpha_min, max_iter=500000, random_state=42))\n",
    "])\n",
    "lasso_1se = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lasso\", Lasso(alpha=alpha_1se, max_iter=500000, random_state=42))\n",
    "])\n",
    "\n",
    "res_lasso_min = fit_eval_one(lasso_min, X_train, y_train, X_test, y_test, folds_oof, name=\"lasso_alpha_min\")\n",
    "res_lasso_1se = fit_eval_one(lasso_1se, X_train, y_train, X_test, y_test, folds_oof, name=\"lasso_alpha_1se\")\n",
    "\n",
    "# Choose lasso variant by OOF RMSE (more honest), tie-break by test RMSE\n",
    "cand = sorted([res_lasso_min, res_lasso_1se], key=lambda r: (r[\"train_oof\"][\"rmse\"], r[\"test\"][\"rmse\"]))\n",
    "res_lasso = cand[0]\n",
    "print(\"\\nChosen Lasso variant:\", res_lasso[\"model\"],\n",
    "      \"| OOF RMSE:\", res_lasso[\"train_oof\"][\"rmse\"],\n",
    "      \"| Test RMSE:\", res_lasso[\"test\"][\"rmse\"])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) MODEL B: RandomForest (nonlinear baseline) + tune\n",
    "# ============================================================\n",
    "rf_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"rf\", RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "rf_param_dist = {\n",
    "    \"rf__n_estimators\": np.arange(300, 1101, 100),\n",
    "    \"rf__max_depth\": [3, 4, 5, 6, 8, 10, None],\n",
    "    \"rf__min_samples_leaf\": [2, 3, 5, 8, 10, 15, 20],\n",
    "    \"rf__min_samples_split\": [10, 20, 30, 40, 60],\n",
    "    \"rf__max_features\": [0.4, 0.5, 0.6, 0.7, \"sqrt\"],\n",
    "    \"rf__bootstrap\": [True],\n",
    "}\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    rf_pipe,\n",
    "    param_distributions=rf_param_dist,\n",
    "    n_iter=50,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=folds_tune,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n--- Tuning RandomForest (train-only, walk-forward CV) ---\")\n",
    "rf_search.fit(X_train, y_train)\n",
    "rf_best = rf_search.best_estimator_\n",
    "print(\"RF best CV RMSE:\", float(-rf_search.best_score_))\n",
    "print(\"RF best params:\", rf_search.best_params_)\n",
    "\n",
    "res_rf = fit_eval_one(rf_best, X_train, y_train, X_test, y_test, folds_oof, name=\"rf_tuned\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) MODEL C: LightGBM (GBDT) + tune (time-aware CV)\n",
    "# ============================================================\n",
    "lgb_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),  # optional but keeps consistency\n",
    "    (\"lgb\", lgb.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Ranges tuned for small-ish panel data (avoid too-deep trees)\n",
    "lgb_param_dist = {\n",
    "    \"lgb__n_estimators\": np.arange(300, 1601, 100),\n",
    "    \"lgb__learning_rate\": np.array([0.01, 0.02, 0.03, 0.05, 0.08, 0.10]),\n",
    "    \"lgb__num_leaves\": np.array([15, 31, 47, 63, 95, 127]),\n",
    "    \"lgb__max_depth\": np.array([-1, 3, 4, 5, 6, 7]),\n",
    "    \"lgb__min_child_samples\": np.array([10, 15, 20, 30, 40, 60]),\n",
    "    \"lgb__subsample\": np.array([0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "    \"lgb__colsample_bytree\": np.array([0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "    \"lgb__reg_alpha\": np.array([0.0, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0]),\n",
    "    \"lgb__reg_lambda\": np.array([0.0, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]),\n",
    "}\n",
    "\n",
    "lgb_search = RandomizedSearchCV(\n",
    "    lgb_pipe,\n",
    "    param_distributions=lgb_param_dist,\n",
    "    n_iter=60,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=folds_tune,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n--- Tuning LightGBM (train-only, walk-forward CV) ---\")\n",
    "lgb_search.fit(X_train, y_train)\n",
    "lgb_best = lgb_search.best_estimator_\n",
    "print(\"LGB best CV RMSE:\", float(-lgb_search.best_score_))\n",
    "print(\"LGB best params:\", lgb_search.best_params_)\n",
    "\n",
    "res_lgb = fit_eval_one(lgb_best, X_train, y_train, X_test, y_test, folds_oof, name=\"lgb_tuned\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) COMPARE MODELS + PICK BEST (by test_rmse, then oof_rmse sanity)\n",
    "# ============================================================\n",
    "results = [res_lasso, res_rf, res_lgb]\n",
    "\n",
    "rows = []\n",
    "for r in results:\n",
    "    rows.append({\n",
    "        \"model\": r[\"model\"],\n",
    "        \"oof_r2\": None if r[\"train_oof\"] is None else r[\"train_oof\"][\"r2\"],\n",
    "        \"oof_rmse\": None if r[\"train_oof\"] is None else r[\"train_oof\"][\"rmse\"],\n",
    "        \"test_r2\": r[\"test\"][\"r2\"],\n",
    "        \"test_rmse\": r[\"test\"][\"rmse\"],\n",
    "        \"test_mae\": r[\"test\"][\"mae\"],\n",
    "        \"oof_kept\": r[\"oof_kept\"],\n",
    "        \"oof_dropped\": r[\"oof_dropped\"],\n",
    "    })\n",
    "\n",
    "perf = pd.DataFrame(rows).sort_values([\"test_rmse\", \"oof_rmse\"])\n",
    "print(\"\\n=== MODEL COMPARISON (sorted by test_rmse then oof_rmse) ===\")\n",
    "print(perf)\n",
    "\n",
    "best_name = perf.iloc[0][\"model\"]\n",
    "best_fitted = {r[\"model\"]: r[\"fitted\"] for r in results}[best_name]\n",
    "print(\"\\n>>> BEST SINGLE MODEL:\", best_name)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) ENSEMBLES as ROBUSTNESS CHECK (not the headline model)\n",
    "#    - mean ensemble\n",
    "#    - ridge stacking (OOF, leakage-safe)\n",
    "# ============================================================\n",
    "# Recreate fixed (cloneable) model objects for stacking:\n",
    "# IMPORTANT: use the \"fixed\" pipelines (no internal CV) where applicable.\n",
    "lasso_for_stack = clone(lasso_min if res_lasso[\"model\"] == \"lasso_alpha_min\" else lasso_1se)\n",
    "rf_for_stack    = clone(rf_best)\n",
    "lgb_for_stack   = clone(lgb_best)\n",
    "\n",
    "base_fixed = {\"lasso\": lasso_for_stack, \"rf\": rf_for_stack, \"lgb\": lgb_for_stack}\n",
    "\n",
    "# (A) Mean ensemble\n",
    "preds_test = []\n",
    "for m in base_fixed.values():\n",
    "    mf = clone(m).fit(X_train, y_train)\n",
    "    preds_test.append(mf.predict(X_test))\n",
    "pred_mean = np.mean(np.column_stack(preds_test), axis=1)\n",
    "print(\"\\nMean-ensemble TEST metrics:\", eval_metrics(y_test, pred_mean))\n",
    "\n",
    "# (B) Ridge stacking with walk-forward OOF meta-features\n",
    "ens_tr_oof, ens_te, valid_meta, weights, info = stack_ridge_oof(base_fixed, X_train, y_train, X_test, folds_oof)\n",
    "print(\"\\nStacking meta info:\", info)\n",
    "print(\"Stacking weights:\\n\", weights)\n",
    "print(\"Stacking TRAIN OOF metrics:\", eval_metrics(y_train[valid_meta], ens_tr_oof[valid_meta]))\n",
    "print(\"Stacking TEST metrics:\", eval_metrics(y_test, ens_te))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Rolling-origin (per-year) diagnostics (robustness & break detection)\n",
    "# ============================================================\n",
    "print(\"\\n=== Rolling-origin per-year diagnostics ===\")\n",
    "start_year = 2017\n",
    "end_year = 2024\n",
    "\n",
    "roll_tables = []\n",
    "for name, m in base_fixed.items():\n",
    "    tab = rolling_origin_eval(m, df_model, X_all, y_all, start_year=start_year, end_year=end_year)\n",
    "    tab[\"model\"] = name\n",
    "    roll_tables.append(tab)\n",
    "\n",
    "roll_all = pd.concat(roll_tables, ignore_index=True).sort_values([\"model\", \"test_year\"])\n",
    "print(roll_all)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) Interpretability hook for paper (Lasso coefficients)\n",
    "# ============================================================\n",
    "print(\"\\n=== LASSO COEFFICIENTS (chosen variant) ===\")\n",
    "chosen_lasso_fitted = res_lasso[\"fitted\"]\n",
    "coef = chosen_lasso_fitted.named_steps[\"lasso\"].coef_\n",
    "coef_s = pd.Series(coef, index=X_train.columns).sort_values(key=lambda s: np.abs(s), ascending=False)\n",
    "\n",
    "print(\"Non-zero coefficients:\", int((coef_s != 0).sum()), \"out of\", len(coef_s))\n",
    "print(\"\\nTop 20 by |coef|:\")\n",
    "print(coef_s.head(20))\n",
    "\n",
    "# Optional: export\n",
    "# coef_s.to_csv(\"lasso_coefficients_rel_emp_idx.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d9fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Global interpretability for the STACKED ensemble\n",
    "#  - Feature-space attributions (magnitude + direction)\n",
    "#  - Leakage-safe background: TRAIN only (2014-2021)\n",
    "#  - Explain set: TEST only (2022-2024) by default\n",
    "#\n",
    "# Requires objects from your modeling cell:\n",
    "#   df_model, X_all, y_all\n",
    "#   train_mask, test_mask\n",
    "#   base_fixed  ({\"lasso\":..., \"rf\":..., \"lgb\":...})  [cloneable pipelines]\n",
    "#   weights     (pd.Series with index [\"lasso\",\"rf\",\"lgb\"])\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- SHAP import ---\n",
    "try:\n",
    "    import shap\n",
    "except ImportError:\n",
    "    raise ImportError(\"Install SHAP first: pip install shap\")\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers: consistent transforms and SHAP per model\n",
    "# ----------------------------\n",
    "def _get_feature_names(X: pd.DataFrame):\n",
    "    return list(X.columns)\n",
    "\n",
    "def _impute_only(pipe, X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return imputed X as DataFrame.\"\"\"\n",
    "    imp = pipe.named_steps[\"imputer\"]\n",
    "    X_imp = pd.DataFrame(imp.transform(X), columns=_get_feature_names(X), index=X.index)\n",
    "    return X_imp\n",
    "\n",
    "def linear_shap_from_lasso_pipeline(lasso_pipe, X_background: pd.DataFrame, X_explain: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Exact additive decomposition for Pipeline(imputer, scaler, lasso) in the *original feature names*.\n",
    "    Works by computing contributions in standardized space:\n",
    "        phi_j = coef_j * (x_scaled_j - E[x_scaled_j])\n",
    "    Returns:\n",
    "        shap_vals: (n, p) array\n",
    "        base_value: float\n",
    "        X_explain_imp: DataFrame (imputed, original units)\n",
    "    \"\"\"\n",
    "    imp = lasso_pipe.named_steps[\"imputer\"]\n",
    "    sc  = lasso_pipe.named_steps[\"scaler\"]\n",
    "    lm  = lasso_pipe.named_steps[\"lasso\"]\n",
    "\n",
    "    # impute\n",
    "    Xb_imp = pd.DataFrame(imp.transform(X_background), columns=_get_feature_names(X_background), index=X_background.index)\n",
    "    Xe_imp = pd.DataFrame(imp.transform(X_explain),    columns=_get_feature_names(X_explain),    index=X_explain.index)\n",
    "\n",
    "    # scale\n",
    "    Xb_sc = sc.transform(Xb_imp)\n",
    "    Xe_sc = sc.transform(Xe_imp)\n",
    "\n",
    "    mean_sc = np.nanmean(Xb_sc, axis=0)  # background mean in standardized space\n",
    "    coefs = np.asarray(lm.coef_, dtype=float)\n",
    "\n",
    "    shap_vals = (Xe_sc - mean_sc) * coefs  # (n, p)\n",
    "    base_value = float(lm.intercept_ + np.dot(mean_sc, coefs))\n",
    "    return shap_vals, base_value, Xe_imp\n",
    "\n",
    "def tree_shap_from_imputed_pipeline(tree_pipe, model_step_name: str,\n",
    "                                   X_background: pd.DataFrame, X_explain: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    SHAP for Pipeline(imputer, <tree_model>) using TreeExplainer.\n",
    "    Returns:\n",
    "        shap_vals: (n, p) array\n",
    "        base_value: float\n",
    "        Xe_imp: DataFrame (imputed)\n",
    "    \"\"\"\n",
    "    imp = tree_pipe.named_steps[\"imputer\"]\n",
    "    model = tree_pipe.named_steps[model_step_name]\n",
    "\n",
    "    Xb_imp = pd.DataFrame(imp.transform(X_background), columns=_get_feature_names(X_background), index=X_background.index)\n",
    "    Xe_imp = pd.DataFrame(imp.transform(X_explain),    columns=_get_feature_names(X_explain),    index=X_explain.index)\n",
    "\n",
    "    explainer = shap.TreeExplainer(model, data=Xb_imp, feature_perturbation=\"interventional\")\n",
    "    shap_vals = explainer.shap_values(Xe_imp)\n",
    "    # shap_vals may be list for multioutput; but you're regression so ensure array\n",
    "    shap_vals = np.asarray(shap_vals)\n",
    "    base_value = float(np.asarray(explainer.expected_value).reshape(-1)[0])\n",
    "    return shap_vals, base_value, Xe_imp\n",
    "\n",
    "def combine_ensemble_shap(shap_by_model: dict, base_by_model: dict, meta_weights: pd.Series, meta_intercept: float = 0.0):\n",
    "    \"\"\"\n",
    "    Combine base-model SHAP values into stacked ensemble SHAP values:\n",
    "        f_ens(x) = meta_intercept + sum_m w_m f_m(x)\n",
    "        phi_ens  = sum_m w_m phi_m\n",
    "        base_ens = meta_intercept + sum_m w_m base_m\n",
    "    \"\"\"\n",
    "    # ensure aligned order and same feature dimension\n",
    "    keys = list(meta_weights.index)\n",
    "    first = keys[0]\n",
    "    n, p = shap_by_model[first].shape\n",
    "\n",
    "    shap_ens = np.zeros((n, p), dtype=float)\n",
    "    base_ens = float(meta_intercept)\n",
    "    for k in keys:\n",
    "        w = float(meta_weights[k])\n",
    "        shap_ens += w * np.asarray(shap_by_model[k], dtype=float)\n",
    "        base_ens += w * float(base_by_model[k])\n",
    "    return shap_ens, base_ens\n",
    "\n",
    "def global_shap_tables(shap_vals: np.ndarray, feature_names: list):\n",
    "    \"\"\"\n",
    "    Returns two pd.Series:\n",
    "      - mean_abs_shap: importance magnitude\n",
    "      - mean_shap: signed direction (average contribution)\n",
    "    \"\"\"\n",
    "    mean_abs = pd.Series(np.abs(shap_vals).mean(axis=0), index=feature_names).sort_values(ascending=False)\n",
    "    mean_signed = pd.Series(shap_vals.mean(axis=0), index=feature_names).loc[mean_abs.index]\n",
    "    return mean_abs, mean_signed\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Define background (TRAIN only) and explain set (TEST only by default)\n",
    "# ----------------------------\n",
    "X_train_bg = X_all.loc[train_mask].copy()   # background: 2014-2021\n",
    "X_explain  = X_all.loc[test_mask].copy()    # explain: 2022-2024 holdout\n",
    "\n",
    "feature_names = _get_feature_names(X_train_bg)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Fit base models on full TRAIN (no leakage into TEST)\n",
    "# ----------------------------\n",
    "base_fitted = {}\n",
    "for name, pipe in base_fixed.items():\n",
    "    base_fitted[name] = pipe.fit(X_train_bg, y_all[train_mask.values])\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Compute SHAP per base model in original feature space\n",
    "# ----------------------------\n",
    "shap_by_model = {}\n",
    "baseval_by_model = {}\n",
    "\n",
    "# LASSO (pipeline with scaler)\n",
    "sh_lasso, bv_lasso, Xe_imp_lasso = linear_shap_from_lasso_pipeline(\n",
    "    base_fitted[\"lasso\"], X_train_bg, X_explain\n",
    ")\n",
    "shap_by_model[\"lasso\"] = sh_lasso\n",
    "baseval_by_model[\"lasso\"] = bv_lasso\n",
    "\n",
    "# RF (tree)\n",
    "sh_rf, bv_rf, Xe_imp_rf = tree_shap_from_imputed_pipeline(\n",
    "    base_fitted[\"rf\"], model_step_name=\"rf\", X_background=X_train_bg, X_explain=X_explain\n",
    ")\n",
    "shap_by_model[\"rf\"] = sh_rf\n",
    "baseval_by_model[\"rf\"] = bv_rf\n",
    "\n",
    "# LGB (tree)\n",
    "sh_lgb, bv_lgb, Xe_imp_lgb = tree_shap_from_imputed_pipeline(\n",
    "    base_fitted[\"lgb\"], model_step_name=\"lgb\", X_background=X_train_bg, X_explain=X_explain\n",
    ")\n",
    "shap_by_model[\"lgb\"] = sh_lgb\n",
    "baseval_by_model[\"lgb\"] = bv_lgb\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Combine into stacked-ensemble SHAP using your stacking weights\n",
    "#    (Meta intercept is unknown here because you didnt keep the fitted meta model.\n",
    "#     Thats OK for importance ranking; SHAP magnitudes are unaffected.)\n",
    "# ----------------------------\n",
    "sh_ens, base_ens = combine_ensemble_shap(\n",
    "    shap_by_model=shap_by_model,\n",
    "    base_by_model=baseval_by_model,\n",
    "    meta_weights=weights,\n",
    "    meta_intercept=0.0\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Global tables: magnitude + direction (on TEST years 2022-2024)\n",
    "# ----------------------------\n",
    "ens_mean_abs, ens_mean_signed = global_shap_tables(sh_ens, feature_names)\n",
    "\n",
    "ens_global_table = pd.DataFrame({\n",
    "    \"mean_abs_shap_ens\": ens_mean_abs,\n",
    "    \"mean_shap_signed_ens\": ens_mean_signed\n",
    "})\n",
    "\n",
    "print(\"\\n=== STACKED ENSEMBLE GLOBAL (TEST 2022-2024) SHAP ===\")\n",
    "print(\"Stacking weights used:\\n\", weights.sort_values(ascending=False))\n",
    "print(\"\\nTop 20 features by ensemble mean(|SHAP|):\")\n",
    "print(ens_global_table.head(20))\n",
    "\n",
    "# Optional: base-model comparison tables\n",
    "for m in [\"lasso\", \"rf\", \"lgb\"]:\n",
    "    m_abs, m_signed = global_shap_tables(shap_by_model[m], feature_names)\n",
    "    tab = pd.DataFrame({\"mean_abs_shap\": m_abs, \"mean_shap_signed\": m_signed})\n",
    "    print(f\"\\n--- {m.upper()} global SHAP (TEST 2022-2024), top 10 ---\")\n",
    "    print(tab.head(10))\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Optional plots (comment out if running headless)\n",
    "# ----------------------------\n",
    "# Xe_imp_for_plot = Xe_imp_lgb  # any imputed X with same columns is fine for plotting\n",
    "# shap.summary_plot(sh_ens, Xe_imp_for_plot, show=False)            # direction + magnitude\n",
    "# shap.summary_plot(sh_ens, Xe_imp_for_plot, plot_type=\"bar\", show=False)  # magnitude only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9691b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Stability of GLOBAL ensemble ranking (rolling-origin SHAP)\n",
    "#  - For each year t:\n",
    "#      train = years <= t-1\n",
    "#      test  = year == t\n",
    "#      fit base models on train\n",
    "#      fit ridge stacker using walk-forward OOF preds in train (leakage-safe)\n",
    "#      compute ensemble SHAP importance on test year t\n",
    "#  - Summarize stability across years: top-k freq, mean rank, meanstd importance\n",
    "#\n",
    "# Uses your existing helper functions:\n",
    "#   make_walk_forward_folds, oof_preds_walk_forward\n",
    "# and uses the same base_fixed pipelines (already tuned) for consistency.\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure shap imported\n",
    "try:\n",
    "    import shap\n",
    "except ImportError:\n",
    "    raise ImportError(\"Install SHAP first: pip install shap\")\n",
    "\n",
    "def fit_meta_ridge_from_oof(base_models_fixed: dict, X_tr: pd.DataFrame, y_tr: np.ndarray,\n",
    "                           years_tr: np.ndarray, min_train_years_oof: int = 3, meta_alphas=None):\n",
    "    \"\"\"\n",
    "    Fit ridge stacker using leakage-safe walk-forward OOF predictions within TRAIN.\n",
    "    Returns:\n",
    "        meta_model (fitted RidgeCV),\n",
    "        meta_weights (pd.Series),\n",
    "        meta_intercept (float),\n",
    "        base_fitted_full_train (dict of fitted base pipelines)\n",
    "    \"\"\"\n",
    "    if meta_alphas is None:\n",
    "        meta_alphas = np.logspace(-3, 3, 19)\n",
    "\n",
    "    folds_oof = make_walk_forward_folds(years_tr, min_train_years=min_train_years_oof)\n",
    "\n",
    "    # OOF meta-features\n",
    "    meta_train = np.column_stack([\n",
    "        oof_preds_walk_forward(m, X_tr, y_tr, folds_oof)\n",
    "        for m in base_models_fixed.values()\n",
    "    ])\n",
    "    valid = np.all(np.isfinite(meta_train), axis=1)\n",
    "    if valid.sum() < 60:\n",
    "        raise RuntimeError(f\"Too few OOF rows for stacking meta-learner: kept={int(valid.sum())}\")\n",
    "\n",
    "    # Fit base models on full train for later prediction + SHAP\n",
    "    base_full = {k: m.fit(X_tr, y_tr) for k, m in base_models_fixed.items()}\n",
    "\n",
    "    # Fit meta model (ridge)\n",
    "    from sklearn.linear_model import RidgeCV\n",
    "    meta = RidgeCV(alphas=meta_alphas, cv=5)\n",
    "    meta.fit(meta_train[valid], y_tr[valid])\n",
    "\n",
    "    w = pd.Series(meta.coef_, index=list(base_models_fixed.keys()))\n",
    "    b0 = float(meta.intercept_)\n",
    "    return meta, w, b0, base_full\n",
    "\n",
    "def shap_for_base_models(base_full: dict, X_bg: pd.DataFrame, X_te: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compute SHAP values for each base model on X_te with background X_bg (both are DataFrames).\n",
    "    Returns:\n",
    "      shap_by_model: dict name -> (n_test, p) array\n",
    "      base_by_model: dict name -> base_value float\n",
    "      X_te_imp_any:  imputed DataFrame with feature names (usable for plots)\n",
    "    \"\"\"\n",
    "    shap_by_model = {}\n",
    "    base_by_model = {}\n",
    "    X_te_imp_any = None\n",
    "\n",
    "    # LASSO\n",
    "    sh_lasso, bv_lasso, X_te_imp_lasso = linear_shap_from_lasso_pipeline(\n",
    "        base_full[\"lasso\"], X_bg, X_te\n",
    "    )\n",
    "    shap_by_model[\"lasso\"] = sh_lasso\n",
    "    base_by_model[\"lasso\"] = bv_lasso\n",
    "    X_te_imp_any = X_te_imp_lasso\n",
    "\n",
    "    # RF\n",
    "    sh_rf, bv_rf, X_te_imp_rf = tree_shap_from_imputed_pipeline(\n",
    "        base_full[\"rf\"], \"rf\", X_bg, X_te\n",
    "    )\n",
    "    shap_by_model[\"rf\"] = sh_rf\n",
    "    base_by_model[\"rf\"] = bv_rf\n",
    "    X_te_imp_any = X_te_imp_any if X_te_imp_any is not None else X_te_imp_rf\n",
    "\n",
    "    # LGB\n",
    "    sh_lgb, bv_lgb, X_te_imp_lgb = tree_shap_from_imputed_pipeline(\n",
    "        base_full[\"lgb\"], \"lgb\", X_bg, X_te\n",
    "    )\n",
    "    shap_by_model[\"lgb\"] = sh_lgb\n",
    "    base_by_model[\"lgb\"] = bv_lgb\n",
    "    X_te_imp_any = X_te_imp_any if X_te_imp_any is not None else X_te_imp_lgb\n",
    "\n",
    "    return shap_by_model, base_by_model, X_te_imp_any\n",
    "\n",
    "def rank_series_desc(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"1 = most important. Ties get average rank.\"\"\"\n",
    "    return s.rank(ascending=False, method=\"average\")\n",
    "\n",
    "# ----------------------------\n",
    "# Rolling-origin SHAP stability\n",
    "# ----------------------------\n",
    "years_all = df_model[\"Year\"].astype(int).values\n",
    "feature_names = list(X_all.columns)\n",
    "\n",
    "start_year = 2018\n",
    "end_year = 2024\n",
    "top_k = 10\n",
    "\n",
    "per_year_tables = []\n",
    "per_year_rank = []\n",
    "\n",
    "for t in range(start_year, end_year + 1):\n",
    "    tr_mask = years_all <= (t - 1)\n",
    "    te_mask = years_all == t\n",
    "\n",
    "    if tr_mask.sum() < 120 or te_mask.sum() < 30:\n",
    "        continue\n",
    "\n",
    "    X_tr = X_all.loc[tr_mask].copy()\n",
    "    y_tr = y_all[tr_mask]\n",
    "    years_tr = years_all[tr_mask]\n",
    "\n",
    "    X_te = X_all.loc[te_mask].copy()\n",
    "\n",
    "    try:\n",
    "        meta, w_t, b0_t, base_full = fit_meta_ridge_from_oof(\n",
    "            base_models_fixed=base_fixed,\n",
    "            X_tr=X_tr,\n",
    "            y_tr=y_tr,\n",
    "            years_tr=years_tr,\n",
    "            min_train_years_oof=3\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP year {t}] stacking fit failed: {e}\")\n",
    "        continue\n",
    "\n",
    "    # SHAP per base model, background = X_tr only (no leakage)\n",
    "    shap_by_model, base_by_model, X_te_imp_any = shap_for_base_models(base_full, X_tr, X_te)\n",
    "\n",
    "    # Combine to ensemble SHAP for year t\n",
    "    sh_ens_t, base_ens_t = combine_ensemble_shap(shap_by_model, base_by_model, w_t, meta_intercept=b0_t)\n",
    "\n",
    "    # Compute year-t global importance (mean abs) + direction (mean signed) on test year t\n",
    "    mean_abs_t = pd.Series(np.abs(sh_ens_t).mean(axis=0), index=feature_names).sort_values(ascending=False)\n",
    "    mean_signed_t = pd.Series(sh_ens_t.mean(axis=0), index=feature_names).loc[mean_abs_t.index]\n",
    "\n",
    "    tab_t = pd.DataFrame({\n",
    "        \"test_year\": t,\n",
    "        \"feature\": mean_abs_t.index,\n",
    "        \"mean_abs_shap\": mean_abs_t.values,\n",
    "        \"mean_shap_signed\": mean_signed_t.values\n",
    "    })\n",
    "    per_year_tables.append(tab_t)\n",
    "\n",
    "    # ranking for stability\n",
    "    ranks_t = rank_series_desc(mean_abs_t)\n",
    "    per_year_rank.append(pd.DataFrame({\"test_year\": t, \"feature\": ranks_t.index, \"rank\": ranks_t.values}))\n",
    "\n",
    "# Combine all year tables\n",
    "if not per_year_tables:\n",
    "    raise RuntimeError(\"No rolling-origin SHAP tables were produced. Check sample sizes / years / folds.\")\n",
    "\n",
    "imp_long = pd.concat(per_year_tables, ignore_index=True)\n",
    "rank_long = pd.concat(per_year_rank, ignore_index=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Stability summary across years\n",
    "# ----------------------------\n",
    "years_used = sorted(imp_long[\"test_year\"].unique())\n",
    "n_years_used = len(years_used)\n",
    "\n",
    "# Top-k frequency\n",
    "topk_flags = (\n",
    "    rank_long.assign(in_topk=lambda d: d[\"rank\"] <= top_k)\n",
    "             .groupby(\"feature\")[\"in_topk\"].mean()\n",
    "             .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# Mean rank + variability\n",
    "rank_stats = (rank_long.groupby(\"feature\")[\"rank\"]\n",
    "                      .agg(mean_rank=\"mean\", std_rank=\"std\", n_years=\"count\"))\n",
    "\n",
    "# Mean(|SHAP|) summary\n",
    "imp_stats = (imp_long.groupby(\"feature\")[\"mean_abs_shap\"]\n",
    "                    .agg(mean_abs_shap_mean=\"mean\",\n",
    "                         mean_abs_shap_std=\"std\",\n",
    "                         mean_abs_shap_median=\"median\"))\n",
    "\n",
    "# Signed direction summary\n",
    "dir_stats = (imp_long.groupby(\"feature\")[\"mean_shap_signed\"]\n",
    "                    .agg(mean_signed_mean=\"mean\", mean_signed_std=\"std\"))\n",
    "\n",
    "stability = (imp_stats.join(dir_stats, how=\"left\")\n",
    "                     .join(rank_stats, how=\"left\")\n",
    "                     .join(topk_flags.rename(f\"top{top_k}_freq\"), how=\"left\")\n",
    "                     .sort_values([\"top10_freq\" if top_k == 10 else f\"top{top_k}_freq\",\n",
    "                                   \"mean_abs_shap_mean\"], ascending=[False, False]))\n",
    "\n",
    "print(f\"\\n=== Rolling-origin ensemble SHAP stability ({start_year}-{end_year}) ===\")\n",
    "print(\"Years used:\", years_used, \"| count:\", n_years_used)\n",
    "print(f\"\\nTop 20 stable features (by top-{top_k} frequency, then mean importance):\")\n",
    "print(stability.head(20))\n",
    "\n",
    "# Optional: show per-year top-k lists (quick sanity)\n",
    "print(\"\\nPer-year top features (top 8 by mean|SHAP|):\")\n",
    "for y in years_used:\n",
    "    top8 = (imp_long[imp_long[\"test_year\"] == y]\n",
    "            .sort_values(\"mean_abs_shap\", ascending=False)\n",
    "            .head(8)[[\"feature\", \"mean_abs_shap\", \"mean_shap_signed\"]])\n",
    "    print(f\"\\nYear {y}:\\n\", top8.to_string(index=False))\n",
    "\n",
    "# Optional: export for paper tables\n",
    "# stability.to_csv(\"ensemble_shap_stability_summary.csv\", index=True)\n",
    "# imp_long.to_csv(\"ensemble_shap_importance_by_year.csv\", index=False)\n",
    "# rank_long.to_csv(\"ensemble_shap_rank_by_year.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0bd94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SETUP: Define your column lists carefully\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1) RAW COUNTS -> SUM across counties (after de-dup at county-year)\n",
    "health_count_cols = [\n",
    "    '# Alcohol-Impaired Driving Deaths',\n",
    "    '# Driving Deaths',\n",
    "    'Premature Deaths'   # confirm this is a count (not a rate)\n",
    "]\n",
    "\n",
    "# 2) RATES / INDICES / PERCENTS -> POP-WEIGHTED average across counties\n",
    "health_rate_cols = [\n",
    "    '% Adults with Obesity',\n",
    "    '% Children in Poverty',\n",
    "    '% Children in Single-Parent Households',\n",
    "    '% Drive Alone to Work',\n",
    "    '% Excessive Drinking',\n",
    "    '% Fair or Poor Health',\n",
    "    '% Long Commute - Drives Alone',\n",
    "    '% Low Birthweight',\n",
    "    '% Severe Housing Problems',\n",
    "    '% Smokers',\n",
    "    '% Some College',\n",
    "    '% Unemployed',\n",
    "    '% Uninsured',\n",
    "    '% With Access to Exercise Opportunities',\n",
    "    'Food Environment Index',\n",
    "    'Income Ratio',\n",
    "    'Mentally Unhealthy Days',\n",
    "    'Physically Unhealthy Days',\n",
    "    'Preventable Hospitalization Rate',\n",
    "    'Primary Care Physicians Rate',\n",
    "    'Social Association Rate',\n",
    "    'Teen Birth Rate',\n",
    "    'Violent Crime Rate'\n",
    "]\n",
    "\n",
    "# 3) Numeric columns to coerce\n",
    "numeric_cols = [\n",
    "    'Year', 'quarter', 'FIPS', 'Population',\n",
    "    'Employment_Count', 'New_Hires', 'Earnings_Beginning_Qtr'\n",
    "] + health_count_cols + health_rate_cols\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 0: PRE-PROCESSING\n",
    "# ---------------------------------------------------------\n",
    "df = merged_full.copy()\n",
    "\n",
    "# Coerce numeric types\n",
    "for c in numeric_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "# Filter Year Window\n",
    "df = df[(df['Year'] >= 2014) & (df['Year'] <= 2024)].copy()\n",
    "\n",
    "# Quarter normalization (in case your data uses 0..3)\n",
    "if 'quarter' in df.columns:\n",
    "    qmin, qmax = df['quarter'].min(), df['quarter'].max()\n",
    "    if pd.notna(qmin) and pd.notna(qmax) and qmin >= 0 and qmax <= 3:\n",
    "        df['quarter'] = df['quarter'] + 1\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 1: HEALTH DATA AGGREGATION (County-Year -> State-Year)\n",
    "#   IMPORTANT: do NOT \"sum\" health across duplicate industry/quarter rows.\n",
    "#   First, collapse to unique county-year with FIRST (or median).\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "health_keep = ['State', 'Year', 'FIPS', 'Population'] + health_count_cols + health_rate_cols\n",
    "county_health_raw = df[health_keep].copy()\n",
    "\n",
    "# Robust de-dup: collapse to one row per county-year\n",
    "# Use 'first' since values should be identical across duplicated rows.\n",
    "county_health = (\n",
    "    county_health_raw\n",
    "    .sort_values(['State', 'FIPS', 'Year'])\n",
    "    .groupby(['State', 'Year', 'FIPS'], as_index=False)\n",
    "    .agg({**{'Population': 'first'},\n",
    "          **{c: 'first' for c in health_count_cols},\n",
    "          **{c: 'first' for c in health_rate_cols}})\n",
    ")\n",
    "\n",
    "# Create numerators for weighted averages\n",
    "for col in health_rate_cols:\n",
    "    county_health[f'{col}_num'] = county_health[col] * county_health['Population']\n",
    "\n",
    "# Aggregate to state-year\n",
    "agg_dict = {\n",
    "    'state_total_pop': ('Population', 'sum'),\n",
    "}\n",
    "\n",
    "for col in health_count_cols:\n",
    "    agg_dict[f'STATE_{col}'] = (col, 'sum')\n",
    "\n",
    "for col in health_rate_cols:\n",
    "    agg_dict[f'{col}_num_sum'] = (f'{col}_num', 'sum')\n",
    "\n",
    "state_health = county_health.groupby(['State', 'Year'], as_index=False).agg(**agg_dict)\n",
    "\n",
    "# Final pop-weighted rates\n",
    "for col in health_rate_cols:\n",
    "    state_health[f'STATE_{col}'] = state_health[f'{col}_num_sum'] / state_health['state_total_pop']\n",
    "    state_health.drop(columns=[f'{col}_num_sum'], inplace=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 2: ECON DATA AGGREGATION (County-Quarter-Industry -> State-Year)\n",
    "#   Key choices for annual:\n",
    "#   - hires: SUM over quarters (flow)\n",
    "#   - employment: MEAN over quarters (level/stock)\n",
    "#   - earnings: employment-weighted across all quarters (person-quarter weighted)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Exclude industry '00' to avoid double counting totals\n",
    "df_econ = df[df['industry'] != '00'].copy()\n",
    "\n",
    "# Wage bill for correct aggregation of average earnings\n",
    "df_econ['wage_bill'] = df_econ['Earnings_Beginning_Qtr'] * df_econ['Employment_Count']\n",
    "\n",
    "# 2A) First build State-Quarter totals (summing across counties + industries)\n",
    "state_qtr = (\n",
    "    df_econ.groupby(['State', 'Year', 'quarter'], as_index=False)\n",
    "    .agg(\n",
    "        state_emp_qtr=('Employment_Count', 'sum'),\n",
    "        state_hires_qtr=('New_Hires', 'sum'),\n",
    "        state_wage_bill_qtr=('wage_bill', 'sum')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Quarter-level avg earnings (optional)\n",
    "state_qtr['state_avg_earnings_qtr'] = state_qtr['state_wage_bill_qtr'] / state_qtr['state_emp_qtr'].replace({0: np.nan})\n",
    "\n",
    "# 2B) Now aggregate State-Quarter -> State-Year\n",
    "state_annual = (\n",
    "    state_qtr.groupby(['State', 'Year'], as_index=False)\n",
    "    .agg(\n",
    "        # employment level: average across quarters\n",
    "        state_emp_avg=('state_emp_qtr', 'mean'),\n",
    "        # also keep sum across quarters (person-quarters), useful for weighting\n",
    "        state_emp_sum=('state_emp_qtr', 'sum'),\n",
    "\n",
    "        # hires: flow total in the year\n",
    "        state_hires_total=('state_hires_qtr', 'sum'),\n",
    "\n",
    "        # wage bill total in the year (sum of person-quarter wage bills)\n",
    "        state_wage_bill_total=('state_wage_bill_qtr', 'sum'),\n",
    "\n",
    "        # optional: average quarterly earnings level (simple average of quarter avg)\n",
    "        state_avg_earnings_meanq=('state_avg_earnings_qtr', 'mean'),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Annual employment-weighted earnings across quarters (recommended)\n",
    "# Interpretation: average earnings per worker-quarter within the year\n",
    "state_annual['state_avg_earnings'] = state_annual['state_wage_bill_total'] / state_annual['state_emp_sum'].replace({0: np.nan})\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 3: MERGE Annual Econ + Annual Health\n",
    "# ---------------------------------------------------------\n",
    "state_df = state_annual.merge(state_health, on=['State', 'Year'], how='left')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 4: ANNUAL FEATURE ENGINEERING\n",
    "# ---------------------------------------------------------\n",
    "pop = state_df['state_total_pop'].replace({0: np.nan})\n",
    "emp_avg = state_df['state_emp_avg'].replace({0: np.nan})\n",
    "\n",
    "# Per-capita / rates\n",
    "state_df['econ_emp_per_1k'] = 1000 * state_df['state_emp_avg'] / pop\n",
    "state_df['econ_hires_per_1k'] = 1000 * state_df['state_hires_total'] / pop\n",
    "\n",
    "# Annual hires per worker-year (uses avg employment level)\n",
    "state_df['econ_hire_rate_annual'] = state_df['state_hires_total'] / emp_avg\n",
    "\n",
    "# Optional: YoY growth on annual series (lag 1 year)\n",
    "state_df = state_df.sort_values(['State', 'Year']).copy()\n",
    "\n",
    "def growth_yoy(x):\n",
    "    prev = x.shift(1)\n",
    "    return (x - prev) / prev.replace({0: np.nan})\n",
    "\n",
    "state_df['growth_emp_yoy'] = state_df.groupby('State')['state_emp_avg'].transform(growth_yoy)\n",
    "state_df['growth_earn_yoy'] = state_df.groupby('State')['state_avg_earnings'].transform(growth_yoy)\n",
    "state_df['growth_hires_yoy'] = state_df.groupby('State')['state_hires_total'].transform(growth_yoy)\n",
    "\n",
    "# Cleanup\n",
    "state_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 5: FINAL OUTPUT\n",
    "# ---------------------------------------------------------\n",
    "base_cols = [\n",
    "    'State', 'Year',\n",
    "    'state_total_pop',\n",
    "    # Econ (annual)\n",
    "    'state_emp_avg', 'state_emp_sum', 'state_hires_total',\n",
    "    'state_avg_earnings', 'state_avg_earnings_meanq',\n",
    "    'econ_emp_per_1k', 'econ_hires_per_1k', 'econ_hire_rate_annual',\n",
    "    'growth_emp_yoy', 'growth_earn_yoy', 'growth_hires_yoy'\n",
    "]\n",
    "\n",
    "health_cols_out = [c for c in state_df.columns if c.startswith('STATE_')]\n",
    "\n",
    "final_cols = base_cols + health_cols_out\n",
    "state_df_annual_final = state_df[final_cols].copy()\n",
    "\n",
    "print(f\"Annual Aggregated DataFrame Shape: {state_df_annual_final.shape}\")\n",
    "print(\"Sample Columns:\", state_df_annual_final.columns.tolist()[:12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bdb812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc480cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df_annual_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b29a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df_annual_final.to_csv(\"CH_ECON_V4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd4bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# SETUP: Define your column lists carefully\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. Variables that are RAW COUNTS -> We will SUM these\n",
    "# Note: I included 'Premature Deaths' here assuming it is a count. \n",
    "# If it is a rate (YPLL Rate), move it to the rate_cols list.\n",
    "health_count_cols = [\n",
    "    '# Alcohol-Impaired Driving Deaths', \n",
    "    '# Driving Deaths',\n",
    "    'Premature Deaths' \n",
    "]\n",
    "\n",
    "# 2. Variables that are RATES / INDICES / PERCENTS -> We will WEIGHTED AVERAGE these\n",
    "health_rate_cols = [\n",
    "    '% Adults with Obesity', \n",
    "    '% Children in Poverty',\n",
    "    '% Children in Single-Parent Households', \n",
    "    '% Drive Alone to Work',\n",
    "    '% Excessive Drinking', \n",
    "    '% Fair or Poor Health',\n",
    "    '% Long Commute - Drives Alone', \n",
    "    '% Low Birthweight',\n",
    "    '% Severe Housing Problems', \n",
    "    '% Smokers', \n",
    "    '% Some College',\n",
    "    '% Unemployed', \n",
    "    '% Uninsured',\n",
    "    '% With Access to Exercise Opportunities', \n",
    "    'Food Environment Index',\n",
    "    'Income Ratio', \n",
    "    'Mentally Unhealthy Days', \n",
    "    'Physically Unhealthy Days',\n",
    "    'Preventable Hospitalization Rate',\n",
    "    'Primary Care Physicians Rate', \n",
    "    'Social Association Rate',\n",
    "    'Teen Birth Rate', \n",
    "    'Violent Crime Rate'\n",
    "]\n",
    "\n",
    "# 3. Numeric columns to coerce (Standard housekeeping)\n",
    "numeric_cols = [\n",
    "    'Year', 'quarter', 'FIPS', 'Population',\n",
    "    'Employment_Count', 'New_Hires', 'Earnings_Beginning_Qtr'\n",
    "] + health_count_cols + health_rate_cols\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 0: PRE-PROCESSING\n",
    "# ---------------------------------------------------------\n",
    "df = merged_full.copy()\n",
    "\n",
    "# Coerce numeric types\n",
    "for c in numeric_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "# Filter Year Window\n",
    "df = df[(df['Year'] >= 2014) & (df['Year'] <= 2024)].copy()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 1: HEALTH DATA AGGREGATION (Annual)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Create a clean County-Year dataset\n",
    "# We drop duplicates because the original df has many rows (industries) per county\n",
    "county_health = df[['State', 'Year', 'FIPS', 'Population'] + health_count_cols + health_rate_cols].drop_duplicates()\n",
    "\n",
    "# --- A. Handle Weighted Averages (Rates) ---\n",
    "# Create numerators: (Rate * Population)\n",
    "for col in health_rate_cols:\n",
    "    county_health[f'{col}_numerator'] = county_health[col] * county_health['Population']\n",
    "\n",
    "# Define aggregation dictionary\n",
    "agg_dict = {\n",
    "    'state_total_pop': ('Population', 'sum'),\n",
    "}\n",
    "\n",
    "# Add Sum logic for Count columns\n",
    "for col in health_count_cols:\n",
    "    agg_dict[f'STATE_{col}'] = (col, 'sum')\n",
    "\n",
    "# Add Sum logic for Rate Numerators\n",
    "for col in health_rate_cols:\n",
    "    agg_dict[f'{col}_num_sum'] = (f'{col}_numerator', 'sum')\n",
    "\n",
    "# perform GroupBy\n",
    "state_health = county_health.groupby(['State', 'Year'], as_index=False).agg(**agg_dict)\n",
    "\n",
    "# Calculate Final Weighted Averages for Rates\n",
    "for col in health_rate_cols:\n",
    "    # Sum of (Rate*Pop) / Total Pop\n",
    "    state_health[f'STATE_{col}'] = state_health[f'{col}_num_sum'] / state_health['state_total_pop']\n",
    "    # Drop the temporary numerator column to keep it clean\n",
    "    state_health.drop(columns=[f'{col}_num_sum'], inplace=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 2: ECONOMIC DATA AGGREGATION (Quarterly)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# CRITICAL: Filter out Industry '00' to avoid double counting\n",
    "# We sum the specific industries to get the State Total\n",
    "df_econ = df[df['industry'] != '00'].copy()\n",
    "\n",
    "# Weight Earnings by Employment (because it's an average per person)\n",
    "df_econ['wage_bill'] = df_econ['Earnings_Beginning_Qtr'] * df_econ['Employment_Count']\n",
    "\n",
    "state_qtr = (\n",
    "    df_econ.groupby(['State', 'Year', 'quarter'], as_index=False)\n",
    "    .agg(\n",
    "        state_emp_total=('Employment_Count', 'sum'),\n",
    "        state_hires_total=('New_Hires', 'sum'),\n",
    "        state_wage_bill_total=('wage_bill', 'sum')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Recover State Average Earnings\n",
    "state_qtr['state_avg_earnings'] = state_qtr['state_wage_bill_total'] / state_qtr['state_emp_total']\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 3: MERGE & FEATURE ENGINEERING\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Merge Annual Health into Quarterly Econ\n",
    "state_df = state_qtr.merge(state_health, on=['State', 'Year'], how='left')\n",
    "\n",
    "# Helper variables\n",
    "pop = state_df['state_total_pop'].replace({0: np.nan})\n",
    "emp = state_df['state_emp_total'].replace({0: np.nan})\n",
    "\n",
    "# Per Capita Econ Metrics\n",
    "state_df['econ_emp_per_1k'] = 1000 * state_df['state_emp_total'] / pop\n",
    "state_df['econ_hires_per_1k'] = 1000 * state_df['state_hires_total'] / pop\n",
    "state_df['econ_hire_rate'] = state_df['state_hires_total'] / emp\n",
    "\n",
    "# Growth Rates (Lagged features)\n",
    "state_df = state_df.sort_values(['State', 'Year', 'quarter'])\n",
    "\n",
    "def calc_growth(series, lag):\n",
    "    prev = series.shift(lag)\n",
    "    den = prev.replace({0: np.nan})\n",
    "    return (series - prev) / den\n",
    "\n",
    "# QoQ Growth\n",
    "state_df['growth_emp_qoq'] = state_df.groupby('State')['state_emp_total'].transform(lambda x: calc_growth(x, 1))\n",
    "state_df['growth_earn_qoq'] = state_df.groupby('State')['state_avg_earnings'].transform(lambda x: calc_growth(x, 1))\n",
    "\n",
    "# YoY Growth (Seasonality adjustment)\n",
    "state_df['growth_emp_yoy'] = state_df.groupby('State')['state_emp_total'].transform(lambda x: calc_growth(x, 4))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 4: CLEANUP & FINAL OUTPUT\n",
    "# ---------------------------------------------------------\n",
    "state_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Organize columns clearly\n",
    "final_cols = [\n",
    "    'State', 'Year', 'quarter', 'state_total_pop',\n",
    "    # Econ\n",
    "    'state_emp_total', 'state_hires_total', 'state_avg_earnings',\n",
    "    'econ_emp_per_1k', 'econ_hires_per_1k', 'econ_hire_rate',\n",
    "    'growth_emp_qoq', 'growth_earn_qoq', 'growth_emp_yoy'\n",
    "] \n",
    "# Add all the State Health Columns (Counts and Rates)\n",
    "# (They are already named STATE_... in the dataframe)\n",
    "health_final_cols = [c for c in state_df.columns if c.startswith('STATE_')]\n",
    "final_cols = final_cols + health_final_cols\n",
    "\n",
    "# Final Selection\n",
    "state_df_final = state_df[final_cols]\n",
    "\n",
    "print(f\"Aggregated DataFrame Shape: {state_df_final.shape}\")\n",
    "print(\"Sample Columns:\", state_df_final.columns.tolist()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e61eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73afd9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r\"C:\\Users\\liuc\\Downloads\\CH_ECON_V3.csv\"  # change if needed\n",
    "#state_df_final.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9bf732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# US State Choropleth (Using state_df_final)\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from ipywidgets import widgets, interactive_output\n",
    "from IPython.display import display\n",
    "\n",
    "# 1. Prepare the Data\n",
    "# --------------------------------------------\n",
    "# We group by State/Year and take the MEAN.\n",
    "# - For Health data (Annual): The value is constant across quarters, so Mean returns the value.\n",
    "# - For Econ data (Quarterly): This gives us the \"Average Quarterly\" level for that year.\n",
    "df_map = state_df_final.groupby(['State', 'Year'], as_index=False).mean(numeric_only=True)\n",
    "\n",
    "# 2. Map State Names to USPS Codes\n",
    "# --------------------------------------------\n",
    "state_to_code = {\n",
    "    'Alabama':'AL','Alaska':'AK','Arizona':'AZ','Arkansas':'AR','California':'CA','Colorado':'CO',\n",
    "    'Connecticut':'CT','Delaware':'DE','District of Columbia':'DC','Florida':'FL','Georgia':'GA',\n",
    "    'Hawaii':'HI','Idaho':'ID','Illinois':'IL','Indiana':'IN','Iowa':'IA','Kansas':'KS','Kentucky':'KY',\n",
    "    'Louisiana':'LA','Maine':'ME','Maryland':'MD','Massachusetts':'MA','Michigan':'MI','Minnesota':'MN',\n",
    "    'Mississippi':'MS','Missouri':'MO','Montana':'MT','Nebraska':'NE','Nevada':'NV','New Hampshire':'NH',\n",
    "    'New Jersey':'NJ','New Mexico':'NM','New York':'NY','North Carolina':'NC','North Dakota':'ND',\n",
    "    'Ohio':'OH','Oklahoma':'OK','Oregon':'OR','Pennsylvania':'PA','Rhode Island':'RI','South Carolina':'SC',\n",
    "    'South Dakota':'SD','Tennessee':'TN','Texas':'TX','Utah':'UT','Vermont':'VT','Virginia':'VA',\n",
    "    'Washington':'WA','West Virginia':'WV','Wisconsin':'WI','Wyoming':'WY'\n",
    "}\n",
    "\n",
    "df_map['code'] = df_map['State'].map(state_to_code)\n",
    "\n",
    "# 3. Define Columns for Dropdown\n",
    "# --------------------------------------------\n",
    "# We dynamically pull the columns available in your dataframe\n",
    "# to ensure the dropdown never breaks.\n",
    "exclude_cols = ['State', 'Year', 'quarter', 'code', 'state_total_pop']\n",
    "available_cols = [c for c in df_map.columns if c not in exclude_cols]\n",
    "available_cols.sort()\n",
    "\n",
    "# Set a smart default\n",
    "default_val = 'econ_emp_per_1k' if 'econ_emp_per_1k' in available_cols else available_cols[0]\n",
    "\n",
    "# 4. Widgets\n",
    "# --------------------------------------------\n",
    "var_dd = widgets.Dropdown(\n",
    "    options=available_cols,\n",
    "    value=default_val,\n",
    "    description='Variable:',\n",
    "    layout=widgets.Layout(width='450px')\n",
    ")\n",
    "\n",
    "yrs = sorted(df_map['Year'].unique())\n",
    "year_dd = widgets.Dropdown(\n",
    "    options=yrs,\n",
    "    value=yrs[-1], # Default to latest year\n",
    "    description='Year:',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "# 5. Custom Colorscale (Your Preferred Yellow-Gold)\n",
    "# --------------------------------------------\n",
    "ylw_scale = [\n",
    "    (0.00, \"#fffde7\"),\n",
    "    (0.33, \"#fff59d\"),\n",
    "    (0.66, \"#fdd835\"),\n",
    "    (1.00, \"#f9a825\")\n",
    "]\n",
    "\n",
    "# 6. Plot Function\n",
    "# --------------------------------------------\n",
    "def show_map(variable, year):\n",
    "    # Filter data for specific year\n",
    "    d = df_map[df_map['Year'] == year].copy()\n",
    "\n",
    "    # Dynamic Range calculation for better contrast\n",
    "    # (Avoids 0s or NaNs skewing the scale)\n",
    "    valid_values = d[variable].dropna()\n",
    "    if len(valid_values) > 0:\n",
    "        low = valid_values.quantile(0.05)\n",
    "        high = valid_values.quantile(0.95)\n",
    "        rc = [low, high]\n",
    "    else:\n",
    "        rc = None\n",
    "\n",
    "    fig = px.choropleth(\n",
    "        d,\n",
    "        locations=\"code\",\n",
    "        locationmode=\"USA-states\",\n",
    "        color=variable,\n",
    "        scope=\"usa\",\n",
    "        range_color=rc,\n",
    "        color_continuous_scale=ylw_scale,\n",
    "        hover_name=\"State\",\n",
    "        hover_data={'code':False, 'Year':True, variable:':.2f'},\n",
    "        labels={variable: variable.replace('_',' ').replace('STATE', '').title()}\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f\"US States  {variable.replace('_',' ').title()} ({year})\",\n",
    "            x=0.5,\n",
    "            xanchor='center'\n",
    "        ),\n",
    "        coloraxis_colorbar=dict(title=\"Value\"),\n",
    "        geo=dict(bgcolor='rgba(0,0,0,0)'), # Transparent geo background\n",
    "        width=1050,\n",
    "        height=600,\n",
    "        margin=dict(l=0,r=0,t=60,b=0)\n",
    "    )\n",
    "\n",
    "    fig.update_traces(marker_line_color=\"white\", marker_line_width=0.5)\n",
    "    fig.show()\n",
    "\n",
    "# 7. Display UI\n",
    "# --------------------------------------------\n",
    "ui = widgets.HBox([var_dd, year_dd])\n",
    "out = interactive_output(show_map, {'variable': var_dd, 'year': year_dd})\n",
    "\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9471c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Interactive State Trend Plot (20142024)\n",
    "# Using pre-aggregated 'state_df_final'\n",
    "# ========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import widgets, interactive_output\n",
    "from IPython.display import display\n",
    "\n",
    "# --------------------------------------\n",
    "# 1) Prepare Annual Data\n",
    "# --------------------------------------\n",
    "# We start with state_df_final (Quarterly) and collapse to Annual.\n",
    "# Logic:\n",
    "# - Health columns are constant for the year, so mean() returns the correct value.\n",
    "# - Econ columns are quarterly, so mean() gives the \"Average Quarterly Level\" for that year.\n",
    "df_trend = state_df_final.groupby(['State', 'Year'], as_index=False).mean(numeric_only=True)\n",
    "\n",
    "# Ensure full 2014-2024 range for every state (handling missing years)\n",
    "def _complete_years(g):\n",
    "    # Create reference index\n",
    "    all_years = pd.DataFrame({'Year': np.arange(2014, 2025)})\n",
    "    # Merge existing data onto it\n",
    "    g = all_years.merge(g, on='Year', how='left')\n",
    "    # Fill State name downwards and upwards\n",
    "    g['State'] = g['State'].ffill().bfill()\n",
    "    return g\n",
    "\n",
    "df_trend = (\n",
    "    df_trend.groupby('State', as_index=False, group_keys=False)\n",
    "    .apply(_complete_years)\n",
    "    .sort_values(['State', 'Year'])\n",
    ")\n",
    "\n",
    "# --------------------------------------\n",
    "# 2) Define Variables for Dropdowns\n",
    "# --------------------------------------\n",
    "exclude_cols = ['State', 'Year', 'quarter', 'state_total_pop']\n",
    "# Get all numeric columns except the excluded ones\n",
    "all_vars = sorted([c for c in df_trend.columns if c not in exclude_cols])\n",
    "\n",
    "state_options = sorted(df_trend['State'].dropna().unique().tolist())\n",
    "\n",
    "# --------------------------------------\n",
    "# 3) Widgets\n",
    "# --------------------------------------\n",
    "state_dd = widgets.Dropdown(\n",
    "    options=state_options,\n",
    "    value=state_options[0] if state_options else None,\n",
    "    description='State:',\n",
    "    layout=widgets.Layout(width='250px')\n",
    ")\n",
    "\n",
    "var1_dd = widgets.Dropdown(\n",
    "    options=all_vars,\n",
    "    value='econ_emp_per_1k' if 'econ_emp_per_1k' in all_vars else all_vars[0],\n",
    "    description='Variable 1:',\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "# Try to find a good default for Var 2 (e.g., Obesity)\n",
    "default_v2 = [v for v in all_vars if 'Obesity' in v]\n",
    "default_v2 = default_v2[0] if default_v2 else (all_vars[1] if len(all_vars) > 1 else all_vars[0])\n",
    "\n",
    "var2_dd = widgets.Dropdown(\n",
    "    options=all_vars,\n",
    "    value=default_v2,\n",
    "    description='Variable 2:',\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "normalize_cb = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Normalize (01)',\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "# --------------------------------------\n",
    "# 4) Plotting Function\n",
    "# --------------------------------------\n",
    "def plot_state_trends(state, var1, var2, normalize):\n",
    "    if not state or not var1 or not var2:\n",
    "        return\n",
    "\n",
    "    # Filter data\n",
    "    sub = df_trend[df_trend['State'] == state].copy()\n",
    "    \n",
    "    x = sub['Year']\n",
    "    y1 = sub[var1]\n",
    "    y2 = sub[var2]\n",
    "\n",
    "    # --- Normalization Logic ---\n",
    "    def _minmax(s):\n",
    "        mn, mx = s.min(), s.max()\n",
    "        if pd.isna(mn) or pd.isna(mx) or mx == mn:\n",
    "            return s\n",
    "        return (s - mn) / (mx - mn)\n",
    "\n",
    "    if normalize:\n",
    "        y1_plot = _minmax(y1)\n",
    "        y2_plot = _minmax(y2)\n",
    "        y1_lbl = f\"{var1} (Scaled)\"\n",
    "        y2_lbl = f\"{var2} (Scaled)\"\n",
    "    else:\n",
    "        y1_plot, y2_plot = y1, y2\n",
    "        y1_lbl = var1\n",
    "        y2_lbl = var2\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    # Style 1\n",
    "    color1 = \"#1f77b4\" # Tab:Blue\n",
    "    line1 = ax1.plot(x, y1_plot, marker='o', linestyle='-', linewidth=2, color=color1, label=y1_lbl)\n",
    "    ax1.set_xlabel(\"Year\", fontsize=10)\n",
    "    ax1.set_ylabel(y1_lbl, color=color1, fontsize=10, fontweight='bold')\n",
    "    ax1.tick_params(axis='y', labelcolor=color1)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Style 2 (Twin Axis)\n",
    "    ax2 = ax1.twinx()\n",
    "    color2 = \"#ff7f0e\" # Tab:Orange\n",
    "    line2 = ax2.plot(x, y2_plot, marker='s', linestyle='--', linewidth=2, color=color2, label=y2_lbl)\n",
    "    ax2.set_ylabel(y2_lbl, color=color2, fontsize=10, fontweight='bold')\n",
    "    ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "    # Title\n",
    "    norm_txt = \" (Normalized Trend)\" if normalize else \"\"\n",
    "    plt.title(f\"{state}: {var1} vs. {var2}{norm_txt}\", fontsize=12)\n",
    "    plt.xticks(np.arange(2014, 2025, 1))\n",
    "\n",
    "    # Unified Legend\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax1.legend(lines, labels, loc='upper left', frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --------------------------------------\n",
    "# 5) Display\n",
    "# --------------------------------------\n",
    "ui = widgets.VBox([\n",
    "    widgets.HBox([state_dd, normalize_cb]),\n",
    "    widgets.HBox([var1_dd, var2_dd])\n",
    "])\n",
    "\n",
    "out = interactive_output(\n",
    "    plot_state_trends,\n",
    "    {'state': state_dd, 'var1': var1_dd, 'var2': var2_dd, 'normalize': normalize_cb}\n",
    ")\n",
    "\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655165c6-bca5-4784-99ae-8001ba204d59",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835ee510-5e13-47a1-bdfa-d68a7da17b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate predictions for the entire dataset to visualize trends\n",
    "df_plot = state_df.copy()\n",
    "X_all_processed = best_fitted.named_steps['scaler'].transform(X_all) # Using notebook's scaler\n",
    "df_plot['Predicted_Index'] = best_fitted.predict(X_all)\n",
    "\n",
    "# Select states with high/low relativity for contrast\n",
    "target_states = ['California', 'Texas', 'New York', 'Florida']\n",
    "sub_df = df_plot[df_plot['State'].isin(target_states)]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=sub_df, x='Year', y='rel_emp_idx', hue='State', marker='o', label='Actual')\n",
    "sns.lineplot(data=sub_df, x='Year', y='Predicted_Index', hue='State', linestyle='--', alpha=0.7, label='Predicted')\n",
    "\n",
    "# Highlight the strict temporal split (2022-2024)\n",
    "plt.axvspan(2022, 2024, color='gray', alpha=0.15, label='Test Period (Holdout)')\n",
    "\n",
    "plt.title(\"Actual vs. Predicted Employment Relativity Index by State\", fontsize=14)\n",
    "plt.ylabel(\"Employment Relativity Index (rel_emp_idx)\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
