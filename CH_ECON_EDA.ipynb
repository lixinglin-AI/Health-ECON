{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1f8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b209c7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2014 County Health Rankings Data - v6.xls: 3141 rows, Year: 2014\n",
      "Processed 2015 County Health Rankings Data - v3.xls: 3141 rows, Year: 2015\n",
      "Processed 2016 County Health Rankings Data - v3.xls: 3141 rows, Year: 2016\n",
      "Processed 2017CountyHealthRankingsData.xls: 3136 rows, Year: 2017\n",
      "Processed 2018 County Health Rankings Data - v2.xls: 3142 rows, Year: 2018\n",
      "Processed 2019 County Health Rankings Data - v3.xls: 3142 rows, Year: 2019\n",
      "Processed 2020 County Health Rankings Data - v2.xlsx: 3193 rows, Year: 2020\n",
      "Processed 2021 County Health Rankings Data - v1.xlsx: 3193 rows, Year: 2021\n",
      "Processed 2022 County Health Rankings Data - v1.xlsx: 3193 rows, Year: 2022\n",
      "Processed 2023 County Health Rankings Data - v2.xlsx: 3193 rows, Year: 2023\n",
      "Processed 2024_county_health_release_data_-_v1 (1).xlsx: 3201 rows, Year: 2024\n"
     ]
    }
   ],
   "source": [
    "# Define the folder path\n",
    "folder_path = r\"C:\\Users\\liuc\\Desktop\\talent rentention\\Healthdata2\"\n",
    "\n",
    "# Get all Excel files\n",
    "excel_files = sorted([f for f in os.listdir(folder_path) if f.endswith(('.xlsx', '.xls'))])\n",
    "\n",
    "# Column name mapping dictionary to standardize names\n",
    "column_mapping = {\n",
    "    # FIPS, State, County (consistent)\n",
    "    'FIPS': 'FIPS',\n",
    "    'State': 'State',\n",
    "    'County': 'County',\n",
    "    \n",
    "    # Deaths/Premature Deaths variations\n",
    "    'pre mature Deaths': 'Premature Deaths',\n",
    "    'premature Deaths': 'Premature Deaths',\n",
    "    'Premature death': 'Premature Deaths',\n",
    "    '# Deaths': 'Premature Deaths',\n",
    "    'Deaths': 'Premature Deaths',\n",
    "    \n",
    "    # Fair/Poor Health variations\n",
    "    '% Fair or Poor Health': '% Fair or Poor Health',\n",
    "    '% Fair/Poor': '% Fair or Poor Health',\n",
    "    \n",
    "    # Physically Unhealthy Days\n",
    "    'Physically Unhealthy Days': 'Physically Unhealthy Days',\n",
    "    'Average Number of Physically Unhealthy Days': 'Physically Unhealthy Days',\n",
    "    \n",
    "    # Mentally Unhealthy Days\n",
    "    'Mentally Unhealthy Days': 'Mentally Unhealthy Days',\n",
    "    'Average Number of Mentally Unhealthy Days': 'Mentally Unhealthy Days',\n",
    "    \n",
    "    # Low Birth Weight\n",
    "    '% low birth weight': '% Low Birthweight',\n",
    "    '% Low birthweight': '% Low Birthweight',\n",
    "    '% Low Birthweight': '% Low Birthweight',\n",
    "    '% LBW': '% Low Birthweight',\n",
    "    \n",
    "    # Smoking\n",
    "    '% Smokers': '% Smokers',\n",
    "    '% Adults Reporting Currently Smoking': '% Smokers',\n",
    "    \n",
    "    # Obesity\n",
    "    '% Obese': '% Adults with Obesity',\n",
    "    '% Adults with Obesity': '% Adults with Obesity',\n",
    "    \n",
    "    # Food Environment Index (consistent)\n",
    "    'Food Environment Index': 'Food Environment Index',\n",
    "    \n",
    "    # Physical Activity/Exercise Access\n",
    "    '% With Access': '% With Access to Exercise Opportunities',\n",
    "    '% With Access exercise': '% With Access to Exercise Opportunities',\n",
    "    '% With Access to Exercise Opportunities': '% With Access to Exercise Opportunities',\n",
    "    '% Physically Inactive': '% Physically Inactive',\n",
    "    \n",
    "    # Excessive Drinking (consistent)\n",
    "    '% Excessive Drinking': '% Excessive Drinking',\n",
    "    \n",
    "    # Driving Deaths\n",
    "    '# Alcohol-Impaired Driving Deaths': '# Alcohol-Impaired Driving Deaths',\n",
    "    '# Driving Deaths': '# Driving Deaths',\n",
    "    \n",
    "    # Teen Birth Rate (consistent)\n",
    "    'Teen Birth Rate': 'Teen Birth Rate',\n",
    "    \n",
    "    # Uninsured\n",
    "    '# Uninsured': '# Uninsured',\n",
    "    '% Uninsured': '% Uninsured',\n",
    "    \n",
    "    # Primary Care Physicians\n",
    "    'PCP Rate': 'Primary Care Physicians Rate',\n",
    "    'Primary Care Physicians Ratio': 'Primary Care Physicians Rate',\n",
    "    'Primary Care Physicians Rate': 'Primary Care Physicians Rate',\n",
    "    \n",
    "    # Medicare\n",
    "    '# Medicare enrollees': '# Medicare Enrollees',\n",
    "    '# Medicare Enrollees': '# Medicare Enrollees',\n",
    "    \n",
    "    # Preventable Hospitalizations\n",
    "    'Preventable Hosp. Rate': 'Preventable Hospitalization Rate',\n",
    "    'Preventable Hospitalization Rate': 'Preventable Hospitalization Rate',\n",
    "    \n",
    "    # Education\n",
    "    '% Some College': '% Some College',\n",
    "    \n",
    "    # Unemployment\n",
    "    '% Unemployed': '% Unemployed',\n",
    "    \n",
    "    # Child Poverty\n",
    "    '% Children in Poverty': '% Children in Poverty',\n",
    "    \n",
    "    # Income Ratio\n",
    "    'Income Ratio': 'Income Ratio',\n",
    "    \n",
    "    # Single-Parent Households\n",
    "    '% Single-Parent Households': '% Children in Single-Parent Households',\n",
    "    '% Children in Single-Parent Households': '% Children in Single-Parent Households',\n",
    "    \n",
    "    # Social Association\n",
    "    'Association Rate': 'Social Association Rate',\n",
    "    'Social Association Rate': 'Social Association Rate',\n",
    "    \n",
    "    # Crime\n",
    "    'Violent Crime Rate': 'Violent Crime Rate',\n",
    "    \n",
    "    # Housing\n",
    "    '% Severe Housing Problems': '% Severe Housing Problems',\n",
    "    \n",
    "    # Commuting\n",
    "    '% Drive Alone': '% Drive Alone to Work',\n",
    "    '% Drive Alone to Work': '% Drive Alone to Work',\n",
    "    'Long Commute - Drives Alone': '% Long Commute - Drives Alone',\n",
    "    '% Long Commute - Drives Alone': '% Long Commute - Drives Alone',\n",
    "    \n",
    "    # Dentist columns (2023 specific)\n",
    "    'Quartile': 'Quartile',\n",
    "    '# Dentists': '# Dentists',\n",
    "    'Dentist Rate': 'Dentist Rate',\n",
    "    'Dentist Ratio': 'Dentist Ratio'\n",
    "}\n",
    "\n",
    "# List to store all dataframes\n",
    "all_dfs = []\n",
    "\n",
    "# Process each file\n",
    "for file in excel_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    \n",
    "    try:\n",
    "        # Extract year from filename\n",
    "        if '2014' in file:\n",
    "            year = 2014\n",
    "        elif '2015' in file:\n",
    "            year = 2015\n",
    "        elif '2016' in file:\n",
    "            year = 2016\n",
    "        elif '2017' in file:\n",
    "            year = 2017\n",
    "        elif '2018' in file:\n",
    "            year = 2018\n",
    "        elif '2019' in file:\n",
    "            year = 2019\n",
    "        elif '2020' in file:\n",
    "            year = 2020\n",
    "        elif '2021' in file:\n",
    "            year = 2021\n",
    "        elif '2022' in file:\n",
    "            year = 2022\n",
    "        elif '2023' in file:\n",
    "            year = 2023\n",
    "        elif '2024' in file:\n",
    "            year = 2024\n",
    "        else:\n",
    "            year = 'Unknown'\n",
    "        \n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(file_path, sheet_name='Ranked Measure Data')\n",
    "        \n",
    "        # Add year column\n",
    "        df['Year'] = year\n",
    "        \n",
    "        # Rename columns using the mapping\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Add to list\n",
    "        all_dfs.append(df)\n",
    "        \n",
    "        print(f\"Processed {file}: {len(df)} rows, Year: {year}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {str(e)}\")\n",
    "\n",
    "# Get all unique columns across all dataframes\n",
    "all_columns = set()\n",
    "for df in all_dfs:\n",
    "    all_columns.update(df.columns)\n",
    "\n",
    "# Remove 'Year' from all_columns as we'll add it at a specific position\n",
    "all_columns.discard('Year')\n",
    "\n",
    "# Define the order of columns (put most important ones first)\n",
    "column_order = ['Year', 'FIPS', 'State', 'County'] + sorted(list(all_columns - {'FIPS', 'State', 'County'}))\n",
    "\n",
    "# Standardize all dataframes to have the same columns\n",
    "standardized_dfs = []\n",
    "for df in all_dfs:\n",
    "    # Add missing columns with NaN\n",
    "    for col in column_order:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "    \n",
    "    # Reorder columns\n",
    "    df = df[column_order]\n",
    "    standardized_dfs.append(df)\n",
    "\n",
    "# Combine all dataframes\n",
    "combined_df = pd.concat(standardized_dfs, ignore_index=True)\n",
    "\n",
    "# Sort by Year, State, and County\n",
    "combined_df = combined_df.sort_values(['Year', 'State', 'County'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50003fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (34816, 38)\n",
      "Rows with null County: 255\n",
      "After removing null counties: (34561, 38)\n",
      "\n",
      "============================================================\n",
      "Columns with missing data:\n",
      "============================================================\n",
      "                                     Column  Missing_Count  Total_Rows  \\\n",
      "25                             Dentist Rate          31505       34561   \n",
      "5                                # Dentists          31505       34561   \n",
      "26                            Dentist Ratio          31505       34561   \n",
      "23                  % With Access exercise           31498       34561   \n",
      "34                                 Quartile          31479       34561   \n",
      "17                    % Physically Inactive          31419       34561   \n",
      "8                               # Uninsured          25139       34561   \n",
      "7                      # Medicare Enrollees          19499       34561   \n",
      "31                         Premature Deaths           8309       34561   \n",
      "37                       Violent Crime Rate           7921       34561   \n",
      "32         Preventable Hospitalization Rate           3913       34561   \n",
      "24  % With Access to Exercise Opportunities           3461       34561   \n",
      "28                             Income Ratio           3184       34561   \n",
      "12                    % Drive Alone to Work           3152       34561   \n",
      "35                  Social Association Rate           3148       34561   \n",
      "13                     % Excessive Drinking           1842       34561   \n",
      "36                          Teen Birth Rate           1602       34561   \n",
      "33             Primary Care Physicians Rate           1584       34561   \n",
      "16                        % Low Birthweight           1258       34561   \n",
      "29                  Mentally Unhealthy Days           1114       34561   \n",
      "19                                % Smokers            870       34561   \n",
      "14                    % Fair or Poor Health            810       34561   \n",
      "30                Physically Unhealthy Days            678       34561   \n",
      "4         # Alcohol-Impaired Driving Deaths            306       34561   \n",
      "6                          # Driving Deaths            306       34561   \n",
      "27                   Food Environment Index            184       34561   \n",
      "11   % Children in Single-Parent Households             21       34561   \n",
      "10                    % Children in Poverty             19       34561   \n",
      "21                             % Unemployed             19       34561   \n",
      "22                              % Uninsured             18       34561   \n",
      "15            % Long Commute - Drives Alone              9       34561   \n",
      "9                     % Adults with Obesity              9       34561   \n",
      "20                           % Some College              8       34561   \n",
      "18                % Severe Housing Problems              7       34561   \n",
      "\n",
      "    Missing_Percentage  \n",
      "25           91.157663  \n",
      "5            91.157663  \n",
      "26           91.157663  \n",
      "23           91.137409  \n",
      "34           91.082434  \n",
      "17           90.908828  \n",
      "8            72.738057  \n",
      "7            56.419085  \n",
      "31           24.041550  \n",
      "37           22.918897  \n",
      "32           11.322010  \n",
      "24           10.014178  \n",
      "28            9.212696  \n",
      "12            9.120106  \n",
      "35            9.108533  \n",
      "13            5.329707  \n",
      "36            4.635283  \n",
      "33            4.583201  \n",
      "16            3.639941  \n",
      "29            3.223286  \n",
      "19            2.517288  \n",
      "14            2.343682  \n",
      "30            1.961749  \n",
      "4             0.885391  \n",
      "6             0.885391  \n",
      "27            0.532392  \n",
      "11            0.060762  \n",
      "10            0.054975  \n",
      "21            0.054975  \n",
      "22            0.052082  \n",
      "15            0.026041  \n",
      "9             0.026041  \n",
      "20            0.023147  \n",
      "18            0.020254  \n",
      "\n",
      "============================================================\n",
      "Columns to remove (>50% missing): 8\n",
      "============================================================\n",
      "  - # Dentists: 91.2% missing\n",
      "  - # Medicare Enrollees: 56.4% missing\n",
      "  - # Uninsured: 72.7% missing\n",
      "  - % Physically Inactive: 90.9% missing\n",
      "  - % With Access exercise : 91.1% missing\n",
      "  - Dentist Rate: 91.2% missing\n",
      "  - Dentist Ratio: 91.2% missing\n",
      "  - Quartile: 91.1% missing\n",
      "\n",
      "============================================================\n",
      "Final cleaned dataset summary:\n",
      "============================================================\n",
      "Shape: (34561, 30)\n",
      "Rows: 34561\n",
      "Columns: 30\n",
      "\n",
      "Remaining columns (30):\n",
      "   1. Year                                               (100.0% data available)\n",
      "   2. FIPS                                               (100.0% data available)\n",
      "   3. State                                              (100.0% data available)\n",
      "   4. County                                             (100.0% data available)\n",
      "   5. # Alcohol-Impaired Driving Deaths                  (99.1% data available)\n",
      "   6. # Driving Deaths                                   (99.1% data available)\n",
      "   7. % Adults with Obesity                              (100.0% data available)\n",
      "   8. % Children in Poverty                              (99.9% data available)\n",
      "   9. % Children in Single-Parent Households             (99.9% data available)\n",
      "  10. % Drive Alone to Work                              (90.9% data available)\n",
      "  11. % Excessive Drinking                               (94.7% data available)\n",
      "  12. % Fair or Poor Health                              (97.7% data available)\n",
      "  13. % Long Commute - Drives Alone                      (100.0% data available)\n",
      "  14. % Low Birthweight                                  (96.4% data available)\n",
      "  15. % Severe Housing Problems                          (100.0% data available)\n",
      "  16. % Smokers                                          (97.5% data available)\n",
      "  17. % Some College                                     (100.0% data available)\n",
      "  18. % Unemployed                                       (99.9% data available)\n",
      "  19. % Uninsured                                        (99.9% data available)\n",
      "  20. % With Access to Exercise Opportunities            (90.0% data available)\n",
      "  21. Food Environment Index                             (99.5% data available)\n",
      "  22. Income Ratio                                       (90.8% data available)\n",
      "  23. Mentally Unhealthy Days                            (96.8% data available)\n",
      "  24. Physically Unhealthy Days                          (98.0% data available)\n",
      "  25. Premature Deaths                                   (76.0% data available)\n",
      "  26. Preventable Hospitalization Rate                   (88.7% data available)\n",
      "  27. Primary Care Physicians Rate                       (95.4% data available)\n",
      "  28. Social Association Rate                            (90.9% data available)\n",
      "  29. Teen Birth Rate                                    (95.4% data available)\n",
      "  30. Violent Crime Rate                                 (77.1% data available)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where County is null\n",
    "print(f\"Original shape: {combined_df.shape}\")\n",
    "print(f\"Rows with null County: {combined_df['County'].isna().sum()}\")\n",
    "\n",
    "# Remove rows where County is null\n",
    "combined_df_cleaned = combined_df[combined_df['County'].notna()].copy()\n",
    "\n",
    "print(f\"After removing null counties: {combined_df_cleaned.shape}\")\n",
    "\n",
    "# Calculate missing percentage for each column\n",
    "missing_percent = (combined_df_cleaned.isna().sum() / len(combined_df_cleaned)) * 100\n",
    "\n",
    "# Create a summary of missing data\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': missing_percent.index,\n",
    "    'Missing_Count': combined_df_cleaned.isna().sum().values,\n",
    "    'Total_Rows': len(combined_df_cleaned),\n",
    "    'Missing_Percentage': missing_percent.values\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Columns with missing data:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(missing_summary[missing_summary['Missing_Percentage'] > 0])\n",
    "\n",
    "# Identify columns to keep (less than or equal to 50% missing)\n",
    "columns_to_keep = missing_percent[missing_percent <= 50].index.tolist()\n",
    "columns_to_remove = missing_percent[missing_percent > 50].index.tolist()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Columns to remove (>50% missing): {len(columns_to_remove)}\")\n",
    "print(f\"{'='*60}\")\n",
    "for col in columns_to_remove:\n",
    "    print(f\"  - {col}: {missing_percent[col]:.1f}% missing\")\n",
    "\n",
    "# Keep only columns with <=50% missing data\n",
    "combined_df_cleaned = combined_df_cleaned[columns_to_keep]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Final cleaned dataset summary:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Shape: {combined_df_cleaned.shape}\")\n",
    "print(f\"Rows: {len(combined_df_cleaned)}\")\n",
    "print(f\"Columns: {len(combined_df_cleaned.columns)}\")\n",
    "print(f\"\\nRemaining columns ({len(combined_df_cleaned.columns)}):\")\n",
    "for i, col in enumerate(combined_df_cleaned.columns, 1):\n",
    "    non_missing = combined_df_cleaned[col].notna().sum()\n",
    "    pct_available = (non_missing / len(combined_df_cleaned)) * 100\n",
    "    print(f\"  {i:2d}. {col:<50} ({pct_available:.1f}% data available)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1657e52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWI Data Shape: (2642015, 24)\n",
      "\n",
      "Unique years: [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
      "Number of unique counties (FIPS): 3222\n",
      "Number of unique industries: 21\n",
      "Suppressed EarnBeg: 11305 records\n",
      "Suppressed Emp: 259199 records\n",
      "Suppressed HirA: 287103 records\n",
      "\n",
      "Original health data shape: (34561, 30)\n",
      "Quarterly health data shape: (138244, 31)\n",
      "\n",
      "Merged data shape: (2573811, 54)\n",
      "Number of columns: 54\n",
      "\n",
      "=== MERGE QUALITY CHECK ===\n",
      "Unique FIPS in health data: 3150\n",
      "Unique FIPS in QWI data: 3222\n",
      "Unique FIPS in merged data: 3127\n",
      "\n",
      "Records per industry in merged data:\n",
      "industry\n",
      "00       135584\n",
      "44-45    135348\n",
      "23       134964\n",
      "62       134904\n",
      "72       134797\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Year coverage in merged data:\n",
      "Year\n",
      "2014    236893\n",
      "2015    236832\n",
      "2016    236126\n",
      "2017    235371\n",
      "2018    235487\n",
      "2019    235625\n",
      "2020    235622\n",
      "2021    235941\n",
      "2022    230093\n",
      "2023    230257\n",
      "2024    225564\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== WORKFORCE METRICS SUMMARY ===\n",
      "                   Emp                      EarnBeg                  \\\n",
      "                  mean  median   count         mean  median   count   \n",
      "industry                                                              \n",
      "00        39179.571764  6320.0  135535  3546.003326  3367.0  135583   \n",
      "11          418.916579   124.0  125795  3019.946179  2883.0  132384   \n",
      "21          403.201091    83.0   65642  5422.420119  5023.0  104418   \n",
      "22          280.050694    82.0   80621  7267.053973  7065.0  120022   \n",
      "23         2345.758641   405.0  132769  3918.102888  3762.0  134846   \n",
      "\n",
      "                 HirA                  \n",
      "                 mean  median   count  \n",
      "industry                               \n",
      "00        7432.224494  1229.0  135545  \n",
      "11         207.062057    25.0  122016  \n",
      "21          49.473474     8.0   73098  \n",
      "22          13.468592     3.0   83196  \n",
      "23         511.466443    93.0  130897  \n",
      "\n",
      "County retention rate: 99.3%\n",
      "\n",
      "Final dataset: 2,573,811 records\n",
      "Unique county-year-quarter-industry combinations\n"
     ]
    }
   ],
   "source": [
    "# Read the QWI data\n",
    "qwi_path = r\"C:\\Users\\liuc\\Downloads\\qwi_47eb6eafd6f449ccbab042fc81879bc1.csv\"\n",
    "qwi_df = pd.read_csv(qwi_path)\n",
    "\n",
    "print(\"QWI Data Shape:\", qwi_df.shape)\n",
    "print(f\"\\nUnique years: {sorted(qwi_df['year'].unique())}\")\n",
    "print(f\"Number of unique counties (FIPS): {qwi_df['geography'].nunique()}\")\n",
    "print(f\"Number of unique industries: {qwi_df['industry'].nunique()}\")\n",
    "\n",
    "# Handle FIPS codes\n",
    "qwi_df['FIPS'] = qwi_df['geography'].astype(str).str.zfill(5)\n",
    "\n",
    "# Handle suppressed data\n",
    "value_cols = ['EarnBeg', 'Emp', 'HirA']\n",
    "suppression_cols = ['sEarnBeg', 'sEmp', 'sHirA']\n",
    "\n",
    "for val_col, supp_col in zip(value_cols, suppression_cols):\n",
    "    qwi_df.loc[qwi_df[supp_col] == 5, val_col] = np.nan\n",
    "    print(f\"Suppressed {val_col}: {(qwi_df[supp_col] == 5).sum()} records\")\n",
    "\n",
    "# Create quarterly version of health data\n",
    "quarters = [1, 2, 3, 4]\n",
    "health_quarterly_list = []\n",
    "\n",
    "for quarter in quarters:\n",
    "    health_q = combined_df_cleaned.copy()\n",
    "    health_q['quarter'] = quarter\n",
    "    health_quarterly_list.append(health_q)\n",
    "\n",
    "health_quarterly = pd.concat(health_quarterly_list, ignore_index=True)\n",
    "\n",
    "print(f\"\\nOriginal health data shape: {combined_df_cleaned.shape}\")\n",
    "print(f\"Quarterly health data shape: {health_quarterly.shape}\")\n",
    "\n",
    "# Ensure FIPS formatting in health data\n",
    "health_quarterly['FIPS'] = health_quarterly['FIPS'].astype(str).str.zfill(5)\n",
    "\n",
    "# Merge - note the lowercase 'year' in QWI data\n",
    "merged_df = pd.merge(\n",
    "    health_quarterly,\n",
    "    qwi_df,\n",
    "    left_on=['FIPS', 'Year', 'quarter'],\n",
    "    right_on=['FIPS', 'year', 'quarter'],\n",
    "    how='inner',\n",
    "    suffixes=('_health', '_qwi')\n",
    ")\n",
    "\n",
    "print(f\"\\nMerged data shape: {merged_df.shape}\")\n",
    "print(f\"Number of columns: {len(merged_df.columns)}\")\n",
    "\n",
    "# Check merge quality\n",
    "print(\"\\n=== MERGE QUALITY CHECK ===\")\n",
    "print(f\"Unique FIPS in health data: {health_quarterly['FIPS'].nunique()}\")\n",
    "print(f\"Unique FIPS in QWI data: {qwi_df['FIPS'].nunique()}\")\n",
    "print(f\"Unique FIPS in merged data: {merged_df['FIPS'].nunique()}\")\n",
    "\n",
    "# Check industries in merged data\n",
    "print(f\"\\nRecords per industry in merged data:\")\n",
    "print(merged_df['industry'].value_counts().head())\n",
    "\n",
    "# Check year coverage\n",
    "print(f\"\\nYear coverage in merged data:\")\n",
    "print(merged_df['Year'].value_counts().sort_index())\n",
    "\n",
    "# Basic statistics on workforce metrics\n",
    "print(\"\\n=== WORKFORCE METRICS SUMMARY ===\")\n",
    "workforce_metrics = merged_df.groupby('industry')[['Emp', 'EarnBeg', 'HirA']].agg(['mean', 'median', 'count'])\n",
    "print(workforce_metrics.head())\n",
    "\n",
    "# Check how much data we retained\n",
    "retention_rate = (merged_df['FIPS'].nunique() / health_quarterly['FIPS'].nunique()) * 100\n",
    "print(f\"\\nCounty retention rate: {retention_rate:.1f}%\")\n",
    "\n",
    "print(f\"\\nFinal dataset: {len(merged_df):,} records\")\n",
    "print(f\"Unique county-year-quarter-industry combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57232fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_columns = [col for col in combined_df_cleaned.columns if col != 'quarter']\n",
    "\n",
    "# From QWI data: only the specific columns you mentioned\n",
    "qwi_columns = ['quarter', 'EarnBeg', 'Emp', 'HirA', 'sEarnBeg', 'sEmp', 'sHirA', 'industry']\n",
    "\n",
    "# Combine the column lists\n",
    "columns_to_keep = health_columns + qwi_columns\n",
    "\n",
    "# Filter the merged dataframe\n",
    "merged_df_filtered = merged_df[columns_to_keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec3070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mapping\n",
    "industry_mapping = {\n",
    "    '00': 'Total, All Industries',\n",
    "    '11': 'Agriculture, Forestry, Fishing, and Hunting',\n",
    "    '21': 'Mining, Quarrying, and Oil and Gas Extraction',\n",
    "    '22': 'Utilities',\n",
    "    '23': 'Construction',\n",
    "    '31-33': 'Manufacturing',\n",
    "    '42': 'Wholesale Trade',\n",
    "    '44-45': 'Retail Trade',\n",
    "    '48-49': 'Transportation and Warehousing',\n",
    "    '51': 'Information',\n",
    "    '52': 'Finance and Insurance',\n",
    "    '53': 'Real Estate and Rental and Leasing',\n",
    "    '54': 'Professional, Scientific, and Technical Services',\n",
    "    '55': 'Management of Companies and Enterprises',\n",
    "    '56': 'Administrative and Support and Waste Management Services',\n",
    "    '61': 'Educational Services',\n",
    "    '62': 'Health Care and Social Assistance',\n",
    "    '71': 'Arts, Entertainment, and Recreation',\n",
    "    '72': 'Accommodation and Food Services',\n",
    "    '81': 'Other Services (except Public Administration)',\n",
    "    '92': 'Public Administration'\n",
    "}\n",
    "\n",
    "# Add readable industry name\n",
    "merged_df_filtered['industry_name'] = merged_df_filtered['industry'].map(industry_mapping)\n",
    "\n",
    "merged_df_filtered = merged_df_filtered.rename(columns={\n",
    "    'EarnBeg': 'Earnings_Beginning_Qtr',\n",
    "    'Emp': 'Employment_Count',\n",
    "    'HirA': 'New_Hires',\n",
    "    'sEarnBeg': 'Earnings_Suppression_Flag',\n",
    "    'sEmp': 'Employment_Suppression_Flag',\n",
    "    'sHirA': 'Hires_Suppression_Flag'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "779a2b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_path = r\"C:\\Users\\liuc\\Downloads\\Population by Age and Sex - US, States, Counties.csv\"\n",
    "df_population = pd.read_csv(population_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b143f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Year is int\n",
    "merged_df_filtered['Year'] = merged_df_filtered['Year'].astype(int)\n",
    "df_population_unique = (\n",
    "    df_population\n",
    "    .sort_values(['Statefips','Countyfips','Year'])\n",
    "    .drop_duplicates(subset=['Statefips','Countyfips','Year'], keep='first')\n",
    ")\n",
    "\n",
    "\n",
    "df_population_unique['FIPS'] = (\n",
    "    df_population_unique['Statefips'].astype(str).str.zfill(2) +\n",
    "    df_population_unique['Countyfips'].astype(str).str.zfill(3)\n",
    ")\n",
    "\n",
    "df_pop_small = df_population_unique[['FIPS','Year','Total Population']].rename(\n",
    "    columns={'Total Population':'Population'}\n",
    ")\n",
    "\n",
    "merged_full = merged_df_filtered.merge(\n",
    "    df_pop_small, on=['FIPS','Year'], how='left'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd7ea032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuc\\AppData\\Local\\Temp\\ipykernel_11560\\631764031.py:211: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  | df_econ[\"industry_name\"].str.lower().str.contains(TOTAL_NAME_PAT, regex=True, na=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECON MODE: total_only | total_coverage = 1.0\n",
      "Inferred suppressed flag values used (empty = not applied):\n",
      "  Employment_Count : {'5'}\n",
      "  New_Hires : {'5'}\n",
      "  Earnings_Beginning_Qtr : {'5'}\n",
      "\n",
      "Non-null rates in annual econ:\n",
      "state_emp_avg         1.0\n",
      "state_hires_total     1.0\n",
      "state_avg_earnings    1.0\n",
      "\n",
      "Share of zeros among non-missing annual econ (should NOT be ~1.0):\n",
      "state_emp_avg zero_share= 0.0\n",
      "state_hires_total zero_share= 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuc\\AppData\\Local\\Temp\\ipykernel_11560\\631764031.py:291: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda s: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CONFIG\n",
    "# ---------------------------------------------------------\n",
    "health_count_cols = [\n",
    "    '# Alcohol-Impaired Driving Deaths',\n",
    "    '# Driving Deaths',\n",
    "    'Premature Deaths'\n",
    "]\n",
    "\n",
    "health_rate_cols = [\n",
    "    '% Adults with Obesity',\n",
    "    '% Children in Poverty',\n",
    "    '% Children in Single-Parent Households',\n",
    "    '% Drive Alone to Work',\n",
    "    '% Excessive Drinking',\n",
    "    '% Fair or Poor Health',\n",
    "    '% Long Commute - Drives Alone',\n",
    "    '% Low Birthweight',\n",
    "    '% Severe Housing Problems',\n",
    "    '% Smokers',\n",
    "    '% Some College',\n",
    "    '% Unemployed',\n",
    "    '% Uninsured',\n",
    "    '% With Access to Exercise Opportunities',\n",
    "    'Food Environment Index',\n",
    "    'Income Ratio',\n",
    "    'Mentally Unhealthy Days',\n",
    "    'Physically Unhealthy Days',\n",
    "    'Preventable Hospitalization Rate',\n",
    "    'Primary Care Physicians Rate',\n",
    "    'Social Association Rate',\n",
    "    'Teen Birth Rate',\n",
    "    'Violent Crime Rate'\n",
    "]\n",
    "\n",
    "econ_cols = ['Employment_Count', 'New_Hires', 'Earnings_Beginning_Qtr']\n",
    "flag_map = {\n",
    "    'Employment_Count': 'Employment_Suppression_Flag',\n",
    "    'New_Hires': 'Hires_Suppression_Flag',\n",
    "    'Earnings_Beginning_Qtr': 'Earnings_Suppression_Flag'\n",
    "}\n",
    "\n",
    "# Common \"total industry\" signals (adjust if your data differs)\n",
    "TOTAL_CODES = {'00', '0', '000', '10'}\n",
    "TOTAL_NAME_PAT = r'\\b(total|all)\\b'\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------------------------------\n",
    "def to_num_clean(s):\n",
    "    # robust numeric conversion: removes commas and common suppression tokens\n",
    "    s = s.astype(str).str.replace(\",\", \"\", regex=False).str.strip()\n",
    "    s = s.replace({\n",
    "        \"\": np.nan, \"nan\": np.nan, \"None\": np.nan, \"NA\": np.nan, \"N/A\": np.nan,\n",
    "        \"*\": np.nan, \"S\": np.nan, \"s\": np.nan\n",
    "    })\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def sum_mc1(x):\n",
    "    return x.sum(min_count=1)\n",
    "\n",
    "def safe_div(n, d):\n",
    "    n = np.asarray(n, dtype=float)\n",
    "    d = np.asarray(d, dtype=float)\n",
    "    return np.where((d == 0) | np.isnan(d), np.nan, n / d)\n",
    "\n",
    "def weighted_mean(x, w):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    w = np.asarray(w, dtype=float)\n",
    "    num = np.nansum(x * w)\n",
    "    den = np.nansum(w)\n",
    "    return np.nan if den == 0 else num / den\n",
    "\n",
    "def infer_suppressed_value(df, val_col, flag_col):\n",
    "    \"\"\"\n",
    "    Tries to infer which flag value indicates suppression by checking which flag value\n",
    "    is most associated with missing/low coverage in val_col.\n",
    "    Returns a set of values to treat as 'suppressed'. If ambiguous, returns empty set.\n",
    "    \"\"\"\n",
    "    if flag_col not in df.columns or val_col not in df.columns:\n",
    "        return set()\n",
    "\n",
    "    f = df[flag_col].copy()\n",
    "    # normalize flag to strings so 1/\"1\"/True collapse\n",
    "    f = f.astype(str).str.strip().replace({\"nan\": np.nan})\n",
    "    tmp = pd.DataFrame({\"flag\": f, \"val\": df[val_col]})\n",
    "\n",
    "    # only keep flags that appear enough\n",
    "    vc = tmp[\"flag\"].value_counts(dropna=True)\n",
    "    cand = vc[vc >= 100].index.tolist()\n",
    "    if not cand:\n",
    "        return set()\n",
    "\n",
    "    stats = []\n",
    "    for v in cand:\n",
    "        sub = tmp[tmp[\"flag\"] == v]\n",
    "        miss = sub[\"val\"].isna().mean()\n",
    "        nonmiss = 1 - miss\n",
    "        stats.append((v, nonmiss, miss, len(sub)))\n",
    "\n",
    "    # pick the flag value with the *lowest* non-missing rate as \"suppressed\"\n",
    "    stats_sorted = sorted(stats, key=lambda t: t[1])\n",
    "    best = stats_sorted[0]\n",
    "    second = stats_sorted[1] if len(stats_sorted) > 1 else None\n",
    "\n",
    "    # require separation to avoid guessing wrong\n",
    "    # (e.g., suppressed has 5% non-missing and next has 95%)\n",
    "    if second is None or (second[1] - best[1] >= 0.20):\n",
    "        return {best[0]}\n",
    "    return set()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 0: Pre-processing\n",
    "# ---------------------------------------------------------\n",
    "df = merged_full.copy()\n",
    "\n",
    "# Keep FIPS as string with leading zeros (do NOT numeric-coerce it)\n",
    "df[\"FIPS\"] = df[\"FIPS\"].astype(str).str.extract(r\"(\\d+)\", expand=False)\n",
    "df[\"FIPS\"] = df[\"FIPS\"].str.zfill(5)\n",
    "\n",
    "# Industry as string\n",
    "df[\"industry\"] = df[\"industry\"].astype(str).str.strip()\n",
    "df[\"industry_name\"] = df[\"industry_name\"].astype(str)\n",
    "\n",
    "# Numeric coercion for relevant cols\n",
    "for c in [\"Year\", \"quarter\", \"Population\"] + econ_cols + health_count_cols + health_rate_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = to_num_clean(df[c])\n",
    "\n",
    "# Filter year window\n",
    "df = df[(df[\"Year\"] >= 2014) & (df[\"Year\"] <= 2024)].copy()\n",
    "\n",
    "# Quarter normalization 0..3 -> 1..4\n",
    "if \"quarter\" in df.columns:\n",
    "    qmin, qmax = df[\"quarter\"].min(), df[\"quarter\"].max()\n",
    "    if pd.notna(qmin) and pd.notna(qmax) and qmin >= 0 and qmax <= 3:\n",
    "        df[\"quarter\"] = df[\"quarter\"] + 1\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 1: HEALTH (County-Year -> State-Year), de-dup first\n",
    "# ---------------------------------------------------------\n",
    "health_keep = [\"State\", \"Year\", \"FIPS\", \"Population\"] + health_count_cols + health_rate_cols\n",
    "county_health_raw = df[health_keep].copy()\n",
    "\n",
    "county_health = (\n",
    "    county_health_raw\n",
    "    .sort_values([\"State\", \"FIPS\", \"Year\"])\n",
    "    .groupby([\"State\", \"Year\", \"FIPS\"], as_index=False)\n",
    "    .agg({**{\"Population\": \"first\"},\n",
    "          **{c: \"first\" for c in health_count_cols},\n",
    "          **{c: \"first\" for c in health_rate_cols}})\n",
    ")\n",
    "\n",
    "# Create numerators for weighted averages\n",
    "for col in health_rate_cols:\n",
    "    county_health[f\"{col}_num\"] = county_health[col] * county_health[\"Population\"]\n",
    "\n",
    "# Aggregate to state-year\n",
    "agg_dict = {\"state_total_pop\": (\"Population\", \"sum\")}\n",
    "for col in health_count_cols:\n",
    "    agg_dict[f\"STATE_{col}\"] = (col, sum_mc1)\n",
    "for col in health_rate_cols:\n",
    "    agg_dict[f\"{col}_num_sum\"] = (f\"{col}_num\", sum_mc1)\n",
    "\n",
    "state_health = county_health.groupby([\"State\", \"Year\"], as_index=False).agg(**agg_dict)\n",
    "\n",
    "# finalize weighted rates\n",
    "for col in health_rate_cols:\n",
    "    state_health[f\"STATE_{col}\"] = state_health[f\"{col}_num_sum\"] / state_health[\"state_total_pop\"].replace({0: np.nan})\n",
    "    state_health.drop(columns=[f\"{col}_num_sum\"], inplace=True)\n",
    "\n",
    "# optional: convert health counts to per-100k at state level\n",
    "for col in health_count_cols:\n",
    "    state_health[f\"STATE_{col}_per_100k\"] = (\n",
    "        100000 * state_health[f\"STATE_{col}\"] / state_health[\"state_total_pop\"].replace({0: np.nan})\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 2: ECON (County-Quarter-Industry -> State-Year)\n",
    "# Key: avoid double counting totals vs components + handle suppression flags safely\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_econ = df[[\"State\", \"Year\", \"quarter\", \"FIPS\", \"industry\", \"industry_name\", \"Population\"] + econ_cols +\n",
    "            [v for v in flag_map.values() if v in df.columns]].copy()\n",
    "\n",
    "# --- 2A) Infer which flag value means \"suppressed\" (so we don't accidentally wipe all data)\n",
    "suppressed_values = {}\n",
    "for col, flag in flag_map.items():\n",
    "    if flag in df_econ.columns:\n",
    "        suppressed_values[col] = infer_suppressed_value(df_econ, col, flag)\n",
    "    else:\n",
    "        suppressed_values[col] = set()\n",
    "\n",
    "# Apply suppression -> NaN ONLY if we inferred a value confidently\n",
    "for col, flag in flag_map.items():\n",
    "    vals = suppressed_values.get(col, set())\n",
    "    if flag in df_econ.columns and vals:\n",
    "        f = df_econ[flag].astype(str).str.strip()\n",
    "        df_econ.loc[f.isin(vals), col] = np.nan\n",
    "\n",
    "# --- 2B) Decide how to handle industry totals\n",
    "# Strategy:\n",
    "#   If \"total\" rows exist and have decent coverage -> use total only (no double count)\n",
    "#   Else -> sum across non-total industries, excluding totals if present\n",
    "\n",
    "is_total = (\n",
    "    df_econ[\"industry\"].isin(TOTAL_CODES)\n",
    "    | df_econ[\"industry_name\"].str.lower().str.contains(TOTAL_NAME_PAT, regex=True, na=False)\n",
    ")\n",
    "\n",
    "econ_total = df_econ[is_total].copy()\n",
    "econ_parts = df_econ[~is_total].copy()\n",
    "\n",
    "# Coverage check: how many State-Year-Quarter groups have any non-missing employment in total rows?\n",
    "tot_grp = econ_total.groupby([\"State\", \"Year\", \"quarter\"])[\"Employment_Count\"].apply(lambda s: s.notna().any())\n",
    "total_coverage = tot_grp.mean() if len(tot_grp) else 0.0\n",
    "\n",
    "MODE = \"total_only\" if (len(econ_total) > 0 and total_coverage >= 0.60) else \"sum_parts\"\n",
    "\n",
    "base_econ = econ_total if MODE == \"total_only\" else econ_parts\n",
    "\n",
    "# Wage bill (use employment-weighted earnings; if Employment_Count missing, wage_bill missing)\n",
    "base_econ[\"wage_bill\"] = base_econ[\"Earnings_Beginning_Qtr\"] * base_econ[\"Employment_Count\"]\n",
    "\n",
    "# --- 2C) State-quarter totals (sum across counties and (if parts) industries)\n",
    "state_qtr = (\n",
    "    base_econ.groupby([\"State\", \"Year\", \"quarter\"], as_index=False)\n",
    "    .agg(\n",
    "        state_emp_qtr=(\"Employment_Count\", sum_mc1),\n",
    "        state_hires_qtr=(\"New_Hires\", sum_mc1),\n",
    "        state_wage_bill_qtr=(\"wage_bill\", sum_mc1),\n",
    "    )\n",
    ")\n",
    "\n",
    "state_qtr[\"state_avg_earnings_qtr\"] = safe_div(state_qtr[\"state_wage_bill_qtr\"], state_qtr[\"state_emp_qtr\"])\n",
    "\n",
    "# --- 2D) State-year aggregation\n",
    "state_annual = (\n",
    "    state_qtr.groupby([\"State\", \"Year\"], as_index=False)\n",
    "    .agg(\n",
    "        state_emp_avg=(\"state_emp_qtr\", \"mean\"),   # stock -> average across quarters\n",
    "        state_emp_sum=(\"state_emp_qtr\", sum_mc1),  # person-quarters\n",
    "        state_hires_total=(\"state_hires_qtr\", sum_mc1),  # flow -> sum across quarters\n",
    "        state_wage_bill_total=(\"state_wage_bill_qtr\", sum_mc1),\n",
    "        state_avg_earnings_meanq=(\"state_avg_earnings_qtr\", \"mean\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Employment-weighted earnings across quarters (recommended)\n",
    "state_annual[\"state_avg_earnings\"] = safe_div(state_annual[\"state_wage_bill_total\"], state_annual[\"state_emp_sum\"])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 3: Merge Econ + Health (annual)\n",
    "# ---------------------------------------------------------\n",
    "state_df = state_annual.merge(state_health, on=[\"State\", \"Year\"], how=\"left\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 4: Core annual rates + YoY (your existing features)\n",
    "# ---------------------------------------------------------\n",
    "pop = state_df[\"state_total_pop\"].replace({0: np.nan})\n",
    "emp_avg = state_df[\"state_emp_avg\"].replace({0: np.nan})\n",
    "\n",
    "state_df[\"econ_emp_per_1k\"] = 1000 * safe_div(state_df[\"state_emp_avg\"], pop)\n",
    "state_df[\"econ_hires_per_1k\"] = 1000 * safe_div(state_df[\"state_hires_total\"], pop)\n",
    "state_df[\"econ_hire_rate_annual\"] = safe_div(state_df[\"state_hires_total\"], emp_avg)\n",
    "\n",
    "state_df = state_df.sort_values([\"State\", \"Year\"]).copy()\n",
    "\n",
    "def yoy_growth(s):\n",
    "    prev = s.shift(1)\n",
    "    return safe_div(s - prev, prev)\n",
    "\n",
    "state_df[\"growth_emp_yoy\"] = state_df.groupby(\"State\")[\"state_emp_avg\"].transform(yoy_growth)\n",
    "state_df[\"growth_earn_yoy\"] = state_df.groupby(\"State\")[\"state_avg_earnings\"].transform(yoy_growth)\n",
    "state_df[\"growth_hires_yoy\"] = state_df.groupby(\"State\")[\"state_hires_total\"].transform(yoy_growth)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 5: NEW TOPIC: Relativity + Divergence from US trends\n",
    "# ---------------------------------------------------------\n",
    "# Level rates\n",
    "state_df[\"emp_rate\"]   = safe_div(state_df[\"state_emp_avg\"], pop)\n",
    "state_df[\"hires_rate\"] = safe_div(state_df[\"state_hires_total\"], pop)\n",
    "state_df[\"earn_level\"] = state_df[\"state_avg_earnings\"]  # rename for clarity\n",
    "\n",
    "# US aggregates by year (consistent definitions)\n",
    "us = (\n",
    "    state_df.groupby(\"Year\", as_index=False)\n",
    "    .apply(lambda s: pd.Series({\n",
    "        \"us_pop\": np.nansum(s[\"state_total_pop\"]),\n",
    "        \"us_emp_avg\": np.nansum(s[\"state_emp_avg\"]),\n",
    "        \"us_hires_total\": np.nansum(s[\"state_hires_total\"]),\n",
    "        \"us_earn_level\": weighted_mean(s[\"earn_level\"].to_numpy(), s[\"state_emp_sum\"].to_numpy())\n",
    "    }))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "us[\"us_emp_rate\"] = safe_div(us[\"us_emp_avg\"], us[\"us_pop\"])\n",
    "us[\"us_hires_rate\"] = safe_div(us[\"us_hires_total\"], us[\"us_pop\"])\n",
    "\n",
    "state_df = state_df.merge(us[[\"Year\", \"us_emp_rate\", \"us_hires_rate\", \"us_earn_level\"]], on=\"Year\", how=\"left\")\n",
    "\n",
    "# Relativity indices (ratio + log ratio)\n",
    "state_df[\"rel_emp_idx\"] = safe_div(state_df[\"emp_rate\"], state_df[\"us_emp_rate\"])\n",
    "state_df[\"rel_hires_idx\"] = safe_div(state_df[\"hires_rate\"], state_df[\"us_hires_rate\"])\n",
    "state_df[\"rel_earn_idx\"] = safe_div(state_df[\"earn_level\"], state_df[\"us_earn_level\"])\n",
    "\n",
    "state_df[\"log_rel_emp\"] = np.log(state_df[\"rel_emp_idx\"])\n",
    "state_df[\"log_rel_hires\"] = np.log(state_df[\"rel_hires_idx\"])\n",
    "state_df[\"log_rel_earn\"] = np.log(state_df[\"rel_earn_idx\"])\n",
    "\n",
    "# Divergence (growth gap vs US)\n",
    "state_df[\"emp_rate_yoy\"] = state_df.groupby(\"State\")[\"emp_rate\"].pct_change()\n",
    "state_df[\"hires_rate_yoy\"] = state_df.groupby(\"State\")[\"hires_rate\"].pct_change()\n",
    "state_df[\"earn_level_yoy\"] = state_df.groupby(\"State\")[\"earn_level\"].pct_change()\n",
    "\n",
    "us_tr = us.sort_values(\"Year\").copy()\n",
    "us_tr[\"us_emp_rate_yoy\"] = us_tr[\"us_emp_rate\"].pct_change()\n",
    "us_tr[\"us_hires_rate_yoy\"] = us_tr[\"us_hires_rate\"].pct_change()\n",
    "us_tr[\"us_earn_level_yoy\"] = us_tr[\"us_earn_level\"].pct_change()\n",
    "\n",
    "state_df = state_df.merge(us_tr[[\"Year\",\"us_emp_rate_yoy\",\"us_hires_rate_yoy\",\"us_earn_level_yoy\"]],\n",
    "                          on=\"Year\", how=\"left\")\n",
    "\n",
    "state_df[\"div_emp_rate_yoy\"] = state_df[\"emp_rate_yoy\"] - state_df[\"us_emp_rate_yoy\"]\n",
    "state_df[\"div_hires_rate_yoy\"] = state_df[\"hires_rate_yoy\"] - state_df[\"us_hires_rate_yoy\"]\n",
    "state_df[\"div_earnings_yoy\"] = state_df[\"earn_level_yoy\"] - state_df[\"us_earn_level_yoy\"]\n",
    "\n",
    "# Optional composite (z within year to remove scale)\n",
    "for c in [\"log_rel_emp\",\"log_rel_hires\",\"log_rel_earn\",\"div_emp_rate_yoy\",\"div_hires_rate_yoy\",\"div_earnings_yoy\"]:\n",
    "    mu = state_df.groupby(\"Year\")[c].transform(\"mean\")\n",
    "    sd = state_df.groupby(\"Year\")[c].transform(\"std\")\n",
    "    state_df[c + \"_z\"] = (state_df[c] - mu) / sd\n",
    "\n",
    "state_df[\"labor_rel_div_index\"] = (\n",
    "    state_df[\"log_rel_emp_z\"] + state_df[\"log_rel_hires_z\"] + state_df[\"log_rel_earn_z\"]\n",
    "    + state_df[\"div_emp_rate_yoy_z\"]\n",
    ")\n",
    "\n",
    "# Final cleanup\n",
    "state_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 6: Quick diagnostics (crucial)\n",
    "# ---------------------------------------------------------\n",
    "print(\"ECON MODE:\", MODE, \"| total_coverage =\", round(float(total_coverage), 3))\n",
    "print(\"Inferred suppressed flag values used (empty = not applied):\")\n",
    "for k,v in suppressed_values.items():\n",
    "    print(\" \", k, \":\", v)\n",
    "\n",
    "print(\"\\nNon-null rates in annual econ:\")\n",
    "print(state_df[[\"state_emp_avg\",\"state_hires_total\",\"state_avg_earnings\"]].notna().mean().to_string())\n",
    "\n",
    "print(\"\\nShare of zeros among non-missing annual econ (should NOT be ~1.0):\")\n",
    "for c in [\"state_emp_avg\",\"state_hires_total\"]:\n",
    "    s = state_df[c]\n",
    "    print(c, \"zero_share=\", float((s.fillna(np.nan) == 0).mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "281d8596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Year</th>\n",
       "      <th>state_emp_avg</th>\n",
       "      <th>state_emp_sum</th>\n",
       "      <th>state_hires_total</th>\n",
       "      <th>state_wage_bill_total</th>\n",
       "      <th>state_avg_earnings_meanq</th>\n",
       "      <th>state_avg_earnings</th>\n",
       "      <th>state_total_pop</th>\n",
       "      <th>STATE_# Alcohol-Impaired Driving Deaths</th>\n",
       "      <th>...</th>\n",
       "      <th>div_emp_rate_yoy</th>\n",
       "      <th>div_hires_rate_yoy</th>\n",
       "      <th>div_earnings_yoy</th>\n",
       "      <th>log_rel_emp_z</th>\n",
       "      <th>log_rel_hires_z</th>\n",
       "      <th>log_rel_earn_z</th>\n",
       "      <th>div_emp_rate_yoy_z</th>\n",
       "      <th>div_hires_rate_yoy_z</th>\n",
       "      <th>div_earnings_yoy_z</th>\n",
       "      <th>labor_rel_div_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2014</td>\n",
       "      <td>1500428.00</td>\n",
       "      <td>6001712.0</td>\n",
       "      <td>1098176.0</td>\n",
       "      <td>1.938636e+10</td>\n",
       "      <td>3229.825614</td>\n",
       "      <td>3230.138191</td>\n",
       "      <td>4843737.0</td>\n",
       "      <td>1273.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.181368</td>\n",
       "      <td>-1.220233</td>\n",
       "      <td>-0.906968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2015</td>\n",
       "      <td>1528070.25</td>\n",
       "      <td>6112281.0</td>\n",
       "      <td>1162571.0</td>\n",
       "      <td>2.016044e+10</td>\n",
       "      <td>3297.805304</td>\n",
       "      <td>3298.350384</td>\n",
       "      <td>4854803.0</td>\n",
       "      <td>1204.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000769</td>\n",
       "      <td>0.013846</td>\n",
       "      <td>-0.008879</td>\n",
       "      <td>-1.191267</td>\n",
       "      <td>-1.289439</td>\n",
       "      <td>-0.938068</td>\n",
       "      <td>0.252237</td>\n",
       "      <td>0.520389</td>\n",
       "      <td>-0.486096</td>\n",
       "      <td>-3.166536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2016</td>\n",
       "      <td>1552800.75</td>\n",
       "      <td>6211203.0</td>\n",
       "      <td>1218301.0</td>\n",
       "      <td>2.072056e+10</td>\n",
       "      <td>3335.534152</td>\n",
       "      <td>3335.997555</td>\n",
       "      <td>4866824.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0.032739</td>\n",
       "      <td>-0.001396</td>\n",
       "      <td>-1.145744</td>\n",
       "      <td>-0.805934</td>\n",
       "      <td>-0.926453</td>\n",
       "      <td>0.380082</td>\n",
       "      <td>0.659294</td>\n",
       "      <td>0.071821</td>\n",
       "      <td>-2.498050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2017</td>\n",
       "      <td>1570609.00</td>\n",
       "      <td>6282436.0</td>\n",
       "      <td>1258492.0</td>\n",
       "      <td>2.136434e+10</td>\n",
       "      <td>3400.619157</td>\n",
       "      <td>3400.645682</td>\n",
       "      <td>4877989.0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.015026</td>\n",
       "      <td>-0.013222</td>\n",
       "      <td>-1.151876</td>\n",
       "      <td>-0.937398</td>\n",
       "      <td>-0.959140</td>\n",
       "      <td>0.465198</td>\n",
       "      <td>0.854117</td>\n",
       "      <td>-1.122061</td>\n",
       "      <td>-2.583216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2018</td>\n",
       "      <td>1593598.00</td>\n",
       "      <td>6374392.0</td>\n",
       "      <td>1345355.0</td>\n",
       "      <td>2.233824e+10</td>\n",
       "      <td>3504.180435</td>\n",
       "      <td>3504.371411</td>\n",
       "      <td>4891628.0</td>\n",
       "      <td>1146.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000372</td>\n",
       "      <td>0.043352</td>\n",
       "      <td>-0.002924</td>\n",
       "      <td>-1.157168</td>\n",
       "      <td>-0.612184</td>\n",
       "      <td>-0.977010</td>\n",
       "      <td>0.240409</td>\n",
       "      <td>2.074639</td>\n",
       "      <td>-0.322340</td>\n",
       "      <td>-2.505953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020</td>\n",
       "      <td>197928.25</td>\n",
       "      <td>791713.0</td>\n",
       "      <td>184673.0</td>\n",
       "      <td>3.045369e+09</td>\n",
       "      <td>3844.774056</td>\n",
       "      <td>3846.556459</td>\n",
       "      <td>570864.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010319</td>\n",
       "      <td>-0.036672</td>\n",
       "      <td>-0.070905</td>\n",
       "      <td>-0.344930</td>\n",
       "      <td>1.637040</td>\n",
       "      <td>-0.910118</td>\n",
       "      <td>0.288401</td>\n",
       "      <td>-0.725790</td>\n",
       "      <td>-2.708239</td>\n",
       "      <td>0.670394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2021</td>\n",
       "      <td>201439.00</td>\n",
       "      <td>805756.0</td>\n",
       "      <td>201970.0</td>\n",
       "      <td>3.218701e+09</td>\n",
       "      <td>3990.137719</td>\n",
       "      <td>3994.634706</td>\n",
       "      <td>572889.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003064</td>\n",
       "      <td>-0.095219</td>\n",
       "      <td>-0.038187</td>\n",
       "      <td>-0.352796</td>\n",
       "      <td>1.031721</td>\n",
       "      <td>-1.039499</td>\n",
       "      <td>0.034312</td>\n",
       "      <td>-1.789914</td>\n",
       "      <td>-1.469311</td>\n",
       "      <td>-0.326263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2022</td>\n",
       "      <td>207782.25</td>\n",
       "      <td>831129.0</td>\n",
       "      <td>206690.0</td>\n",
       "      <td>3.579169e+09</td>\n",
       "      <td>4303.279713</td>\n",
       "      <td>4306.393788</td>\n",
       "      <td>575106.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018378</td>\n",
       "      <td>0.006598</td>\n",
       "      <td>0.042850</td>\n",
       "      <td>-0.440596</td>\n",
       "      <td>1.036538</td>\n",
       "      <td>-0.921560</td>\n",
       "      <td>-0.805266</td>\n",
       "      <td>0.521618</td>\n",
       "      <td>1.739180</td>\n",
       "      <td>-1.130883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2023</td>\n",
       "      <td>213206.50</td>\n",
       "      <td>852826.0</td>\n",
       "      <td>199070.0</td>\n",
       "      <td>3.855354e+09</td>\n",
       "      <td>4520.934060</td>\n",
       "      <td>4520.680791</td>\n",
       "      <td>578239.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006757</td>\n",
       "      <td>0.061931</td>\n",
       "      <td>0.013430</td>\n",
       "      <td>-0.406893</td>\n",
       "      <td>1.476636</td>\n",
       "      <td>-0.866242</td>\n",
       "      <td>1.078364</td>\n",
       "      <td>1.092691</td>\n",
       "      <td>1.420374</td>\n",
       "      <td>1.281865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2024</td>\n",
       "      <td>214534.00</td>\n",
       "      <td>858136.0</td>\n",
       "      <td>189813.0</td>\n",
       "      <td>4.011901e+09</td>\n",
       "      <td>4674.161578</td>\n",
       "      <td>4675.134297</td>\n",
       "      <td>580752.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.033569</td>\n",
       "      <td>-0.008304</td>\n",
       "      <td>-0.418045</td>\n",
       "      <td>1.469600</td>\n",
       "      <td>-0.911370</td>\n",
       "      <td>0.032868</td>\n",
       "      <td>0.397667</td>\n",
       "      <td>-1.107268</td>\n",
       "      <td>0.173054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>539 rows  72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       State  Year  state_emp_avg  state_emp_sum  state_hires_total  \\\n",
       "0    Alabama  2014     1500428.00      6001712.0          1098176.0   \n",
       "1    Alabama  2015     1528070.25      6112281.0          1162571.0   \n",
       "2    Alabama  2016     1552800.75      6211203.0          1218301.0   \n",
       "3    Alabama  2017     1570609.00      6282436.0          1258492.0   \n",
       "4    Alabama  2018     1593598.00      6374392.0          1345355.0   \n",
       "..       ...   ...            ...            ...                ...   \n",
       "534  Wyoming  2020      197928.25       791713.0           184673.0   \n",
       "535  Wyoming  2021      201439.00       805756.0           201970.0   \n",
       "536  Wyoming  2022      207782.25       831129.0           206690.0   \n",
       "537  Wyoming  2023      213206.50       852826.0           199070.0   \n",
       "538  Wyoming  2024      214534.00       858136.0           189813.0   \n",
       "\n",
       "     state_wage_bill_total  state_avg_earnings_meanq  state_avg_earnings  \\\n",
       "0             1.938636e+10               3229.825614         3230.138191   \n",
       "1             2.016044e+10               3297.805304         3298.350384   \n",
       "2             2.072056e+10               3335.534152         3335.997555   \n",
       "3             2.136434e+10               3400.619157         3400.645682   \n",
       "4             2.233824e+10               3504.180435         3504.371411   \n",
       "..                     ...                       ...                 ...   \n",
       "534           3.045369e+09               3844.774056         3846.556459   \n",
       "535           3.218701e+09               3990.137719         3994.634706   \n",
       "536           3.579169e+09               4303.279713         4306.393788   \n",
       "537           3.855354e+09               4520.934060         4520.680791   \n",
       "538           4.011901e+09               4674.161578         4675.134297   \n",
       "\n",
       "     state_total_pop  STATE_# Alcohol-Impaired Driving Deaths  ...  \\\n",
       "0          4843737.0                                   1273.0  ...   \n",
       "1          4854803.0                                   1204.0  ...   \n",
       "2          4866824.0                                   1274.0  ...   \n",
       "3          4877989.0                                   1260.0  ...   \n",
       "4          4891628.0                                   1146.0  ...   \n",
       "..               ...                                      ...  ...   \n",
       "534         570864.0                                    219.0  ...   \n",
       "535         572889.0                                    209.0  ...   \n",
       "536         575106.0                                    197.0  ...   \n",
       "537         578239.0                                    197.0  ...   \n",
       "538         580752.0                                    200.0  ...   \n",
       "\n",
       "     div_emp_rate_yoy  div_hires_rate_yoy  div_earnings_yoy  log_rel_emp_z  \\\n",
       "0                 NaN                 NaN               NaN      -1.181368   \n",
       "1           -0.000769            0.013846         -0.008879      -1.191267   \n",
       "2            0.001445            0.032739         -0.001396      -1.145744   \n",
       "3            0.000301            0.015026         -0.013222      -1.151876   \n",
       "4           -0.000372            0.043352         -0.002924      -1.157168   \n",
       "..                ...                 ...               ...            ...   \n",
       "534          0.010319           -0.036672         -0.070905      -0.344930   \n",
       "535         -0.003064           -0.095219         -0.038187      -0.352796   \n",
       "536         -0.018378            0.006598          0.042850      -0.440596   \n",
       "537          0.006757            0.061931          0.013430      -0.406893   \n",
       "538          0.001610            0.033569         -0.008304      -0.418045   \n",
       "\n",
       "     log_rel_hires_z  log_rel_earn_z  div_emp_rate_yoy_z  \\\n",
       "0          -1.220233       -0.906968                 NaN   \n",
       "1          -1.289439       -0.938068            0.252237   \n",
       "2          -0.805934       -0.926453            0.380082   \n",
       "3          -0.937398       -0.959140            0.465198   \n",
       "4          -0.612184       -0.977010            0.240409   \n",
       "..               ...             ...                 ...   \n",
       "534         1.637040       -0.910118            0.288401   \n",
       "535         1.031721       -1.039499            0.034312   \n",
       "536         1.036538       -0.921560           -0.805266   \n",
       "537         1.476636       -0.866242            1.078364   \n",
       "538         1.469600       -0.911370            0.032868   \n",
       "\n",
       "     div_hires_rate_yoy_z  div_earnings_yoy_z  labor_rel_div_index  \n",
       "0                     NaN                 NaN                  NaN  \n",
       "1                0.520389           -0.486096            -3.166536  \n",
       "2                0.659294            0.071821            -2.498050  \n",
       "3                0.854117           -1.122061            -2.583216  \n",
       "4                2.074639           -0.322340            -2.505953  \n",
       "..                    ...                 ...                  ...  \n",
       "534             -0.725790           -2.708239             0.670394  \n",
       "535             -1.789914           -1.469311            -0.326263  \n",
       "536              0.521618            1.739180            -1.130883  \n",
       "537              1.092691            1.420374             1.281865  \n",
       "538              0.397667           -1.107268             0.173054  \n",
       "\n",
       "[539 rows x 72 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1397513b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: rel_emp_idx\n",
      "Rows: 539 | Features: 23\n",
      "X columns: ['state_hires_total', 'econ_hires_per_1k', 'state_avg_earnings', 'state_avg_earnings_meanq', 'state_total_pop', 'STATE_% Children in Poverty', 'STATE_% Uninsured', 'STATE_% Adults with Obesity', 'STATE_% Smokers', 'STATE_% Fair or Poor Health', 'STATE_Mentally Unhealthy Days', 'STATE_Physically Unhealthy Days', 'STATE_Food Environment Index', 'STATE_Income Ratio', 'STATE_Primary Care Physicians Rate', 'STATE_Preventable Hospitalization Rate', 'STATE_Violent Crime Rate', 'STATE_Teen Birth Rate', 'STATE_% With Access to Exercise Opportunities', 'STATE_% Some College', 'STATE_Premature Deaths_per_100k', 'STATE_# Driving Deaths_per_100k', 'STATE_# Alcohol-Impaired Driving Deaths_per_100k']\n",
      "\n",
      "Train years: [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
      "Test years : [2022, 2023, 2024]\n",
      "Train n: 395 | Test n: 144\n",
      "folds_tune: 4 | folds_oof: 5\n",
      "\n",
      "Lasso alpha_min: 0.00011905765105954004 | alpha_1se: 0.01964032247770044\n",
      "\n",
      "Chosen Lasso variant: lasso_alpha_1se | OOF RMSE: 0.08333308134017785 | Test RMSE: 0.08656111669828241\n",
      "\n",
      "--- Tuning RandomForest (train-only, walk-forward CV) ---\n",
      "RF best CV RMSE: 0.0605965086987604\n",
      "RF best params: {'rf__n_estimators': 300, 'rf__min_samples_split': 30, 'rf__min_samples_leaf': 2, 'rf__max_features': 0.7, 'rf__max_depth': 10, 'rf__bootstrap': True}\n",
      "\n",
      "--- Tuning LightGBM (train-only, walk-forward CV) ---\n",
      "LGB best CV RMSE: 0.056691339016706616\n",
      "LGB best params: {'lgb__subsample': 1.0, 'lgb__reg_lambda': 5.0, 'lgb__reg_alpha': 0.05, 'lgb__num_leaves': 95, 'lgb__n_estimators': 1500, 'lgb__min_child_samples': 30, 'lgb__max_depth': 5, 'lgb__learning_rate': 0.1, 'lgb__colsample_bytree': 0.6}\n",
      "\n",
      "=== MODEL COMPARISON (sorted by test_rmse then oof_rmse) ===\n",
      "             model    oof_r2  oof_rmse   test_r2  test_rmse  test_mae  \\\n",
      "2        lgb_tuned  0.865422  0.061885  0.846024   0.063200  0.045686   \n",
      "1         rf_tuned  0.866937  0.061536  0.747293   0.080966  0.055745   \n",
      "0  lasso_alpha_1se  0.755973  0.083333  0.711159   0.086561  0.062732   \n",
      "\n",
      "   oof_kept  oof_dropped  \n",
      "2       245          150  \n",
      "1       245          150  \n",
      "0       245          150  \n",
      "\n",
      ">>> BEST SINGLE MODEL: lgb_tuned\n",
      "\n",
      "Mean-ensemble TEST metrics: {'r2': 0.8148506559309068, 'rmse': 0.06930343807936068, 'mae': 0.05093604540102793}\n",
      "\n",
      "Stacking meta info: {'meta_kept': 245, 'meta_dropped': 150}\n",
      "Stacking weights:\n",
      " lasso    0.202393\n",
      "rf       0.507854\n",
      "lgb      0.405861\n",
      "dtype: float64\n",
      "Stacking TRAIN OOF metrics: {'r2': 0.9303462706648966, 'rmse': 0.04452161099435748, 'mae': 0.033787836732719914}\n",
      "Stacking TEST metrics: {'r2': 0.863382072008039, 'rmse': 0.05953159771709765, 'mae': 0.04102602407135201}\n",
      "\n",
      "=== Rolling-origin per-year diagnostics ===\n",
      "    test_year  train_n  test_n        r2      rmse       mae  model\n",
      "0        2017      150      49  0.759379  0.084343  0.067651  lasso\n",
      "1        2018      199      49  0.733683  0.087743  0.071288  lasso\n",
      "2        2019      248      49  0.794569  0.076164  0.057332  lasso\n",
      "3        2020      297      49  0.766332  0.082784  0.054181  lasso\n",
      "4        2021      346      49  0.723257  0.085177  0.070911  lasso\n",
      "5        2022      395      48  0.741134  0.083695  0.066644  lasso\n",
      "6        2023      443      48  0.693150  0.090052  0.061578  lasso\n",
      "7        2024      491      48  0.679148  0.088361  0.059311  lasso\n",
      "16       2017      150      49  0.796329  0.077598  0.060449    lgb\n",
      "17       2018      199      49  0.840322  0.067941  0.056002    lgb\n",
      "18       2019      248      49  0.920539  0.047369  0.034981    lgb\n",
      "19       2020      297      49  0.913331  0.050417  0.043150    lgb\n",
      "20       2021      346      49  0.857891  0.061038  0.046138    lgb\n",
      "21       2022      395      48  0.925120  0.045014  0.033383    lgb\n",
      "22       2023      443      48  0.826753  0.067665  0.052031    lgb\n",
      "23       2024      491      48  0.910801  0.046589  0.036395    lgb\n",
      "8        2017      150      49  0.864301  0.063339  0.047170     rf\n",
      "9        2018      199      49  0.867431  0.061906  0.050381     rf\n",
      "10       2019      248      49  0.910147  0.050371  0.040045     rf\n",
      "11       2020      297      49  0.824848  0.071673  0.049194     rf\n",
      "12       2021      346      49  0.869749  0.058436  0.042912     rf\n",
      "13       2022      395      48  0.881105  0.056721  0.043601     rf\n",
      "14       2023      443      48  0.805618  0.071673  0.056035     rf\n",
      "15       2024      491      48  0.916318  0.045126  0.035858     rf\n",
      "\n",
      "=== LASSO COEFFICIENTS (chosen variant) ===\n",
      "Non-zero coefficients: 7 out of 23\n",
      "\n",
      "Top 20 by |coef|:\n",
      "econ_hires_per_1k                                0.053177\n",
      "STATE_% Some College                             0.043331\n",
      "STATE_Violent Crime Rate                         0.037932\n",
      "STATE_Primary Care Physicians Rate               0.032813\n",
      "STATE_Physically Unhealthy Days                 -0.016583\n",
      "STATE_# Driving Deaths_per_100k                 -0.008828\n",
      "STATE_Income Ratio                               0.007670\n",
      "STATE_Food Environment Index                     0.000000\n",
      "STATE_Premature Deaths_per_100k                 -0.000000\n",
      "STATE_% With Access to Exercise Opportunities    0.000000\n",
      "STATE_Teen Birth Rate                            0.000000\n",
      "STATE_Preventable Hospitalization Rate          -0.000000\n",
      "state_hires_total                               -0.000000\n",
      "STATE_Mentally Unhealthy Days                   -0.000000\n",
      "STATE_% Fair or Poor Health                     -0.000000\n",
      "STATE_% Smokers                                  0.000000\n",
      "STATE_% Adults with Obesity                     -0.000000\n",
      "STATE_% Uninsured                               -0.000000\n",
      "STATE_% Children in Poverty                      0.000000\n",
      "state_total_pop                                 -0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FULL UPDATED PIPELINE (Lasso + RandomForest + LightGBM)\n",
    "#  - Target: rel_emp_idx\n",
    "#  - Test years: 2022-2024 only\n",
    "#  - Time-aware tuning: walk-forward CV on TRAIN only\n",
    "#  - Honest evaluation:\n",
    "#      * Walk-forward OOF on TRAIN (for sanity / robustness)\n",
    "#      * True holdout TEST (2022-2024)\n",
    "#  - Robustness:\n",
    "#      * Mean ensemble\n",
    "#      * Ridge stacking with walk-forward OOF meta-features (leakage-safe)\n",
    "#      * Rolling-origin (per-year) diagnostics\n",
    "#  - X constraints enforced:\n",
    "#      * NO emp-related predictors\n",
    "#      * NO yoy/log/z/us/rel variables in X\n",
    "#      * X includes hires + earnings + health\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, Lasso, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import root_mean_squared_error\n",
    "except Exception:\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# ---- LightGBM ----\n",
    "# If you get ImportError: install first:\n",
    "#   pip install lightgbm\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        \"LightGBM is not installed. Install it via: pip install lightgbm\\n\"\n",
    "        \"On some Windows setups you may need Visual C++ Build Tools.\"\n",
    "    ) from e\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helpers\n",
    "# ------------------------------------------------------------\n",
    "def eval_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "        \"rmse\": float(root_mean_squared_error(y_true, y_pred)),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "def make_walk_forward_folds(years: np.ndarray, min_train_years: int = 1):\n",
    "    years = np.asarray(years).astype(int)\n",
    "    uniq = np.sort(np.unique(years))\n",
    "    if len(uniq) < (min_train_years + 1):\n",
    "        raise ValueError(\"Not enough unique years for walk-forward folds.\")\n",
    "\n",
    "    folds = []\n",
    "    for i in range(min_train_years, len(uniq)):\n",
    "        tr_years = uniq[:i]\n",
    "        va_year = uniq[i]\n",
    "        tr_idx = np.where(np.isin(years, tr_years))[0]\n",
    "        va_idx = np.where(years == va_year)[0]\n",
    "        if len(tr_idx) and len(va_idx):\n",
    "            folds.append((tr_idx, va_idx))\n",
    "\n",
    "    if not folds:\n",
    "        raise ValueError(\"No folds created.\")\n",
    "    return folds\n",
    "\n",
    "def oof_preds_walk_forward(model_fixed, X_train, y_train, folds):\n",
    "    y_train = np.asarray(y_train)\n",
    "    oof = np.full(len(y_train), np.nan, dtype=float)\n",
    "    for tr_idx, va_idx in folds:\n",
    "        m = clone(model_fixed)\n",
    "        m.fit(X_train.iloc[tr_idx], y_train[tr_idx])\n",
    "        oof[va_idx] = m.predict(X_train.iloc[va_idx])\n",
    "    return oof\n",
    "\n",
    "def pick_alpha_1se_from_lassocv(lasso_cv: LassoCV):\n",
    "    \"\"\"\n",
    "    1SE rule: pick the MOST regularized alpha whose mean CV MSE is within\n",
    "    1 std error of the minimum mean MSE.\n",
    "    \"\"\"\n",
    "    mse_path = lasso_cv.mse_path_\n",
    "    if mse_path.shape[0] == len(lasso_cv.alphas_):\n",
    "        mean_mse = mse_path.mean(axis=1)\n",
    "        std_mse  = mse_path.std(axis=1)\n",
    "    else:\n",
    "        mean_mse = mse_path.mean(axis=0)\n",
    "        std_mse  = mse_path.std(axis=0)\n",
    "\n",
    "    alphas = np.asarray(lasso_cv.alphas_).astype(float)\n",
    "    i_min = int(np.argmin(mean_mse))\n",
    "    thresh = mean_mse[i_min] + std_mse[i_min]\n",
    "    candidates = alphas[mean_mse <= thresh]\n",
    "    return float(np.max(candidates)) if len(candidates) else float(lasso_cv.alpha_)\n",
    "\n",
    "def fit_eval_one(model_fixed, X_train, y_train, X_test, y_test, folds_oof, name=\"model\"):\n",
    "    # Walk-forward OOF on TRAIN\n",
    "    oof = oof_preds_walk_forward(model_fixed, X_train, y_train, folds_oof)\n",
    "    valid = np.isfinite(oof)\n",
    "\n",
    "    # Fit on full TRAIN\n",
    "    m = clone(model_fixed)\n",
    "    m.fit(X_train, y_train)\n",
    "\n",
    "    pred_tr_in = m.predict(X_train)\n",
    "    pred_te = m.predict(X_test)\n",
    "\n",
    "    out = {\n",
    "        \"model\": name,\n",
    "        \"train_in\": eval_metrics(y_train, pred_tr_in),\n",
    "        \"train_oof\": eval_metrics(y_train[valid], oof[valid]) if valid.sum() else None,\n",
    "        \"test\": eval_metrics(y_test, pred_te),\n",
    "        \"oof_kept\": int(valid.sum()),\n",
    "        \"oof_dropped\": int((~valid).sum()),\n",
    "        \"fitted\": m\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def stack_ridge_oof(base_models_fixed, X_train, y_train, X_test, folds_oof, meta_alphas=None):\n",
    "    \"\"\"\n",
    "    Ridge stacking using walk-forward OOF features (leakage-safe).\n",
    "    - meta-train features are true OOF predictions by year\n",
    "    - meta-test uses base models fit on full TRAIN\n",
    "    \"\"\"\n",
    "    if meta_alphas is None:\n",
    "        meta_alphas = np.logspace(-3, 3, 19)\n",
    "\n",
    "    y_train = np.asarray(y_train)\n",
    "\n",
    "    meta_train = np.column_stack([\n",
    "        oof_preds_walk_forward(m, X_train, y_train, folds_oof)\n",
    "        for m in base_models_fixed.values()\n",
    "    ])\n",
    "    valid = np.all(np.isfinite(meta_train), axis=1)\n",
    "    if valid.sum() < 60:\n",
    "        raise RuntimeError(f\"Too few OOF rows for stacking meta-learner: kept={int(valid.sum())}\")\n",
    "\n",
    "    meta_test = np.column_stack([\n",
    "        clone(m).fit(X_train, y_train).predict(X_test)\n",
    "        for m in base_models_fixed.values()\n",
    "    ])\n",
    "\n",
    "    meta = RidgeCV(alphas=meta_alphas, cv=5)\n",
    "    meta.fit(meta_train[valid], y_train[valid])\n",
    "\n",
    "    ens_train_oof = np.full(len(y_train), np.nan, dtype=float)\n",
    "    ens_train_oof[valid] = meta.predict(meta_train[valid])\n",
    "    ens_test = meta.predict(meta_test)\n",
    "\n",
    "    weights = pd.Series(meta.coef_, index=list(base_models_fixed.keys()))\n",
    "    info = {\"meta_kept\": int(valid.sum()), \"meta_dropped\": int((~valid).sum())}\n",
    "    return ens_train_oof, ens_test, valid, weights, info\n",
    "\n",
    "def rolling_origin_eval(model_fixed, df_model, X_all, y_all, start_year=2017, end_year=2024):\n",
    "    years = df_model[\"Year\"].astype(int).values\n",
    "    out = []\n",
    "    for t in range(start_year, end_year + 1):\n",
    "        tr_mask = years <= (t - 1)\n",
    "        te_mask = years == t\n",
    "        if tr_mask.sum() < 80 or te_mask.sum() < 30:\n",
    "            continue\n",
    "        m = clone(model_fixed)\n",
    "        m.fit(X_all.loc[tr_mask], y_all[tr_mask])\n",
    "        pred = m.predict(X_all.loc[te_mask])\n",
    "        met = eval_metrics(y_all[te_mask], pred)\n",
    "        out.append({\"test_year\": t, \"train_n\": int(tr_mask.sum()), \"test_n\": int(te_mask.sum()), **met})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0) BUILD MODELING FRAME FROM state_df\n",
    "# ============================================================\n",
    "df_model = state_df.copy()\n",
    "df_model[\"Year\"] = pd.to_numeric(df_model[\"Year\"], errors=\"coerce\")\n",
    "df_model = df_model[(df_model[\"Year\"] >= 2014) & (df_model[\"Year\"] <= 2024)].copy()\n",
    "df_model[\"State\"] = df_model[\"State\"].astype(str)\n",
    "df_model = df_model.dropna(subset=[\"Year\"]).copy()\n",
    "df_model[\"Year\"] = df_model[\"Year\"].astype(int)\n",
    "\n",
    "target = \"rel_emp_idx\"\n",
    "df_model[target] = pd.to_numeric(df_model[target], errors=\"coerce\")\n",
    "df_model = df_model[np.isfinite(df_model[target])].copy()\n",
    "y_all = df_model[target].values\n",
    "\n",
    "econ_x = [\n",
    "    \"state_hires_total\",\n",
    "    \"econ_hires_per_1k\",\n",
    "    \"state_avg_earnings\",\n",
    "    \"state_avg_earnings_meanq\",\n",
    "    \"state_total_pop\",\n",
    "]\n",
    "health_x = [\n",
    "    \"STATE_% Children in Poverty\",\n",
    "    \"STATE_% Uninsured\",\n",
    "    \"STATE_% Adults with Obesity\",\n",
    "    \"STATE_% Smokers\",\n",
    "    \"STATE_% Fair or Poor Health\",\n",
    "    \"STATE_Mentally Unhealthy Days\",\n",
    "    \"STATE_Physically Unhealthy Days\",\n",
    "    \"STATE_Food Environment Index\",\n",
    "    \"STATE_Income Ratio\",\n",
    "    \"STATE_Primary Care Physicians Rate\",\n",
    "    \"STATE_Preventable Hospitalization Rate\",\n",
    "    \"STATE_Violent Crime Rate\",\n",
    "    \"STATE_Teen Birth Rate\",\n",
    "    \"STATE_% With Access to Exercise Opportunities\",\n",
    "    \"STATE_% Some College\",\n",
    "    \"STATE_Premature Deaths_per_100k\",\n",
    "    \"STATE_# Driving Deaths_per_100k\",\n",
    "    \"STATE_# Alcohol-Impaired Driving Deaths_per_100k\",\n",
    "]\n",
    "\n",
    "X_cols = [c for c in (econ_x + health_x) if c in df_model.columns]\n",
    "X_all = df_model[X_cols].select_dtypes(include=[np.number]).copy()\n",
    "X_all = X_all.dropna(axis=1, how=\"all\")\n",
    "X_all = X_all.loc[:, X_all.nunique(dropna=True) > 1]\n",
    "\n",
    "# ---- HARD GUARDS: enforce your constraints ----\n",
    "bad_patterns = [\n",
    "    \"state_emp\", \"Employment\", \"econ_emp\", \"emp_rate\", \"emp_per\", \"hire_to_emp\",\n",
    "    \"growth_\", \"log_\", \"_yoy\", \"_z\", \"us_\", \"rel_\"  # allow rel_* only as Y, not X\n",
    "]\n",
    "bad_in_X = [c for c in X_all.columns if any(p.lower() in c.lower() for p in bad_patterns)]\n",
    "if bad_in_X:\n",
    "    raise ValueError(f\"Disallowed columns ended up in X: {bad_in_X}\")\n",
    "\n",
    "print(\"Target:\", target)\n",
    "print(\"Rows:\", len(df_model), \"| Features:\", X_all.shape[1])\n",
    "print(\"X columns:\", X_all.columns.tolist())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) TIME SPLIT: Test = 2022-2024 only\n",
    "# ============================================================\n",
    "test_years = {2022, 2023, 2024}\n",
    "train_mask = ~df_model[\"Year\"].isin(test_years)\n",
    "test_mask  =  df_model[\"Year\"].isin(test_years)\n",
    "\n",
    "X_train, X_test = X_all.loc[train_mask].copy(), X_all.loc[test_mask].copy()\n",
    "y_train, y_test = y_all[train_mask.values], y_all[test_mask.values]\n",
    "years_train = df_model.loc[train_mask, \"Year\"].astype(int).values\n",
    "\n",
    "print(\"\\nTrain years:\", sorted(df_model.loc[train_mask, \"Year\"].unique()))\n",
    "print(\"Test years :\", sorted(df_model.loc[test_mask, \"Year\"].unique()))\n",
    "print(\"Train n:\", X_train.shape[0], \"| Test n:\", X_test.shape[0])\n",
    "\n",
    "# Walk-forward folds on TRAIN only:\n",
    "# - folds_tune used for hyperparam selection (validate later years in train window)\n",
    "# - folds_oof used for honest OOF estimation (drops early years with no prior train history)\n",
    "folds_tune = make_walk_forward_folds(years_train, min_train_years=4)  # validate 2018-2021\n",
    "folds_oof  = make_walk_forward_folds(years_train, min_train_years=3)  # validate 2017-2021\n",
    "print(\"folds_tune:\", len(folds_tune), \"| folds_oof:\", len(folds_oof))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) MODEL A: LASSO (main, interpretable) + choose alpha by OOF sanity\n",
    "# ============================================================\n",
    "lasso_cv = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lasso_cv\", LassoCV(cv=folds_tune, random_state=42, n_alphas=300, max_iter=400000))\n",
    "])\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "alpha_min = float(lasso_cv.named_steps[\"lasso_cv\"].alpha_)\n",
    "alpha_1se = pick_alpha_1se_from_lassocv(lasso_cv.named_steps[\"lasso_cv\"])\n",
    "\n",
    "print(\"\\nLasso alpha_min:\", alpha_min, \"| alpha_1se:\", alpha_1se)\n",
    "\n",
    "lasso_min = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lasso\", Lasso(alpha=alpha_min, max_iter=500000, random_state=42))\n",
    "])\n",
    "lasso_1se = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lasso\", Lasso(alpha=alpha_1se, max_iter=500000, random_state=42))\n",
    "])\n",
    "\n",
    "res_lasso_min = fit_eval_one(lasso_min, X_train, y_train, X_test, y_test, folds_oof, name=\"lasso_alpha_min\")\n",
    "res_lasso_1se = fit_eval_one(lasso_1se, X_train, y_train, X_test, y_test, folds_oof, name=\"lasso_alpha_1se\")\n",
    "\n",
    "# Choose lasso variant by OOF RMSE (more honest), tie-break by test RMSE\n",
    "cand = sorted([res_lasso_min, res_lasso_1se], key=lambda r: (r[\"train_oof\"][\"rmse\"], r[\"test\"][\"rmse\"]))\n",
    "res_lasso = cand[0]\n",
    "print(\"\\nChosen Lasso variant:\", res_lasso[\"model\"],\n",
    "      \"| OOF RMSE:\", res_lasso[\"train_oof\"][\"rmse\"],\n",
    "      \"| Test RMSE:\", res_lasso[\"test\"][\"rmse\"])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) MODEL B: RandomForest (nonlinear baseline) + tune\n",
    "# ============================================================\n",
    "rf_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"rf\", RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "rf_param_dist = {\n",
    "    \"rf__n_estimators\": np.arange(300, 1101, 100),\n",
    "    \"rf__max_depth\": [3, 4, 5, 6, 8, 10, None],\n",
    "    \"rf__min_samples_leaf\": [2, 3, 5, 8, 10, 15, 20],\n",
    "    \"rf__min_samples_split\": [10, 20, 30, 40, 60],\n",
    "    \"rf__max_features\": [0.4, 0.5, 0.6, 0.7, \"sqrt\"],\n",
    "    \"rf__bootstrap\": [True],\n",
    "}\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    rf_pipe,\n",
    "    param_distributions=rf_param_dist,\n",
    "    n_iter=50,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=folds_tune,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n--- Tuning RandomForest (train-only, walk-forward CV) ---\")\n",
    "rf_search.fit(X_train, y_train)\n",
    "rf_best = rf_search.best_estimator_\n",
    "print(\"RF best CV RMSE:\", float(-rf_search.best_score_))\n",
    "print(\"RF best params:\", rf_search.best_params_)\n",
    "\n",
    "res_rf = fit_eval_one(rf_best, X_train, y_train, X_test, y_test, folds_oof, name=\"rf_tuned\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) MODEL C: LightGBM (GBDT) + tune (time-aware CV)\n",
    "# ============================================================\n",
    "lgb_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),  # optional but keeps consistency\n",
    "    (\"lgb\", lgb.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Ranges tuned for small-ish panel data (avoid too-deep trees)\n",
    "lgb_param_dist = {\n",
    "    \"lgb__n_estimators\": np.arange(300, 1601, 100),\n",
    "    \"lgb__learning_rate\": np.array([0.01, 0.02, 0.03, 0.05, 0.08, 0.10]),\n",
    "    \"lgb__num_leaves\": np.array([15, 31, 47, 63, 95, 127]),\n",
    "    \"lgb__max_depth\": np.array([-1, 3, 4, 5, 6, 7]),\n",
    "    \"lgb__min_child_samples\": np.array([10, 15, 20, 30, 40, 60]),\n",
    "    \"lgb__subsample\": np.array([0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "    \"lgb__colsample_bytree\": np.array([0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "    \"lgb__reg_alpha\": np.array([0.0, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0]),\n",
    "    \"lgb__reg_lambda\": np.array([0.0, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]),\n",
    "}\n",
    "\n",
    "lgb_search = RandomizedSearchCV(\n",
    "    lgb_pipe,\n",
    "    param_distributions=lgb_param_dist,\n",
    "    n_iter=60,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=folds_tune,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n--- Tuning LightGBM (train-only, walk-forward CV) ---\")\n",
    "lgb_search.fit(X_train, y_train)\n",
    "lgb_best = lgb_search.best_estimator_\n",
    "print(\"LGB best CV RMSE:\", float(-lgb_search.best_score_))\n",
    "print(\"LGB best params:\", lgb_search.best_params_)\n",
    "\n",
    "res_lgb = fit_eval_one(lgb_best, X_train, y_train, X_test, y_test, folds_oof, name=\"lgb_tuned\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) COMPARE MODELS + PICK BEST (by test_rmse, then oof_rmse sanity)\n",
    "# ============================================================\n",
    "results = [res_lasso, res_rf, res_lgb]\n",
    "\n",
    "rows = []\n",
    "for r in results:\n",
    "    rows.append({\n",
    "        \"model\": r[\"model\"],\n",
    "        \"oof_r2\": None if r[\"train_oof\"] is None else r[\"train_oof\"][\"r2\"],\n",
    "        \"oof_rmse\": None if r[\"train_oof\"] is None else r[\"train_oof\"][\"rmse\"],\n",
    "        \"test_r2\": r[\"test\"][\"r2\"],\n",
    "        \"test_rmse\": r[\"test\"][\"rmse\"],\n",
    "        \"test_mae\": r[\"test\"][\"mae\"],\n",
    "        \"oof_kept\": r[\"oof_kept\"],\n",
    "        \"oof_dropped\": r[\"oof_dropped\"],\n",
    "    })\n",
    "\n",
    "perf = pd.DataFrame(rows).sort_values([\"test_rmse\", \"oof_rmse\"])\n",
    "print(\"\\n=== MODEL COMPARISON (sorted by test_rmse then oof_rmse) ===\")\n",
    "print(perf)\n",
    "\n",
    "best_name = perf.iloc[0][\"model\"]\n",
    "best_fitted = {r[\"model\"]: r[\"fitted\"] for r in results}[best_name]\n",
    "print(\"\\n>>> BEST SINGLE MODEL:\", best_name)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) ENSEMBLES as ROBUSTNESS CHECK (not the headline model)\n",
    "#    - mean ensemble\n",
    "#    - ridge stacking (OOF, leakage-safe)\n",
    "# ============================================================\n",
    "# Recreate fixed (cloneable) model objects for stacking:\n",
    "# IMPORTANT: use the \"fixed\" pipelines (no internal CV) where applicable.\n",
    "lasso_for_stack = clone(lasso_min if res_lasso[\"model\"] == \"lasso_alpha_min\" else lasso_1se)\n",
    "rf_for_stack    = clone(rf_best)\n",
    "lgb_for_stack   = clone(lgb_best)\n",
    "\n",
    "base_fixed = {\"lasso\": lasso_for_stack, \"rf\": rf_for_stack, \"lgb\": lgb_for_stack}\n",
    "\n",
    "# (A) Mean ensemble\n",
    "preds_test = []\n",
    "for m in base_fixed.values():\n",
    "    mf = clone(m).fit(X_train, y_train)\n",
    "    preds_test.append(mf.predict(X_test))\n",
    "pred_mean = np.mean(np.column_stack(preds_test), axis=1)\n",
    "print(\"\\nMean-ensemble TEST metrics:\", eval_metrics(y_test, pred_mean))\n",
    "\n",
    "# (B) Ridge stacking with walk-forward OOF meta-features\n",
    "ens_tr_oof, ens_te, valid_meta, weights, info = stack_ridge_oof(base_fixed, X_train, y_train, X_test, folds_oof)\n",
    "print(\"\\nStacking meta info:\", info)\n",
    "print(\"Stacking weights:\\n\", weights)\n",
    "print(\"Stacking TRAIN OOF metrics:\", eval_metrics(y_train[valid_meta], ens_tr_oof[valid_meta]))\n",
    "print(\"Stacking TEST metrics:\", eval_metrics(y_test, ens_te))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Rolling-origin (per-year) diagnostics (robustness & break detection)\n",
    "# ============================================================\n",
    "print(\"\\n=== Rolling-origin per-year diagnostics ===\")\n",
    "start_year = 2017\n",
    "end_year = 2024\n",
    "\n",
    "roll_tables = []\n",
    "for name, m in base_fixed.items():\n",
    "    tab = rolling_origin_eval(m, df_model, X_all, y_all, start_year=start_year, end_year=end_year)\n",
    "    tab[\"model\"] = name\n",
    "    roll_tables.append(tab)\n",
    "\n",
    "roll_all = pd.concat(roll_tables, ignore_index=True).sort_values([\"model\", \"test_year\"])\n",
    "print(roll_all)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) Interpretability hook for paper (Lasso coefficients)\n",
    "# ============================================================\n",
    "print(\"\\n=== LASSO COEFFICIENTS (chosen variant) ===\")\n",
    "chosen_lasso_fitted = res_lasso[\"fitted\"]\n",
    "coef = chosen_lasso_fitted.named_steps[\"lasso\"].coef_\n",
    "coef_s = pd.Series(coef, index=X_train.columns).sort_values(key=lambda s: np.abs(s), ascending=False)\n",
    "\n",
    "print(\"Non-zero coefficients:\", int((coef_s != 0).sum()), \"out of\", len(coef_s))\n",
    "print(\"\\nTop 20 by |coef|:\")\n",
    "print(coef_s.head(20))\n",
    "\n",
    "# Optional: export\n",
    "# coef_s.to_csv(\"lasso_coefficients_rel_emp_idx.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47d9fa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liuc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\liuc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STACKED ENSEMBLE GLOBAL (TEST 2022-2024) SHAP ===\n",
      "Stacking weights used:\n",
      " rf       0.507854\n",
      "lgb      0.405861\n",
      "lasso    0.202393\n",
      "dtype: float64\n",
      "\n",
      "Top 20 features by ensemble mean(|SHAP|):\n",
      "                                                  mean_abs_shap_ens  \\\n",
      "STATE_% Some College                                       0.033261   \n",
      "STATE_# Driving Deaths_per_100k                            0.023598   \n",
      "econ_hires_per_1k                                          0.014847   \n",
      "STATE_% Fair or Poor Health                                0.010264   \n",
      "STATE_Violent Crime Rate                                   0.009405   \n",
      "state_total_pop                                            0.008976   \n",
      "STATE_Physically Unhealthy Days                            0.008756   \n",
      "STATE_Food Environment Index                               0.008280   \n",
      "STATE_# Alcohol-Impaired Driving Deaths_per_100k           0.007913   \n",
      "STATE_Primary Care Physicians Rate                         0.006824   \n",
      "STATE_% Children in Poverty                                0.004806   \n",
      "STATE_% Uninsured                                          0.003652   \n",
      "STATE_Mentally Unhealthy Days                              0.003312   \n",
      "STATE_% Smokers                                            0.003260   \n",
      "STATE_% Adults with Obesity                                0.003100   \n",
      "state_hires_total                                          0.003064   \n",
      "state_avg_earnings                                         0.002889   \n",
      "STATE_Income Ratio                                         0.002133   \n",
      "STATE_% With Access to Exercise Opportunities              0.002071   \n",
      "STATE_Premature Deaths_per_100k                            0.001619   \n",
      "\n",
      "                                                  mean_shap_signed_ens  \n",
      "STATE_% Some College                                          0.014841  \n",
      "STATE_# Driving Deaths_per_100k                              -0.009566  \n",
      "econ_hires_per_1k                                            -0.000770  \n",
      "STATE_% Fair or Poor Health                                   0.000917  \n",
      "STATE_Violent Crime Rate                                     -0.005973  \n",
      "state_total_pop                                              -0.002253  \n",
      "STATE_Physically Unhealthy Days                               0.002735  \n",
      "STATE_Food Environment Index                                  0.004666  \n",
      "STATE_# Alcohol-Impaired Driving Deaths_per_100k             -0.001658  \n",
      "STATE_Primary Care Physicians Rate                           -0.000054  \n",
      "STATE_% Children in Poverty                                   0.001537  \n",
      "STATE_% Uninsured                                             0.001977  \n",
      "STATE_Mentally Unhealthy Days                                -0.003213  \n",
      "STATE_% Smokers                                              -0.000075  \n",
      "STATE_% Adults with Obesity                                   0.000243  \n",
      "state_hires_total                                            -0.000393  \n",
      "state_avg_earnings                                            0.001481  \n",
      "STATE_Income Ratio                                           -0.000080  \n",
      "STATE_% With Access to Exercise Opportunities                -0.000198  \n",
      "STATE_Premature Deaths_per_100k                               0.001155  \n",
      "\n",
      "--- LASSO global SHAP (TEST 2022-2024), top 10 ---\n",
      "                                               mean_abs_shap  mean_shap_signed\n",
      "econ_hires_per_1k                                   0.039209          0.000633\n",
      "STATE_% Some College                                0.037746          0.018659\n",
      "STATE_Primary Care Physicians Rate                  0.025533          0.001775\n",
      "STATE_Physically Unhealthy Days                     0.015295          0.003758\n",
      "STATE_Violent Crime Rate                            0.013858         -0.005453\n",
      "STATE_# Driving Deaths_per_100k                     0.007250         -0.000696\n",
      "STATE_Income Ratio                                  0.005705         -0.001006\n",
      "STATE_Food Environment Index                        0.000000          0.000000\n",
      "STATE_Premature Deaths_per_100k                     0.000000          0.000000\n",
      "STATE_% With Access to Exercise Opportunities       0.000000          0.000000\n",
      "\n",
      "--- RF global SHAP (TEST 2022-2024), top 10 ---\n",
      "                                                  mean_abs_shap  \\\n",
      "STATE_% Some College                                   0.034255   \n",
      "STATE_# Driving Deaths_per_100k                        0.028722   \n",
      "STATE_% Fair or Poor Health                            0.011171   \n",
      "STATE_% Children in Poverty                            0.010596   \n",
      "STATE_Food Environment Index                           0.009961   \n",
      "STATE_Violent Crime Rate                               0.004931   \n",
      "STATE_Physically Unhealthy Days                        0.004743   \n",
      "econ_hires_per_1k                                      0.003611   \n",
      "STATE_% Smokers                                        0.003607   \n",
      "STATE_# Alcohol-Impaired Driving Deaths_per_100k       0.003240   \n",
      "\n",
      "                                                  mean_shap_signed  \n",
      "STATE_% Some College                                      0.013179  \n",
      "STATE_# Driving Deaths_per_100k                          -0.015713  \n",
      "STATE_% Fair or Poor Health                               0.001412  \n",
      "STATE_% Children in Poverty                               0.007642  \n",
      "STATE_Food Environment Index                              0.005784  \n",
      "STATE_Violent Crime Rate                                 -0.003284  \n",
      "STATE_Physically Unhealthy Days                           0.001437  \n",
      "econ_hires_per_1k                                        -0.001714  \n",
      "STATE_% Smokers                                          -0.000135  \n",
      "STATE_# Alcohol-Impaired Driving Deaths_per_100k         -0.001095  \n",
      "\n",
      "--- LGB global SHAP (TEST 2022-2024), top 10 ---\n",
      "                                                  mean_abs_shap  \\\n",
      "STATE_% Some College                                   0.029952   \n",
      "STATE_# Driving Deaths_per_100k                        0.027590   \n",
      "state_total_pop                                        0.022116   \n",
      "STATE_# Alcohol-Impaired Driving Deaths_per_100k       0.015569   \n",
      "econ_hires_per_1k                                      0.013481   \n",
      "STATE_Violent Crime Rate                               0.013004   \n",
      "STATE_% Fair or Poor Health                            0.011771   \n",
      "STATE_% Children in Poverty                            0.009525   \n",
      "STATE_Physically Unhealthy Days                        0.009356   \n",
      "STATE_Primary Care Physicians Rate                     0.008671   \n",
      "\n",
      "                                                  mean_shap_signed  \n",
      "STATE_% Some College                                      0.010773  \n",
      "STATE_# Driving Deaths_per_100k                          -0.003561  \n",
      "state_total_pop                                          -0.005324  \n",
      "STATE_# Alcohol-Impaired Driving Deaths_per_100k         -0.002715  \n",
      "econ_hires_per_1k                                        -0.000067  \n",
      "STATE_Violent Crime Rate                                 -0.007890  \n",
      "STATE_% Fair or Poor Health                               0.000492  \n",
      "STATE_% Children in Poverty                              -0.005775  \n",
      "STATE_Physically Unhealthy Days                           0.003066  \n",
      "STATE_Primary Care Physicians Rate                       -0.001091  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Global interpretability for the STACKED ensemble\n",
    "#  - Feature-space attributions (magnitude + direction)\n",
    "#  - Leakage-safe background: TRAIN only (2014-2021)\n",
    "#  - Explain set: TEST only (2022-2024) by default\n",
    "#\n",
    "# Requires objects from your modeling cell:\n",
    "#   df_model, X_all, y_all\n",
    "#   train_mask, test_mask\n",
    "#   base_fixed  ({\"lasso\":..., \"rf\":..., \"lgb\":...})  [cloneable pipelines]\n",
    "#   weights     (pd.Series with index [\"lasso\",\"rf\",\"lgb\"])\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- SHAP import ---\n",
    "try:\n",
    "    import shap\n",
    "except ImportError:\n",
    "    raise ImportError(\"Install SHAP first: pip install shap\")\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers: consistent transforms and SHAP per model\n",
    "# ----------------------------\n",
    "def _get_feature_names(X: pd.DataFrame):\n",
    "    return list(X.columns)\n",
    "\n",
    "def _impute_only(pipe, X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return imputed X as DataFrame.\"\"\"\n",
    "    imp = pipe.named_steps[\"imputer\"]\n",
    "    X_imp = pd.DataFrame(imp.transform(X), columns=_get_feature_names(X), index=X.index)\n",
    "    return X_imp\n",
    "\n",
    "def linear_shap_from_lasso_pipeline(lasso_pipe, X_background: pd.DataFrame, X_explain: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Exact additive decomposition for Pipeline(imputer, scaler, lasso) in the *original feature names*.\n",
    "    Works by computing contributions in standardized space:\n",
    "        phi_j = coef_j * (x_scaled_j - E[x_scaled_j])\n",
    "    Returns:\n",
    "        shap_vals: (n, p) array\n",
    "        base_value: float\n",
    "        X_explain_imp: DataFrame (imputed, original units)\n",
    "    \"\"\"\n",
    "    imp = lasso_pipe.named_steps[\"imputer\"]\n",
    "    sc  = lasso_pipe.named_steps[\"scaler\"]\n",
    "    lm  = lasso_pipe.named_steps[\"lasso\"]\n",
    "\n",
    "    # impute\n",
    "    Xb_imp = pd.DataFrame(imp.transform(X_background), columns=_get_feature_names(X_background), index=X_background.index)\n",
    "    Xe_imp = pd.DataFrame(imp.transform(X_explain),    columns=_get_feature_names(X_explain),    index=X_explain.index)\n",
    "\n",
    "    # scale\n",
    "    Xb_sc = sc.transform(Xb_imp)\n",
    "    Xe_sc = sc.transform(Xe_imp)\n",
    "\n",
    "    mean_sc = np.nanmean(Xb_sc, axis=0)  # background mean in standardized space\n",
    "    coefs = np.asarray(lm.coef_, dtype=float)\n",
    "\n",
    "    shap_vals = (Xe_sc - mean_sc) * coefs  # (n, p)\n",
    "    base_value = float(lm.intercept_ + np.dot(mean_sc, coefs))\n",
    "    return shap_vals, base_value, Xe_imp\n",
    "\n",
    "def tree_shap_from_imputed_pipeline(tree_pipe, model_step_name: str,\n",
    "                                   X_background: pd.DataFrame, X_explain: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    SHAP for Pipeline(imputer, <tree_model>) using TreeExplainer.\n",
    "    Returns:\n",
    "        shap_vals: (n, p) array\n",
    "        base_value: float\n",
    "        Xe_imp: DataFrame (imputed)\n",
    "    \"\"\"\n",
    "    imp = tree_pipe.named_steps[\"imputer\"]\n",
    "    model = tree_pipe.named_steps[model_step_name]\n",
    "\n",
    "    Xb_imp = pd.DataFrame(imp.transform(X_background), columns=_get_feature_names(X_background), index=X_background.index)\n",
    "    Xe_imp = pd.DataFrame(imp.transform(X_explain),    columns=_get_feature_names(X_explain),    index=X_explain.index)\n",
    "\n",
    "    explainer = shap.TreeExplainer(model, data=Xb_imp, feature_perturbation=\"interventional\")\n",
    "    shap_vals = explainer.shap_values(Xe_imp)\n",
    "    # shap_vals may be list for multioutput; but you're regression so ensure array\n",
    "    shap_vals = np.asarray(shap_vals)\n",
    "    base_value = float(np.asarray(explainer.expected_value).reshape(-1)[0])\n",
    "    return shap_vals, base_value, Xe_imp\n",
    "\n",
    "def combine_ensemble_shap(shap_by_model: dict, base_by_model: dict, meta_weights: pd.Series, meta_intercept: float = 0.0):\n",
    "    \"\"\"\n",
    "    Combine base-model SHAP values into stacked ensemble SHAP values:\n",
    "        f_ens(x) = meta_intercept + sum_m w_m f_m(x)\n",
    "        phi_ens  = sum_m w_m phi_m\n",
    "        base_ens = meta_intercept + sum_m w_m base_m\n",
    "    \"\"\"\n",
    "    # ensure aligned order and same feature dimension\n",
    "    keys = list(meta_weights.index)\n",
    "    first = keys[0]\n",
    "    n, p = shap_by_model[first].shape\n",
    "\n",
    "    shap_ens = np.zeros((n, p), dtype=float)\n",
    "    base_ens = float(meta_intercept)\n",
    "    for k in keys:\n",
    "        w = float(meta_weights[k])\n",
    "        shap_ens += w * np.asarray(shap_by_model[k], dtype=float)\n",
    "        base_ens += w * float(base_by_model[k])\n",
    "    return shap_ens, base_ens\n",
    "\n",
    "def global_shap_tables(shap_vals: np.ndarray, feature_names: list):\n",
    "    \"\"\"\n",
    "    Returns two pd.Series:\n",
    "      - mean_abs_shap: importance magnitude\n",
    "      - mean_shap: signed direction (average contribution)\n",
    "    \"\"\"\n",
    "    mean_abs = pd.Series(np.abs(shap_vals).mean(axis=0), index=feature_names).sort_values(ascending=False)\n",
    "    mean_signed = pd.Series(shap_vals.mean(axis=0), index=feature_names).loc[mean_abs.index]\n",
    "    return mean_abs, mean_signed\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Define background (TRAIN only) and explain set (TEST only by default)\n",
    "# ----------------------------\n",
    "X_train_bg = X_all.loc[train_mask].copy()   # background: 2014-2021\n",
    "X_explain  = X_all.loc[test_mask].copy()    # explain: 2022-2024 holdout\n",
    "\n",
    "feature_names = _get_feature_names(X_train_bg)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Fit base models on full TRAIN (no leakage into TEST)\n",
    "# ----------------------------\n",
    "base_fitted = {}\n",
    "for name, pipe in base_fixed.items():\n",
    "    base_fitted[name] = pipe.fit(X_train_bg, y_all[train_mask.values])\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Compute SHAP per base model in original feature space\n",
    "# ----------------------------\n",
    "shap_by_model = {}\n",
    "baseval_by_model = {}\n",
    "\n",
    "# LASSO (pipeline with scaler)\n",
    "sh_lasso, bv_lasso, Xe_imp_lasso = linear_shap_from_lasso_pipeline(\n",
    "    base_fitted[\"lasso\"], X_train_bg, X_explain\n",
    ")\n",
    "shap_by_model[\"lasso\"] = sh_lasso\n",
    "baseval_by_model[\"lasso\"] = bv_lasso\n",
    "\n",
    "# RF (tree)\n",
    "sh_rf, bv_rf, Xe_imp_rf = tree_shap_from_imputed_pipeline(\n",
    "    base_fitted[\"rf\"], model_step_name=\"rf\", X_background=X_train_bg, X_explain=X_explain\n",
    ")\n",
    "shap_by_model[\"rf\"] = sh_rf\n",
    "baseval_by_model[\"rf\"] = bv_rf\n",
    "\n",
    "# LGB (tree)\n",
    "sh_lgb, bv_lgb, Xe_imp_lgb = tree_shap_from_imputed_pipeline(\n",
    "    base_fitted[\"lgb\"], model_step_name=\"lgb\", X_background=X_train_bg, X_explain=X_explain\n",
    ")\n",
    "shap_by_model[\"lgb\"] = sh_lgb\n",
    "baseval_by_model[\"lgb\"] = bv_lgb\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Combine into stacked-ensemble SHAP using your stacking weights\n",
    "#    (Meta intercept is unknown here because you didnt keep the fitted meta model.\n",
    "#     Thats OK for importance ranking; SHAP magnitudes are unaffected.)\n",
    "# ----------------------------\n",
    "sh_ens, base_ens = combine_ensemble_shap(\n",
    "    shap_by_model=shap_by_model,\n",
    "    base_by_model=baseval_by_model,\n",
    "    meta_weights=weights,\n",
    "    meta_intercept=0.0\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Global tables: magnitude + direction (on TEST years 2022-2024)\n",
    "# ----------------------------\n",
    "ens_mean_abs, ens_mean_signed = global_shap_tables(sh_ens, feature_names)\n",
    "\n",
    "ens_global_table = pd.DataFrame({\n",
    "    \"mean_abs_shap_ens\": ens_mean_abs,\n",
    "    \"mean_shap_signed_ens\": ens_mean_signed\n",
    "})\n",
    "\n",
    "print(\"\\n=== STACKED ENSEMBLE GLOBAL (TEST 2022-2024) SHAP ===\")\n",
    "print(\"Stacking weights used:\\n\", weights.sort_values(ascending=False))\n",
    "print(\"\\nTop 20 features by ensemble mean(|SHAP|):\")\n",
    "print(ens_global_table.head(20))\n",
    "\n",
    "# Optional: base-model comparison tables\n",
    "for m in [\"lasso\", \"rf\", \"lgb\"]:\n",
    "    m_abs, m_signed = global_shap_tables(shap_by_model[m], feature_names)\n",
    "    tab = pd.DataFrame({\"mean_abs_shap\": m_abs, \"mean_shap_signed\": m_signed})\n",
    "    print(f\"\\n--- {m.upper()} global SHAP (TEST 2022-2024), top 10 ---\")\n",
    "    print(tab.head(10))\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Optional plots (comment out if running headless)\n",
    "# ----------------------------\n",
    "# Xe_imp_for_plot = Xe_imp_lgb  # any imputed X with same columns is fine for plotting\n",
    "# shap.summary_plot(sh_ens, Xe_imp_for_plot, show=False)            # direction + magnitude\n",
    "# shap.summary_plot(sh_ens, Xe_imp_for_plot, plot_type=\"bar\", show=False)  # magnitude only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9691b9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP year 2018] stacking fit failed: Too few OOF rows for stacking meta-learner: kept=49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liuc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\liuc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\liuc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\liuc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\liuc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\liuc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\liuc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\liuc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\liuc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\liuc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\liuc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\liuc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Rolling-origin ensemble SHAP stability (2018-2024) ===\n",
      "Years used: [2019, 2020, 2021, 2022, 2023, 2024] | count: 6\n",
      "\n",
      "Top 20 stable features (by top-10 frequency, then mean importance):\n",
      "                                                  mean_abs_shap_mean  \\\n",
      "feature                                                                \n",
      "STATE_% Some College                                        0.029003   \n",
      "STATE_# Driving Deaths_per_100k                             0.022715   \n",
      "econ_hires_per_1k                                           0.018165   \n",
      "STATE_Primary Care Physicians Rate                          0.010688   \n",
      "STATE_Food Environment Index                                0.009345   \n",
      "state_total_pop                                             0.007756   \n",
      "STATE_# Alcohol-Impaired Driving Deaths_per_100k            0.006187   \n",
      "STATE_% Fair or Poor Health                                 0.009330   \n",
      "STATE_Violent Crime Rate                                    0.009301   \n",
      "STATE_Physically Unhealthy Days                             0.006209   \n",
      "state_avg_earnings                                          0.005268   \n",
      "STATE_% Uninsured                                           0.005188   \n",
      "STATE_% Children in Poverty                                 0.005171   \n",
      "STATE_% Adults with Obesity                                 0.004677   \n",
      "STATE_Mentally Unhealthy Days                               0.003444   \n",
      "STATE_Teen Birth Rate                                       0.001982   \n",
      "state_hires_total                                           0.003816   \n",
      "STATE_% Smokers                                             0.003773   \n",
      "state_avg_earnings_meanq                                    0.003209   \n",
      "STATE_Income Ratio                                          0.002635   \n",
      "\n",
      "                                                  mean_abs_shap_std  \\\n",
      "feature                                                               \n",
      "STATE_% Some College                                       0.003615   \n",
      "STATE_# Driving Deaths_per_100k                            0.004845   \n",
      "econ_hires_per_1k                                          0.004497   \n",
      "STATE_Primary Care Physicians Rate                         0.004299   \n",
      "STATE_Food Environment Index                               0.002069   \n",
      "state_total_pop                                            0.001622   \n",
      "STATE_# Alcohol-Impaired Driving Deaths_per_100k           0.001802   \n",
      "STATE_% Fair or Poor Health                                0.005361   \n",
      "STATE_Violent Crime Rate                                   0.004986   \n",
      "STATE_Physically Unhealthy Days                            0.003650   \n",
      "state_avg_earnings                                         0.003036   \n",
      "STATE_% Uninsured                                          0.003061   \n",
      "STATE_% Children in Poverty                                0.002103   \n",
      "STATE_% Adults with Obesity                                0.002197   \n",
      "STATE_Mentally Unhealthy Days                              0.003461   \n",
      "STATE_Teen Birth Rate                                      0.001826   \n",
      "state_hires_total                                          0.001457   \n",
      "STATE_% Smokers                                            0.001194   \n",
      "state_avg_earnings_meanq                                   0.002052   \n",
      "STATE_Income Ratio                                         0.000465   \n",
      "\n",
      "                                                  mean_abs_shap_median  \\\n",
      "feature                                                                  \n",
      "STATE_% Some College                                          0.028961   \n",
      "STATE_# Driving Deaths_per_100k                               0.022873   \n",
      "econ_hires_per_1k                                             0.018040   \n",
      "STATE_Primary Care Physicians Rate                            0.008575   \n",
      "STATE_Food Environment Index                                  0.008167   \n",
      "state_total_pop                                               0.008156   \n",
      "STATE_# Alcohol-Impaired Driving Deaths_per_100k              0.006304   \n",
      "STATE_% Fair or Poor Health                                   0.007161   \n",
      "STATE_Violent Crime Rate                                      0.012037   \n",
      "STATE_Physically Unhealthy Days                               0.006630   \n",
      "state_avg_earnings                                            0.004757   \n",
      "STATE_% Uninsured                                             0.004013   \n",
      "STATE_% Children in Poverty                                   0.005596   \n",
      "STATE_% Adults with Obesity                                   0.004114   \n",
      "STATE_Mentally Unhealthy Days                                 0.002406   \n",
      "STATE_Teen Birth Rate                                         0.001260   \n",
      "state_hires_total                                             0.003252   \n",
      "STATE_% Smokers                                               0.003309   \n",
      "state_avg_earnings_meanq                                      0.003639   \n",
      "STATE_Income Ratio                                            0.002594   \n",
      "\n",
      "                                                  mean_signed_mean  \\\n",
      "feature                                                              \n",
      "STATE_% Some College                                      0.013098   \n",
      "STATE_# Driving Deaths_per_100k                          -0.001557   \n",
      "econ_hires_per_1k                                        -0.000546   \n",
      "STATE_Primary Care Physicians Rate                        0.000629   \n",
      "STATE_Food Environment Index                              0.004698   \n",
      "state_total_pop                                          -0.000597   \n",
      "STATE_# Alcohol-Impaired Driving Deaths_per_100k          0.000387   \n",
      "STATE_% Fair or Poor Health                              -0.001172   \n",
      "STATE_Violent Crime Rate                                 -0.001151   \n",
      "STATE_Physically Unhealthy Days                          -0.000416   \n",
      "state_avg_earnings                                        0.002645   \n",
      "STATE_% Uninsured                                         0.003612   \n",
      "STATE_% Children in Poverty                               0.001178   \n",
      "STATE_% Adults with Obesity                               0.001329   \n",
      "STATE_Mentally Unhealthy Days                            -0.003203   \n",
      "STATE_Teen Birth Rate                                    -0.001207   \n",
      "state_hires_total                                        -0.000244   \n",
      "STATE_% Smokers                                          -0.000529   \n",
      "state_avg_earnings_meanq                                  0.001481   \n",
      "STATE_Income Ratio                                        0.000095   \n",
      "\n",
      "                                                  mean_signed_std  mean_rank  \\\n",
      "feature                                                                        \n",
      "STATE_% Some College                                     0.002148   1.000000   \n",
      "STATE_# Driving Deaths_per_100k                          0.003630   2.500000   \n",
      "econ_hires_per_1k                                        0.012217   2.833333   \n",
      "STATE_Primary Care Physicians Rate                       0.001457   5.333333   \n",
      "STATE_Food Environment Index                             0.000877   6.333333   \n",
      "state_total_pop                                          0.001259   7.666667   \n",
      "STATE_# Alcohol-Impaired Driving Deaths_per_100k         0.001054  10.166667   \n",
      "STATE_% Fair or Poor Health                              0.008369   8.000000   \n",
      "STATE_Violent Crime Rate                                 0.001660   8.333333   \n",
      "STATE_Physically Unhealthy Days                          0.005295  12.333333   \n",
      "state_avg_earnings                                       0.002222  13.000000   \n",
      "STATE_% Uninsured                                        0.003346  11.666667   \n",
      "STATE_% Children in Poverty                              0.001565  12.833333   \n",
      "STATE_% Adults with Obesity                              0.001034  13.000000   \n",
      "STATE_Mentally Unhealthy Days                            0.003608  15.666667   \n",
      "STATE_Teen Birth Rate                                    0.001912  19.333333   \n",
      "state_hires_total                                        0.000421  15.500000   \n",
      "STATE_% Smokers                                          0.001924  15.500000   \n",
      "state_avg_earnings_meanq                                 0.001813  16.500000   \n",
      "STATE_Income Ratio                                       0.000262  17.666667   \n",
      "\n",
      "                                                  std_rank  n_years  \\\n",
      "feature                                                               \n",
      "STATE_% Some College                              0.000000        6   \n",
      "STATE_# Driving Deaths_per_100k                   0.836660        6   \n",
      "econ_hires_per_1k                                 0.752773        6   \n",
      "STATE_Primary Care Physicians Rate                2.065591        6   \n",
      "STATE_Food Environment Index                      0.516398        6   \n",
      "state_total_pop                                   1.505545        6   \n",
      "STATE_# Alcohol-Impaired Driving Deaths_per_100k  3.060501        6   \n",
      "STATE_% Fair or Poor Health                       3.224903        6   \n",
      "STATE_Violent Crime Rate                          6.088240        6   \n",
      "STATE_Physically Unhealthy Days                   5.391351        6   \n",
      "state_avg_earnings                                3.741657        6   \n",
      "STATE_% Uninsured                                 3.983298        6   \n",
      "STATE_% Children in Poverty                       2.714160        6   \n",
      "STATE_% Adults with Obesity                       3.741657        6   \n",
      "STATE_Mentally Unhealthy Days                     7.366591        6   \n",
      "STATE_Teen Birth Rate                             5.715476        6   \n",
      "state_hires_total                                 1.643168        6   \n",
      "STATE_% Smokers                                   2.073644        6   \n",
      "state_avg_earnings_meanq                          4.324350        6   \n",
      "STATE_Income Ratio                                1.505545        6   \n",
      "\n",
      "                                                  top10_freq  \n",
      "feature                                                       \n",
      "STATE_% Some College                                1.000000  \n",
      "STATE_# Driving Deaths_per_100k                     1.000000  \n",
      "econ_hires_per_1k                                   1.000000  \n",
      "STATE_Primary Care Physicians Rate                  1.000000  \n",
      "STATE_Food Environment Index                        1.000000  \n",
      "state_total_pop                                     1.000000  \n",
      "STATE_# Alcohol-Impaired Driving Deaths_per_100k    0.833333  \n",
      "STATE_% Fair or Poor Health                         0.666667  \n",
      "STATE_Violent Crime Rate                            0.666667  \n",
      "STATE_Physically Unhealthy Days                     0.500000  \n",
      "state_avg_earnings                                  0.333333  \n",
      "STATE_% Uninsured                                   0.333333  \n",
      "STATE_% Children in Poverty                         0.166667  \n",
      "STATE_% Adults with Obesity                         0.166667  \n",
      "STATE_Mentally Unhealthy Days                       0.166667  \n",
      "STATE_Teen Birth Rate                               0.166667  \n",
      "state_hires_total                                   0.000000  \n",
      "STATE_% Smokers                                     0.000000  \n",
      "state_avg_earnings_meanq                            0.000000  \n",
      "STATE_Income Ratio                                  0.000000  \n",
      "\n",
      "Per-year top features (top 8 by mean|SHAP|):\n",
      "\n",
      "Year 2019:\n",
      "                            feature  mean_abs_shap  mean_shap_signed\n",
      "              STATE_% Some College       0.025007          0.010523\n",
      "                 econ_hires_per_1k       0.022112          0.007026\n",
      "STATE_Primary Care Physicians Rate       0.017666          0.000539\n",
      "   STATE_# Driving Deaths_per_100k       0.015317         -0.002241\n",
      "          STATE_Violent Crime Rate       0.013320         -0.000655\n",
      "                 STATE_% Uninsured       0.011381          0.010366\n",
      "      STATE_Food Environment Index       0.011345          0.005695\n",
      "                state_avg_earnings       0.009729          0.001883\n",
      "\n",
      "Year 2020:\n",
      "                            feature  mean_abs_shap  mean_shap_signed\n",
      "              STATE_% Some College       0.025018          0.013293\n",
      "                 econ_hires_per_1k       0.023373         -0.015823\n",
      "   STATE_# Driving Deaths_per_100k       0.020747         -0.000060\n",
      "STATE_Primary Care Physicians Rate       0.014336          0.002529\n",
      "          STATE_Violent Crime Rate       0.012622          0.000862\n",
      "      STATE_Food Environment Index       0.012578          0.004916\n",
      "       STATE_% Fair or Poor Health       0.009240         -0.003976\n",
      "                   state_total_pop       0.008919         -0.001114\n",
      "\n",
      "Year 2021:\n",
      "                            feature  mean_abs_shap  mean_shap_signed\n",
      "              STATE_% Some College       0.029029          0.015901\n",
      "   STATE_# Driving Deaths_per_100k       0.020354          0.003054\n",
      "                 econ_hires_per_1k       0.015358          0.011574\n",
      "          STATE_Violent Crime Rate       0.011903          0.000548\n",
      "STATE_Primary Care Physicians Rate       0.008751          0.002022\n",
      "      STATE_Food Environment Index       0.008271          0.005512\n",
      "                   state_total_pop       0.007394          0.001562\n",
      "       STATE_% Adults with Obesity       0.007286          0.001273\n",
      "\n",
      "Year 2022:\n",
      "                                          feature  mean_abs_shap  mean_shap_signed\n",
      "                            STATE_% Some College       0.033725          0.014180\n",
      "                 STATE_# Driving Deaths_per_100k       0.024999         -0.007965\n",
      "                               econ_hires_per_1k       0.015461          0.010349\n",
      "                        STATE_Violent Crime Rate       0.012171         -0.001876\n",
      "                                 state_total_pop       0.008971         -0.002122\n",
      "                    STATE_Food Environment Index       0.008035          0.004598\n",
      "STATE_# Alcohol-Impaired Driving Deaths_per_100k       0.008023         -0.001700\n",
      "                     STATE_% Fair or Poor Health       0.007594         -0.007218\n",
      "\n",
      "Year 2023:\n",
      "                            feature  mean_abs_shap  mean_shap_signed\n",
      "              STATE_% Some College       0.028894          0.010578\n",
      "   STATE_# Driving Deaths_per_100k       0.026140         -0.001507\n",
      "       STATE_% Fair or Poor Health       0.020021          0.015525\n",
      "                 econ_hires_per_1k       0.012065         -0.002172\n",
      "   STATE_Physically Unhealthy Days       0.011540          0.009991\n",
      "STATE_Primary Care Physicians Rate       0.008398         -0.001476\n",
      "      STATE_Food Environment Index       0.007781          0.003364\n",
      "                   state_total_pop       0.006278         -0.001259\n",
      "\n",
      "Year 2024:\n",
      "                            feature  mean_abs_shap  mean_shap_signed\n",
      "              STATE_% Some College       0.032346          0.014111\n",
      "   STATE_# Driving Deaths_per_100k       0.028737         -0.000626\n",
      "                 econ_hires_per_1k       0.020619         -0.014231\n",
      "     STATE_Mentally Unhealthy Days       0.010140         -0.010140\n",
      "STATE_Primary Care Physicians Rate       0.008173         -0.000015\n",
      "      STATE_Food Environment Index       0.008063          0.004103\n",
      "       STATE_% Fair or Poor Health       0.006233         -0.002056\n",
      "             STATE_Teen Birth Rate       0.005593         -0.005041\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Stability of GLOBAL ensemble ranking (rolling-origin SHAP)\n",
    "#  - For each year t:\n",
    "#      train = years <= t-1\n",
    "#      test  = year == t\n",
    "#      fit base models on train\n",
    "#      fit ridge stacker using walk-forward OOF preds in train (leakage-safe)\n",
    "#      compute ensemble SHAP importance on test year t\n",
    "#  - Summarize stability across years: top-k freq, mean rank, meanstd importance\n",
    "#\n",
    "# Uses your existing helper functions:\n",
    "#   make_walk_forward_folds, oof_preds_walk_forward\n",
    "# and uses the same base_fixed pipelines (already tuned) for consistency.\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure shap imported\n",
    "try:\n",
    "    import shap\n",
    "except ImportError:\n",
    "    raise ImportError(\"Install SHAP first: pip install shap\")\n",
    "\n",
    "def fit_meta_ridge_from_oof(base_models_fixed: dict, X_tr: pd.DataFrame, y_tr: np.ndarray,\n",
    "                           years_tr: np.ndarray, min_train_years_oof: int = 3, meta_alphas=None):\n",
    "    \"\"\"\n",
    "    Fit ridge stacker using leakage-safe walk-forward OOF predictions within TRAIN.\n",
    "    Returns:\n",
    "        meta_model (fitted RidgeCV),\n",
    "        meta_weights (pd.Series),\n",
    "        meta_intercept (float),\n",
    "        base_fitted_full_train (dict of fitted base pipelines)\n",
    "    \"\"\"\n",
    "    if meta_alphas is None:\n",
    "        meta_alphas = np.logspace(-3, 3, 19)\n",
    "\n",
    "    folds_oof = make_walk_forward_folds(years_tr, min_train_years=min_train_years_oof)\n",
    "\n",
    "    # OOF meta-features\n",
    "    meta_train = np.column_stack([\n",
    "        oof_preds_walk_forward(m, X_tr, y_tr, folds_oof)\n",
    "        for m in base_models_fixed.values()\n",
    "    ])\n",
    "    valid = np.all(np.isfinite(meta_train), axis=1)\n",
    "    if valid.sum() < 60:\n",
    "        raise RuntimeError(f\"Too few OOF rows for stacking meta-learner: kept={int(valid.sum())}\")\n",
    "\n",
    "    # Fit base models on full train for later prediction + SHAP\n",
    "    base_full = {k: m.fit(X_tr, y_tr) for k, m in base_models_fixed.items()}\n",
    "\n",
    "    # Fit meta model (ridge)\n",
    "    from sklearn.linear_model import RidgeCV\n",
    "    meta = RidgeCV(alphas=meta_alphas, cv=5)\n",
    "    meta.fit(meta_train[valid], y_tr[valid])\n",
    "\n",
    "    w = pd.Series(meta.coef_, index=list(base_models_fixed.keys()))\n",
    "    b0 = float(meta.intercept_)\n",
    "    return meta, w, b0, base_full\n",
    "\n",
    "def shap_for_base_models(base_full: dict, X_bg: pd.DataFrame, X_te: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compute SHAP values for each base model on X_te with background X_bg (both are DataFrames).\n",
    "    Returns:\n",
    "      shap_by_model: dict name -> (n_test, p) array\n",
    "      base_by_model: dict name -> base_value float\n",
    "      X_te_imp_any:  imputed DataFrame with feature names (usable for plots)\n",
    "    \"\"\"\n",
    "    shap_by_model = {}\n",
    "    base_by_model = {}\n",
    "    X_te_imp_any = None\n",
    "\n",
    "    # LASSO\n",
    "    sh_lasso, bv_lasso, X_te_imp_lasso = linear_shap_from_lasso_pipeline(\n",
    "        base_full[\"lasso\"], X_bg, X_te\n",
    "    )\n",
    "    shap_by_model[\"lasso\"] = sh_lasso\n",
    "    base_by_model[\"lasso\"] = bv_lasso\n",
    "    X_te_imp_any = X_te_imp_lasso\n",
    "\n",
    "    # RF\n",
    "    sh_rf, bv_rf, X_te_imp_rf = tree_shap_from_imputed_pipeline(\n",
    "        base_full[\"rf\"], \"rf\", X_bg, X_te\n",
    "    )\n",
    "    shap_by_model[\"rf\"] = sh_rf\n",
    "    base_by_model[\"rf\"] = bv_rf\n",
    "    X_te_imp_any = X_te_imp_any if X_te_imp_any is not None else X_te_imp_rf\n",
    "\n",
    "    # LGB\n",
    "    sh_lgb, bv_lgb, X_te_imp_lgb = tree_shap_from_imputed_pipeline(\n",
    "        base_full[\"lgb\"], \"lgb\", X_bg, X_te\n",
    "    )\n",
    "    shap_by_model[\"lgb\"] = sh_lgb\n",
    "    base_by_model[\"lgb\"] = bv_lgb\n",
    "    X_te_imp_any = X_te_imp_any if X_te_imp_any is not None else X_te_imp_lgb\n",
    "\n",
    "    return shap_by_model, base_by_model, X_te_imp_any\n",
    "\n",
    "def rank_series_desc(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"1 = most important. Ties get average rank.\"\"\"\n",
    "    return s.rank(ascending=False, method=\"average\")\n",
    "\n",
    "# ----------------------------\n",
    "# Rolling-origin SHAP stability\n",
    "# ----------------------------\n",
    "years_all = df_model[\"Year\"].astype(int).values\n",
    "feature_names = list(X_all.columns)\n",
    "\n",
    "start_year = 2018\n",
    "end_year = 2024\n",
    "top_k = 10\n",
    "\n",
    "per_year_tables = []\n",
    "per_year_rank = []\n",
    "\n",
    "for t in range(start_year, end_year + 1):\n",
    "    tr_mask = years_all <= (t - 1)\n",
    "    te_mask = years_all == t\n",
    "\n",
    "    if tr_mask.sum() < 120 or te_mask.sum() < 30:\n",
    "        continue\n",
    "\n",
    "    X_tr = X_all.loc[tr_mask].copy()\n",
    "    y_tr = y_all[tr_mask]\n",
    "    years_tr = years_all[tr_mask]\n",
    "\n",
    "    X_te = X_all.loc[te_mask].copy()\n",
    "\n",
    "    try:\n",
    "        meta, w_t, b0_t, base_full = fit_meta_ridge_from_oof(\n",
    "            base_models_fixed=base_fixed,\n",
    "            X_tr=X_tr,\n",
    "            y_tr=y_tr,\n",
    "            years_tr=years_tr,\n",
    "            min_train_years_oof=3\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP year {t}] stacking fit failed: {e}\")\n",
    "        continue\n",
    "\n",
    "    # SHAP per base model, background = X_tr only (no leakage)\n",
    "    shap_by_model, base_by_model, X_te_imp_any = shap_for_base_models(base_full, X_tr, X_te)\n",
    "\n",
    "    # Combine to ensemble SHAP for year t\n",
    "    sh_ens_t, base_ens_t = combine_ensemble_shap(shap_by_model, base_by_model, w_t, meta_intercept=b0_t)\n",
    "\n",
    "    # Compute year-t global importance (mean abs) + direction (mean signed) on test year t\n",
    "    mean_abs_t = pd.Series(np.abs(sh_ens_t).mean(axis=0), index=feature_names).sort_values(ascending=False)\n",
    "    mean_signed_t = pd.Series(sh_ens_t.mean(axis=0), index=feature_names).loc[mean_abs_t.index]\n",
    "\n",
    "    tab_t = pd.DataFrame({\n",
    "        \"test_year\": t,\n",
    "        \"feature\": mean_abs_t.index,\n",
    "        \"mean_abs_shap\": mean_abs_t.values,\n",
    "        \"mean_shap_signed\": mean_signed_t.values\n",
    "    })\n",
    "    per_year_tables.append(tab_t)\n",
    "\n",
    "    # ranking for stability\n",
    "    ranks_t = rank_series_desc(mean_abs_t)\n",
    "    per_year_rank.append(pd.DataFrame({\"test_year\": t, \"feature\": ranks_t.index, \"rank\": ranks_t.values}))\n",
    "\n",
    "# Combine all year tables\n",
    "if not per_year_tables:\n",
    "    raise RuntimeError(\"No rolling-origin SHAP tables were produced. Check sample sizes / years / folds.\")\n",
    "\n",
    "imp_long = pd.concat(per_year_tables, ignore_index=True)\n",
    "rank_long = pd.concat(per_year_rank, ignore_index=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Stability summary across years\n",
    "# ----------------------------\n",
    "years_used = sorted(imp_long[\"test_year\"].unique())\n",
    "n_years_used = len(years_used)\n",
    "\n",
    "# Top-k frequency\n",
    "topk_flags = (\n",
    "    rank_long.assign(in_topk=lambda d: d[\"rank\"] <= top_k)\n",
    "             .groupby(\"feature\")[\"in_topk\"].mean()\n",
    "             .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# Mean rank + variability\n",
    "rank_stats = (rank_long.groupby(\"feature\")[\"rank\"]\n",
    "                      .agg(mean_rank=\"mean\", std_rank=\"std\", n_years=\"count\"))\n",
    "\n",
    "# Mean(|SHAP|) summary\n",
    "imp_stats = (imp_long.groupby(\"feature\")[\"mean_abs_shap\"]\n",
    "                    .agg(mean_abs_shap_mean=\"mean\",\n",
    "                         mean_abs_shap_std=\"std\",\n",
    "                         mean_abs_shap_median=\"median\"))\n",
    "\n",
    "# Signed direction summary\n",
    "dir_stats = (imp_long.groupby(\"feature\")[\"mean_shap_signed\"]\n",
    "                    .agg(mean_signed_mean=\"mean\", mean_signed_std=\"std\"))\n",
    "\n",
    "stability = (imp_stats.join(dir_stats, how=\"left\")\n",
    "                     .join(rank_stats, how=\"left\")\n",
    "                     .join(topk_flags.rename(f\"top{top_k}_freq\"), how=\"left\")\n",
    "                     .sort_values([\"top10_freq\" if top_k == 10 else f\"top{top_k}_freq\",\n",
    "                                   \"mean_abs_shap_mean\"], ascending=[False, False]))\n",
    "\n",
    "print(f\"\\n=== Rolling-origin ensemble SHAP stability ({start_year}-{end_year}) ===\")\n",
    "print(\"Years used:\", years_used, \"| count:\", n_years_used)\n",
    "print(f\"\\nTop 20 stable features (by top-{top_k} frequency, then mean importance):\")\n",
    "print(stability.head(20))\n",
    "\n",
    "# Optional: show per-year top-k lists (quick sanity)\n",
    "print(\"\\nPer-year top features (top 8 by mean|SHAP|):\")\n",
    "for y in years_used:\n",
    "    top8 = (imp_long[imp_long[\"test_year\"] == y]\n",
    "            .sort_values(\"mean_abs_shap\", ascending=False)\n",
    "            .head(8)[[\"feature\", \"mean_abs_shap\", \"mean_shap_signed\"]])\n",
    "    print(f\"\\nYear {y}:\\n\", top8.to_string(index=False))\n",
    "\n",
    "# Optional: export for paper tables\n",
    "# stability.to_csv(\"ensemble_shap_stability_summary.csv\", index=True)\n",
    "# imp_long.to_csv(\"ensemble_shap_importance_by_year.csv\", index=False)\n",
    "# rank_long.to_csv(\"ensemble_shap_rank_by_year.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c0bd94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual Aggregated DataFrame Shape: (539, 40)\n",
      "Sample Columns: ['State', 'Year', 'state_total_pop', 'state_emp_avg', 'state_emp_sum', 'state_hires_total', 'state_avg_earnings', 'state_avg_earnings_meanq', 'econ_emp_per_1k', 'econ_hires_per_1k', 'econ_hire_rate_annual', 'growth_emp_yoy']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SETUP: Define your column lists carefully\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1) RAW COUNTS -> SUM across counties (after de-dup at county-year)\n",
    "health_count_cols = [\n",
    "    '# Alcohol-Impaired Driving Deaths',\n",
    "    '# Driving Deaths',\n",
    "    'Premature Deaths'   # confirm this is a count (not a rate)\n",
    "]\n",
    "\n",
    "# 2) RATES / INDICES / PERCENTS -> POP-WEIGHTED average across counties\n",
    "health_rate_cols = [\n",
    "    '% Adults with Obesity',\n",
    "    '% Children in Poverty',\n",
    "    '% Children in Single-Parent Households',\n",
    "    '% Drive Alone to Work',\n",
    "    '% Excessive Drinking',\n",
    "    '% Fair or Poor Health',\n",
    "    '% Long Commute - Drives Alone',\n",
    "    '% Low Birthweight',\n",
    "    '% Severe Housing Problems',\n",
    "    '% Smokers',\n",
    "    '% Some College',\n",
    "    '% Unemployed',\n",
    "    '% Uninsured',\n",
    "    '% With Access to Exercise Opportunities',\n",
    "    'Food Environment Index',\n",
    "    'Income Ratio',\n",
    "    'Mentally Unhealthy Days',\n",
    "    'Physically Unhealthy Days',\n",
    "    'Preventable Hospitalization Rate',\n",
    "    'Primary Care Physicians Rate',\n",
    "    'Social Association Rate',\n",
    "    'Teen Birth Rate',\n",
    "    'Violent Crime Rate'\n",
    "]\n",
    "\n",
    "# 3) Numeric columns to coerce\n",
    "numeric_cols = [\n",
    "    'Year', 'quarter', 'FIPS', 'Population',\n",
    "    'Employment_Count', 'New_Hires', 'Earnings_Beginning_Qtr'\n",
    "] + health_count_cols + health_rate_cols\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 0: PRE-PROCESSING\n",
    "# ---------------------------------------------------------\n",
    "df = merged_full.copy()\n",
    "\n",
    "# Coerce numeric types\n",
    "for c in numeric_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "# Filter Year Window\n",
    "df = df[(df['Year'] >= 2014) & (df['Year'] <= 2024)].copy()\n",
    "\n",
    "# Quarter normalization (in case your data uses 0..3)\n",
    "if 'quarter' in df.columns:\n",
    "    qmin, qmax = df['quarter'].min(), df['quarter'].max()\n",
    "    if pd.notna(qmin) and pd.notna(qmax) and qmin >= 0 and qmax <= 3:\n",
    "        df['quarter'] = df['quarter'] + 1\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 1: HEALTH DATA AGGREGATION (County-Year -> State-Year)\n",
    "#   IMPORTANT: do NOT \"sum\" health across duplicate industry/quarter rows.\n",
    "#   First, collapse to unique county-year with FIRST (or median).\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "health_keep = ['State', 'Year', 'FIPS', 'Population'] + health_count_cols + health_rate_cols\n",
    "county_health_raw = df[health_keep].copy()\n",
    "\n",
    "# Robust de-dup: collapse to one row per county-year\n",
    "# Use 'first' since values should be identical across duplicated rows.\n",
    "county_health = (\n",
    "    county_health_raw\n",
    "    .sort_values(['State', 'FIPS', 'Year'])\n",
    "    .groupby(['State', 'Year', 'FIPS'], as_index=False)\n",
    "    .agg({**{'Population': 'first'},\n",
    "          **{c: 'first' for c in health_count_cols},\n",
    "          **{c: 'first' for c in health_rate_cols}})\n",
    ")\n",
    "\n",
    "# Create numerators for weighted averages\n",
    "for col in health_rate_cols:\n",
    "    county_health[f'{col}_num'] = county_health[col] * county_health['Population']\n",
    "\n",
    "# Aggregate to state-year\n",
    "agg_dict = {\n",
    "    'state_total_pop': ('Population', 'sum'),\n",
    "}\n",
    "\n",
    "for col in health_count_cols:\n",
    "    agg_dict[f'STATE_{col}'] = (col, 'sum')\n",
    "\n",
    "for col in health_rate_cols:\n",
    "    agg_dict[f'{col}_num_sum'] = (f'{col}_num', 'sum')\n",
    "\n",
    "state_health = county_health.groupby(['State', 'Year'], as_index=False).agg(**agg_dict)\n",
    "\n",
    "# Final pop-weighted rates\n",
    "for col in health_rate_cols:\n",
    "    state_health[f'STATE_{col}'] = state_health[f'{col}_num_sum'] / state_health['state_total_pop']\n",
    "    state_health.drop(columns=[f'{col}_num_sum'], inplace=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 2: ECON DATA AGGREGATION (County-Quarter-Industry -> State-Year)\n",
    "#   Key choices for annual:\n",
    "#   - hires: SUM over quarters (flow)\n",
    "#   - employment: MEAN over quarters (level/stock)\n",
    "#   - earnings: employment-weighted across all quarters (person-quarter weighted)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Exclude industry '00' to avoid double counting totals\n",
    "df_econ = df[df['industry'] != '00'].copy()\n",
    "\n",
    "# Wage bill for correct aggregation of average earnings\n",
    "df_econ['wage_bill'] = df_econ['Earnings_Beginning_Qtr'] * df_econ['Employment_Count']\n",
    "\n",
    "# 2A) First build State-Quarter totals (summing across counties + industries)\n",
    "state_qtr = (\n",
    "    df_econ.groupby(['State', 'Year', 'quarter'], as_index=False)\n",
    "    .agg(\n",
    "        state_emp_qtr=('Employment_Count', 'sum'),\n",
    "        state_hires_qtr=('New_Hires', 'sum'),\n",
    "        state_wage_bill_qtr=('wage_bill', 'sum')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Quarter-level avg earnings (optional)\n",
    "state_qtr['state_avg_earnings_qtr'] = state_qtr['state_wage_bill_qtr'] / state_qtr['state_emp_qtr'].replace({0: np.nan})\n",
    "\n",
    "# 2B) Now aggregate State-Quarter -> State-Year\n",
    "state_annual = (\n",
    "    state_qtr.groupby(['State', 'Year'], as_index=False)\n",
    "    .agg(\n",
    "        # employment level: average across quarters\n",
    "        state_emp_avg=('state_emp_qtr', 'mean'),\n",
    "        # also keep sum across quarters (person-quarters), useful for weighting\n",
    "        state_emp_sum=('state_emp_qtr', 'sum'),\n",
    "\n",
    "        # hires: flow total in the year\n",
    "        state_hires_total=('state_hires_qtr', 'sum'),\n",
    "\n",
    "        # wage bill total in the year (sum of person-quarter wage bills)\n",
    "        state_wage_bill_total=('state_wage_bill_qtr', 'sum'),\n",
    "\n",
    "        # optional: average quarterly earnings level (simple average of quarter avg)\n",
    "        state_avg_earnings_meanq=('state_avg_earnings_qtr', 'mean'),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Annual employment-weighted earnings across quarters (recommended)\n",
    "# Interpretation: average earnings per worker-quarter within the year\n",
    "state_annual['state_avg_earnings'] = state_annual['state_wage_bill_total'] / state_annual['state_emp_sum'].replace({0: np.nan})\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 3: MERGE Annual Econ + Annual Health\n",
    "# ---------------------------------------------------------\n",
    "state_df = state_annual.merge(state_health, on=['State', 'Year'], how='left')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 4: ANNUAL FEATURE ENGINEERING\n",
    "# ---------------------------------------------------------\n",
    "pop = state_df['state_total_pop'].replace({0: np.nan})\n",
    "emp_avg = state_df['state_emp_avg'].replace({0: np.nan})\n",
    "\n",
    "# Per-capita / rates\n",
    "state_df['econ_emp_per_1k'] = 1000 * state_df['state_emp_avg'] / pop\n",
    "state_df['econ_hires_per_1k'] = 1000 * state_df['state_hires_total'] / pop\n",
    "\n",
    "# Annual hires per worker-year (uses avg employment level)\n",
    "state_df['econ_hire_rate_annual'] = state_df['state_hires_total'] / emp_avg\n",
    "\n",
    "# Optional: YoY growth on annual series (lag 1 year)\n",
    "state_df = state_df.sort_values(['State', 'Year']).copy()\n",
    "\n",
    "def growth_yoy(x):\n",
    "    prev = x.shift(1)\n",
    "    return (x - prev) / prev.replace({0: np.nan})\n",
    "\n",
    "state_df['growth_emp_yoy'] = state_df.groupby('State')['state_emp_avg'].transform(growth_yoy)\n",
    "state_df['growth_earn_yoy'] = state_df.groupby('State')['state_avg_earnings'].transform(growth_yoy)\n",
    "state_df['growth_hires_yoy'] = state_df.groupby('State')['state_hires_total'].transform(growth_yoy)\n",
    "\n",
    "# Cleanup\n",
    "state_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 5: FINAL OUTPUT\n",
    "# ---------------------------------------------------------\n",
    "base_cols = [\n",
    "    'State', 'Year',\n",
    "    'state_total_pop',\n",
    "    # Econ (annual)\n",
    "    'state_emp_avg', 'state_emp_sum', 'state_hires_total',\n",
    "    'state_avg_earnings', 'state_avg_earnings_meanq',\n",
    "    'econ_emp_per_1k', 'econ_hires_per_1k', 'econ_hire_rate_annual',\n",
    "    'growth_emp_yoy', 'growth_earn_yoy', 'growth_hires_yoy'\n",
    "]\n",
    "\n",
    "health_cols_out = [c for c in state_df.columns if c.startswith('STATE_')]\n",
    "\n",
    "final_cols = base_cols + health_cols_out\n",
    "state_df_annual_final = state_df[final_cols].copy()\n",
    "\n",
    "print(f\"Annual Aggregated DataFrame Shape: {state_df_annual_final.shape}\")\n",
    "print(\"Sample Columns:\", state_df_annual_final.columns.tolist()[:12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bdb812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc480cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Year</th>\n",
       "      <th>state_total_pop</th>\n",
       "      <th>state_emp_avg</th>\n",
       "      <th>state_emp_sum</th>\n",
       "      <th>state_hires_total</th>\n",
       "      <th>state_avg_earnings</th>\n",
       "      <th>state_avg_earnings_meanq</th>\n",
       "      <th>econ_emp_per_1k</th>\n",
       "      <th>econ_hires_per_1k</th>\n",
       "      <th>...</th>\n",
       "      <th>STATE_% With Access to Exercise Opportunities</th>\n",
       "      <th>STATE_Food Environment Index</th>\n",
       "      <th>STATE_Income Ratio</th>\n",
       "      <th>STATE_Mentally Unhealthy Days</th>\n",
       "      <th>STATE_Physically Unhealthy Days</th>\n",
       "      <th>STATE_Preventable Hospitalization Rate</th>\n",
       "      <th>STATE_Primary Care Physicians Rate</th>\n",
       "      <th>STATE_Social Association Rate</th>\n",
       "      <th>STATE_Teen Birth Rate</th>\n",
       "      <th>STATE_Violent Crime Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2014</td>\n",
       "      <td>4843737.0</td>\n",
       "      <td>1498318.50</td>\n",
       "      <td>5993274.0</td>\n",
       "      <td>1096661.0</td>\n",
       "      <td>3228.483924</td>\n",
       "      <td>3228.168019</td>\n",
       "      <td>309.331101</td>\n",
       "      <td>226.408040</td>\n",
       "      <td>...</td>\n",
       "      <td>51.935239</td>\n",
       "      <td>6.932836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.254047</td>\n",
       "      <td>4.289236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.132991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.186843</td>\n",
       "      <td>410.745302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2015</td>\n",
       "      <td>4854803.0</td>\n",
       "      <td>1525839.00</td>\n",
       "      <td>6103356.0</td>\n",
       "      <td>1161013.0</td>\n",
       "      <td>3296.603814</td>\n",
       "      <td>3296.053334</td>\n",
       "      <td>314.294730</td>\n",
       "      <td>239.147294</td>\n",
       "      <td>...</td>\n",
       "      <td>64.355654</td>\n",
       "      <td>6.694086</td>\n",
       "      <td>5.030240</td>\n",
       "      <td>4.252503</td>\n",
       "      <td>4.285823</td>\n",
       "      <td>69.088952</td>\n",
       "      <td>62.883135</td>\n",
       "      <td>12.483551</td>\n",
       "      <td>46.968524</td>\n",
       "      <td>410.093125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2016</td>\n",
       "      <td>4866824.0</td>\n",
       "      <td>1550592.00</td>\n",
       "      <td>6202368.0</td>\n",
       "      <td>1216125.0</td>\n",
       "      <td>3333.740663</td>\n",
       "      <td>3333.279657</td>\n",
       "      <td>318.604494</td>\n",
       "      <td>249.880620</td>\n",
       "      <td>...</td>\n",
       "      <td>63.141529</td>\n",
       "      <td>6.630406</td>\n",
       "      <td>5.064997</td>\n",
       "      <td>4.613115</td>\n",
       "      <td>4.642889</td>\n",
       "      <td>63.233968</td>\n",
       "      <td>63.920448</td>\n",
       "      <td>12.495976</td>\n",
       "      <td>44.568666</td>\n",
       "      <td>409.633102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2017</td>\n",
       "      <td>4877989.0</td>\n",
       "      <td>1568279.25</td>\n",
       "      <td>6273117.0</td>\n",
       "      <td>1256587.0</td>\n",
       "      <td>3396.834061</td>\n",
       "      <td>3396.809367</td>\n",
       "      <td>321.501186</td>\n",
       "      <td>257.603492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.505342</td>\n",
       "      <td>5.082714</td>\n",
       "      <td>4.346475</td>\n",
       "      <td>4.317696</td>\n",
       "      <td>59.030987</td>\n",
       "      <td>64.602109</td>\n",
       "      <td>12.411106</td>\n",
       "      <td>41.759701</td>\n",
       "      <td>430.881423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2018</td>\n",
       "      <td>4891628.0</td>\n",
       "      <td>1591449.75</td>\n",
       "      <td>6365799.0</td>\n",
       "      <td>1343611.0</td>\n",
       "      <td>3499.525725</td>\n",
       "      <td>3499.332000</td>\n",
       "      <td>325.341533</td>\n",
       "      <td>274.675629</td>\n",
       "      <td>...</td>\n",
       "      <td>63.172629</td>\n",
       "      <td>6.945420</td>\n",
       "      <td>5.107864</td>\n",
       "      <td>4.469942</td>\n",
       "      <td>4.378239</td>\n",
       "      <td>59.751012</td>\n",
       "      <td>65.327281</td>\n",
       "      <td>12.249050</td>\n",
       "      <td>35.987034</td>\n",
       "      <td>430.315343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020</td>\n",
       "      <td>570864.0</td>\n",
       "      <td>197677.50</td>\n",
       "      <td>790710.0</td>\n",
       "      <td>184565.0</td>\n",
       "      <td>3836.381008</td>\n",
       "      <td>3834.619197</td>\n",
       "      <td>346.277747</td>\n",
       "      <td>323.308178</td>\n",
       "      <td>...</td>\n",
       "      <td>76.782077</td>\n",
       "      <td>7.687710</td>\n",
       "      <td>4.166759</td>\n",
       "      <td>3.515226</td>\n",
       "      <td>3.366822</td>\n",
       "      <td>3369.450393</td>\n",
       "      <td>72.041549</td>\n",
       "      <td>11.824352</td>\n",
       "      <td>28.210169</td>\n",
       "      <td>187.296452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2021</td>\n",
       "      <td>572889.0</td>\n",
       "      <td>201160.75</td>\n",
       "      <td>804643.0</td>\n",
       "      <td>201699.0</td>\n",
       "      <td>3986.156927</td>\n",
       "      <td>3981.600173</td>\n",
       "      <td>351.133902</td>\n",
       "      <td>352.073438</td>\n",
       "      <td>...</td>\n",
       "      <td>76.801626</td>\n",
       "      <td>7.649759</td>\n",
       "      <td>4.126810</td>\n",
       "      <td>3.959356</td>\n",
       "      <td>3.694561</td>\n",
       "      <td>3378.711441</td>\n",
       "      <td>68.743140</td>\n",
       "      <td>12.110035</td>\n",
       "      <td>25.969930</td>\n",
       "      <td>186.751076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2022</td>\n",
       "      <td>575106.0</td>\n",
       "      <td>207544.50</td>\n",
       "      <td>830178.0</td>\n",
       "      <td>206479.0</td>\n",
       "      <td>4295.698368</td>\n",
       "      <td>4292.581068</td>\n",
       "      <td>360.880429</td>\n",
       "      <td>359.027727</td>\n",
       "      <td>...</td>\n",
       "      <td>73.810856</td>\n",
       "      <td>7.765695</td>\n",
       "      <td>4.213440</td>\n",
       "      <td>4.001026</td>\n",
       "      <td>3.506198</td>\n",
       "      <td>3096.947537</td>\n",
       "      <td>72.137204</td>\n",
       "      <td>12.021146</td>\n",
       "      <td>24.283107</td>\n",
       "      <td>186.710976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2023</td>\n",
       "      <td>578239.0</td>\n",
       "      <td>212949.25</td>\n",
       "      <td>851797.0</td>\n",
       "      <td>198793.0</td>\n",
       "      <td>4510.154996</td>\n",
       "      <td>4510.339141</td>\n",
       "      <td>368.272029</td>\n",
       "      <td>343.790370</td>\n",
       "      <td>...</td>\n",
       "      <td>77.638870</td>\n",
       "      <td>7.779140</td>\n",
       "      <td>4.235807</td>\n",
       "      <td>4.019662</td>\n",
       "      <td>2.849604</td>\n",
       "      <td>2324.557970</td>\n",
       "      <td>70.486996</td>\n",
       "      <td>11.973542</td>\n",
       "      <td>24.269712</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2024</td>\n",
       "      <td>580752.0</td>\n",
       "      <td>214311.50</td>\n",
       "      <td>857246.0</td>\n",
       "      <td>189504.0</td>\n",
       "      <td>4666.200252</td>\n",
       "      <td>4665.226123</td>\n",
       "      <td>369.024127</td>\n",
       "      <td>326.307959</td>\n",
       "      <td>...</td>\n",
       "      <td>77.761949</td>\n",
       "      <td>7.781328</td>\n",
       "      <td>4.259097</td>\n",
       "      <td>4.750646</td>\n",
       "      <td>3.437344</td>\n",
       "      <td>2178.144266</td>\n",
       "      <td>70.319097</td>\n",
       "      <td>12.253024</td>\n",
       "      <td>20.177949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>539 rows  40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       State  Year  state_total_pop  state_emp_avg  state_emp_sum  \\\n",
       "0    Alabama  2014        4843737.0     1498318.50      5993274.0   \n",
       "1    Alabama  2015        4854803.0     1525839.00      6103356.0   \n",
       "2    Alabama  2016        4866824.0     1550592.00      6202368.0   \n",
       "3    Alabama  2017        4877989.0     1568279.25      6273117.0   \n",
       "4    Alabama  2018        4891628.0     1591449.75      6365799.0   \n",
       "..       ...   ...              ...            ...            ...   \n",
       "534  Wyoming  2020         570864.0      197677.50       790710.0   \n",
       "535  Wyoming  2021         572889.0      201160.75       804643.0   \n",
       "536  Wyoming  2022         575106.0      207544.50       830178.0   \n",
       "537  Wyoming  2023         578239.0      212949.25       851797.0   \n",
       "538  Wyoming  2024         580752.0      214311.50       857246.0   \n",
       "\n",
       "     state_hires_total  state_avg_earnings  state_avg_earnings_meanq  \\\n",
       "0            1096661.0         3228.483924               3228.168019   \n",
       "1            1161013.0         3296.603814               3296.053334   \n",
       "2            1216125.0         3333.740663               3333.279657   \n",
       "3            1256587.0         3396.834061               3396.809367   \n",
       "4            1343611.0         3499.525725               3499.332000   \n",
       "..                 ...                 ...                       ...   \n",
       "534           184565.0         3836.381008               3834.619197   \n",
       "535           201699.0         3986.156927               3981.600173   \n",
       "536           206479.0         4295.698368               4292.581068   \n",
       "537           198793.0         4510.154996               4510.339141   \n",
       "538           189504.0         4666.200252               4665.226123   \n",
       "\n",
       "     econ_emp_per_1k  econ_hires_per_1k  ...  \\\n",
       "0         309.331101         226.408040  ...   \n",
       "1         314.294730         239.147294  ...   \n",
       "2         318.604494         249.880620  ...   \n",
       "3         321.501186         257.603492  ...   \n",
       "4         325.341533         274.675629  ...   \n",
       "..               ...                ...  ...   \n",
       "534       346.277747         323.308178  ...   \n",
       "535       351.133902         352.073438  ...   \n",
       "536       360.880429         359.027727  ...   \n",
       "537       368.272029         343.790370  ...   \n",
       "538       369.024127         326.307959  ...   \n",
       "\n",
       "     STATE_% With Access to Exercise Opportunities  \\\n",
       "0                                        51.935239   \n",
       "1                                        64.355654   \n",
       "2                                        63.141529   \n",
       "3                                         0.000000   \n",
       "4                                        63.172629   \n",
       "..                                             ...   \n",
       "534                                      76.782077   \n",
       "535                                      76.801626   \n",
       "536                                      73.810856   \n",
       "537                                      77.638870   \n",
       "538                                      77.761949   \n",
       "\n",
       "     STATE_Food Environment Index  STATE_Income Ratio  \\\n",
       "0                        6.932836            0.000000   \n",
       "1                        6.694086            5.030240   \n",
       "2                        6.630406            5.064997   \n",
       "3                        6.505342            5.082714   \n",
       "4                        6.945420            5.107864   \n",
       "..                            ...                 ...   \n",
       "534                      7.687710            4.166759   \n",
       "535                      7.649759            4.126810   \n",
       "536                      7.765695            4.213440   \n",
       "537                      7.779140            4.235807   \n",
       "538                      7.781328            4.259097   \n",
       "\n",
       "     STATE_Mentally Unhealthy Days  STATE_Physically Unhealthy Days  \\\n",
       "0                         4.254047                         4.289236   \n",
       "1                         4.252503                         4.285823   \n",
       "2                         4.613115                         4.642889   \n",
       "3                         4.346475                         4.317696   \n",
       "4                         4.469942                         4.378239   \n",
       "..                             ...                              ...   \n",
       "534                       3.515226                         3.366822   \n",
       "535                       3.959356                         3.694561   \n",
       "536                       4.001026                         3.506198   \n",
       "537                       4.019662                         2.849604   \n",
       "538                       4.750646                         3.437344   \n",
       "\n",
       "     STATE_Preventable Hospitalization Rate  \\\n",
       "0                                  0.000000   \n",
       "1                                 69.088952   \n",
       "2                                 63.233968   \n",
       "3                                 59.030987   \n",
       "4                                 59.751012   \n",
       "..                                      ...   \n",
       "534                             3369.450393   \n",
       "535                             3378.711441   \n",
       "536                             3096.947537   \n",
       "537                             2324.557970   \n",
       "538                             2178.144266   \n",
       "\n",
       "     STATE_Primary Care Physicians Rate  STATE_Social Association Rate  \\\n",
       "0                             62.132991                       0.000000   \n",
       "1                             62.883135                      12.483551   \n",
       "2                             63.920448                      12.495976   \n",
       "3                             64.602109                      12.411106   \n",
       "4                             65.327281                      12.249050   \n",
       "..                                  ...                            ...   \n",
       "534                           72.041549                      11.824352   \n",
       "535                           68.743140                      12.110035   \n",
       "536                           72.137204                      12.021146   \n",
       "537                           70.486996                      11.973542   \n",
       "538                           70.319097                      12.253024   \n",
       "\n",
       "     STATE_Teen Birth Rate  STATE_Violent Crime Rate  \n",
       "0                48.186843                410.745302  \n",
       "1                46.968524                410.093125  \n",
       "2                44.568666                409.633102  \n",
       "3                41.759701                430.881423  \n",
       "4                35.987034                430.315343  \n",
       "..                     ...                       ...  \n",
       "534              28.210169                187.296452  \n",
       "535              25.969930                186.751076  \n",
       "536              24.283107                186.710976  \n",
       "537              24.269712                  0.000000  \n",
       "538              20.177949                  0.000000  \n",
       "\n",
       "[539 rows x 40 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_df_annual_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4b29a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df_annual_final.to_csv(\"CH_ECON_V4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fd4bb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated DataFrame Shape: (2151, 39)\n",
      "Sample Columns: ['State', 'Year', 'quarter', 'state_total_pop', 'state_emp_total', 'state_hires_total', 'state_avg_earnings', 'econ_emp_per_1k', 'econ_hires_per_1k', 'econ_hire_rate']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# SETUP: Define your column lists carefully\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. Variables that are RAW COUNTS -> We will SUM these\n",
    "# Note: I included 'Premature Deaths' here assuming it is a count. \n",
    "# If it is a rate (YPLL Rate), move it to the rate_cols list.\n",
    "health_count_cols = [\n",
    "    '# Alcohol-Impaired Driving Deaths', \n",
    "    '# Driving Deaths',\n",
    "    'Premature Deaths' \n",
    "]\n",
    "\n",
    "# 2. Variables that are RATES / INDICES / PERCENTS -> We will WEIGHTED AVERAGE these\n",
    "health_rate_cols = [\n",
    "    '% Adults with Obesity', \n",
    "    '% Children in Poverty',\n",
    "    '% Children in Single-Parent Households', \n",
    "    '% Drive Alone to Work',\n",
    "    '% Excessive Drinking', \n",
    "    '% Fair or Poor Health',\n",
    "    '% Long Commute - Drives Alone', \n",
    "    '% Low Birthweight',\n",
    "    '% Severe Housing Problems', \n",
    "    '% Smokers', \n",
    "    '% Some College',\n",
    "    '% Unemployed', \n",
    "    '% Uninsured',\n",
    "    '% With Access to Exercise Opportunities', \n",
    "    'Food Environment Index',\n",
    "    'Income Ratio', \n",
    "    'Mentally Unhealthy Days', \n",
    "    'Physically Unhealthy Days',\n",
    "    'Preventable Hospitalization Rate',\n",
    "    'Primary Care Physicians Rate', \n",
    "    'Social Association Rate',\n",
    "    'Teen Birth Rate', \n",
    "    'Violent Crime Rate'\n",
    "]\n",
    "\n",
    "# 3. Numeric columns to coerce (Standard housekeeping)\n",
    "numeric_cols = [\n",
    "    'Year', 'quarter', 'FIPS', 'Population',\n",
    "    'Employment_Count', 'New_Hires', 'Earnings_Beginning_Qtr'\n",
    "] + health_count_cols + health_rate_cols\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 0: PRE-PROCESSING\n",
    "# ---------------------------------------------------------\n",
    "df = merged_full.copy()\n",
    "\n",
    "# Coerce numeric types\n",
    "for c in numeric_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "# Filter Year Window\n",
    "df = df[(df['Year'] >= 2014) & (df['Year'] <= 2024)].copy()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 1: HEALTH DATA AGGREGATION (Annual)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Create a clean County-Year dataset\n",
    "# We drop duplicates because the original df has many rows (industries) per county\n",
    "county_health = df[['State', 'Year', 'FIPS', 'Population'] + health_count_cols + health_rate_cols].drop_duplicates()\n",
    "\n",
    "# --- A. Handle Weighted Averages (Rates) ---\n",
    "# Create numerators: (Rate * Population)\n",
    "for col in health_rate_cols:\n",
    "    county_health[f'{col}_numerator'] = county_health[col] * county_health['Population']\n",
    "\n",
    "# Define aggregation dictionary\n",
    "agg_dict = {\n",
    "    'state_total_pop': ('Population', 'sum'),\n",
    "}\n",
    "\n",
    "# Add Sum logic for Count columns\n",
    "for col in health_count_cols:\n",
    "    agg_dict[f'STATE_{col}'] = (col, 'sum')\n",
    "\n",
    "# Add Sum logic for Rate Numerators\n",
    "for col in health_rate_cols:\n",
    "    agg_dict[f'{col}_num_sum'] = (f'{col}_numerator', 'sum')\n",
    "\n",
    "# perform GroupBy\n",
    "state_health = county_health.groupby(['State', 'Year'], as_index=False).agg(**agg_dict)\n",
    "\n",
    "# Calculate Final Weighted Averages for Rates\n",
    "for col in health_rate_cols:\n",
    "    # Sum of (Rate*Pop) / Total Pop\n",
    "    state_health[f'STATE_{col}'] = state_health[f'{col}_num_sum'] / state_health['state_total_pop']\n",
    "    # Drop the temporary numerator column to keep it clean\n",
    "    state_health.drop(columns=[f'{col}_num_sum'], inplace=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 2: ECONOMIC DATA AGGREGATION (Quarterly)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# CRITICAL: Filter out Industry '00' to avoid double counting\n",
    "# We sum the specific industries to get the State Total\n",
    "df_econ = df[df['industry'] != '00'].copy()\n",
    "\n",
    "# Weight Earnings by Employment (because it's an average per person)\n",
    "df_econ['wage_bill'] = df_econ['Earnings_Beginning_Qtr'] * df_econ['Employment_Count']\n",
    "\n",
    "state_qtr = (\n",
    "    df_econ.groupby(['State', 'Year', 'quarter'], as_index=False)\n",
    "    .agg(\n",
    "        state_emp_total=('Employment_Count', 'sum'),\n",
    "        state_hires_total=('New_Hires', 'sum'),\n",
    "        state_wage_bill_total=('wage_bill', 'sum')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Recover State Average Earnings\n",
    "state_qtr['state_avg_earnings'] = state_qtr['state_wage_bill_total'] / state_qtr['state_emp_total']\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 3: MERGE & FEATURE ENGINEERING\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Merge Annual Health into Quarterly Econ\n",
    "state_df = state_qtr.merge(state_health, on=['State', 'Year'], how='left')\n",
    "\n",
    "# Helper variables\n",
    "pop = state_df['state_total_pop'].replace({0: np.nan})\n",
    "emp = state_df['state_emp_total'].replace({0: np.nan})\n",
    "\n",
    "# Per Capita Econ Metrics\n",
    "state_df['econ_emp_per_1k'] = 1000 * state_df['state_emp_total'] / pop\n",
    "state_df['econ_hires_per_1k'] = 1000 * state_df['state_hires_total'] / pop\n",
    "state_df['econ_hire_rate'] = state_df['state_hires_total'] / emp\n",
    "\n",
    "# Growth Rates (Lagged features)\n",
    "state_df = state_df.sort_values(['State', 'Year', 'quarter'])\n",
    "\n",
    "def calc_growth(series, lag):\n",
    "    prev = series.shift(lag)\n",
    "    den = prev.replace({0: np.nan})\n",
    "    return (series - prev) / den\n",
    "\n",
    "# QoQ Growth\n",
    "state_df['growth_emp_qoq'] = state_df.groupby('State')['state_emp_total'].transform(lambda x: calc_growth(x, 1))\n",
    "state_df['growth_earn_qoq'] = state_df.groupby('State')['state_avg_earnings'].transform(lambda x: calc_growth(x, 1))\n",
    "\n",
    "# YoY Growth (Seasonality adjustment)\n",
    "state_df['growth_emp_yoy'] = state_df.groupby('State')['state_emp_total'].transform(lambda x: calc_growth(x, 4))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 4: CLEANUP & FINAL OUTPUT\n",
    "# ---------------------------------------------------------\n",
    "state_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Organize columns clearly\n",
    "final_cols = [\n",
    "    'State', 'Year', 'quarter', 'state_total_pop',\n",
    "    # Econ\n",
    "    'state_emp_total', 'state_hires_total', 'state_avg_earnings',\n",
    "    'econ_emp_per_1k', 'econ_hires_per_1k', 'econ_hire_rate',\n",
    "    'growth_emp_qoq', 'growth_earn_qoq', 'growth_emp_yoy'\n",
    "] \n",
    "# Add all the State Health Columns (Counts and Rates)\n",
    "# (They are already named STATE_... in the dataframe)\n",
    "health_final_cols = [c for c in state_df.columns if c.startswith('STATE_')]\n",
    "final_cols = final_cols + health_final_cols\n",
    "\n",
    "# Final Selection\n",
    "state_df_final = state_df[final_cols]\n",
    "\n",
    "print(f\"Aggregated DataFrame Shape: {state_df_final.shape}\")\n",
    "print(\"Sample Columns:\", state_df_final.columns.tolist()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b4bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e61eb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>state_total_pop</th>\n",
       "      <th>state_emp_total</th>\n",
       "      <th>state_hires_total</th>\n",
       "      <th>state_avg_earnings</th>\n",
       "      <th>econ_emp_per_1k</th>\n",
       "      <th>econ_hires_per_1k</th>\n",
       "      <th>econ_hire_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>STATE_% With Access to Exercise Opportunities</th>\n",
       "      <th>STATE_Food Environment Index</th>\n",
       "      <th>STATE_Income Ratio</th>\n",
       "      <th>STATE_Mentally Unhealthy Days</th>\n",
       "      <th>STATE_Physically Unhealthy Days</th>\n",
       "      <th>STATE_Preventable Hospitalization Rate</th>\n",
       "      <th>STATE_Primary Care Physicians Rate</th>\n",
       "      <th>STATE_Social Association Rate</th>\n",
       "      <th>STATE_Teen Birth Rate</th>\n",
       "      <th>STATE_Violent Crime Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>4843737.0</td>\n",
       "      <td>1484428.0</td>\n",
       "      <td>234866.0</td>\n",
       "      <td>3183.492990</td>\n",
       "      <td>306.463377</td>\n",
       "      <td>48.488595</td>\n",
       "      <td>0.158220</td>\n",
       "      <td>...</td>\n",
       "      <td>51.935239</td>\n",
       "      <td>6.932836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.254047</td>\n",
       "      <td>4.289236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.132991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.186843</td>\n",
       "      <td>410.745302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>4843737.0</td>\n",
       "      <td>1488384.0</td>\n",
       "      <td>296168.0</td>\n",
       "      <td>3186.477239</td>\n",
       "      <td>307.280102</td>\n",
       "      <td>61.144525</td>\n",
       "      <td>0.198986</td>\n",
       "      <td>...</td>\n",
       "      <td>51.935239</td>\n",
       "      <td>6.932836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.254047</td>\n",
       "      <td>4.289236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.132991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.186843</td>\n",
       "      <td>410.745302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>4843737.0</td>\n",
       "      <td>1510987.0</td>\n",
       "      <td>292385.0</td>\n",
       "      <td>3158.744593</td>\n",
       "      <td>311.946540</td>\n",
       "      <td>60.363517</td>\n",
       "      <td>0.193506</td>\n",
       "      <td>...</td>\n",
       "      <td>51.935239</td>\n",
       "      <td>6.932836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.254047</td>\n",
       "      <td>4.289236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.132991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.186843</td>\n",
       "      <td>410.745302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>4843737.0</td>\n",
       "      <td>1509475.0</td>\n",
       "      <td>273242.0</td>\n",
       "      <td>3383.957254</td>\n",
       "      <td>311.634385</td>\n",
       "      <td>56.411403</td>\n",
       "      <td>0.181018</td>\n",
       "      <td>...</td>\n",
       "      <td>51.935239</td>\n",
       "      <td>6.932836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.254047</td>\n",
       "      <td>4.289236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.132991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.186843</td>\n",
       "      <td>410.745302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>4854803.0</td>\n",
       "      <td>1510048.0</td>\n",
       "      <td>246062.0</td>\n",
       "      <td>3224.297608</td>\n",
       "      <td>311.042075</td>\n",
       "      <td>50.684240</td>\n",
       "      <td>0.162950</td>\n",
       "      <td>...</td>\n",
       "      <td>64.355654</td>\n",
       "      <td>6.694086</td>\n",
       "      <td>5.030240</td>\n",
       "      <td>4.252503</td>\n",
       "      <td>4.285823</td>\n",
       "      <td>69.088952</td>\n",
       "      <td>62.883135</td>\n",
       "      <td>12.483551</td>\n",
       "      <td>46.968524</td>\n",
       "      <td>410.093125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>578239.0</td>\n",
       "      <td>215710.0</td>\n",
       "      <td>45344.0</td>\n",
       "      <td>4717.370878</td>\n",
       "      <td>373.046439</td>\n",
       "      <td>78.417402</td>\n",
       "      <td>0.210208</td>\n",
       "      <td>...</td>\n",
       "      <td>77.638870</td>\n",
       "      <td>7.779140</td>\n",
       "      <td>4.235807</td>\n",
       "      <td>4.019662</td>\n",
       "      <td>2.849604</td>\n",
       "      <td>2324.557970</td>\n",
       "      <td>70.486996</td>\n",
       "      <td>11.973542</td>\n",
       "      <td>24.269712</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>580752.0</td>\n",
       "      <td>208388.0</td>\n",
       "      <td>36781.0</td>\n",
       "      <td>4665.328800</td>\n",
       "      <td>358.824421</td>\n",
       "      <td>63.333402</td>\n",
       "      <td>0.176502</td>\n",
       "      <td>...</td>\n",
       "      <td>77.761949</td>\n",
       "      <td>7.781328</td>\n",
       "      <td>4.259097</td>\n",
       "      <td>4.750646</td>\n",
       "      <td>3.437344</td>\n",
       "      <td>2178.144266</td>\n",
       "      <td>70.319097</td>\n",
       "      <td>12.253024</td>\n",
       "      <td>20.177949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>580752.0</td>\n",
       "      <td>208453.0</td>\n",
       "      <td>59642.0</td>\n",
       "      <td>4530.285609</td>\n",
       "      <td>358.936345</td>\n",
       "      <td>102.697881</td>\n",
       "      <td>0.286117</td>\n",
       "      <td>...</td>\n",
       "      <td>77.761949</td>\n",
       "      <td>7.781328</td>\n",
       "      <td>4.259097</td>\n",
       "      <td>4.750646</td>\n",
       "      <td>3.437344</td>\n",
       "      <td>2178.144266</td>\n",
       "      <td>70.319097</td>\n",
       "      <td>12.253024</td>\n",
       "      <td>20.177949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>580752.0</td>\n",
       "      <td>222565.0</td>\n",
       "      <td>51118.0</td>\n",
       "      <td>4574.083715</td>\n",
       "      <td>383.235873</td>\n",
       "      <td>88.020360</td>\n",
       "      <td>0.229677</td>\n",
       "      <td>...</td>\n",
       "      <td>77.761949</td>\n",
       "      <td>7.781328</td>\n",
       "      <td>4.259097</td>\n",
       "      <td>4.750646</td>\n",
       "      <td>3.437344</td>\n",
       "      <td>2178.144266</td>\n",
       "      <td>70.319097</td>\n",
       "      <td>12.253024</td>\n",
       "      <td>20.177949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2024</td>\n",
       "      <td>4</td>\n",
       "      <td>580752.0</td>\n",
       "      <td>217840.0</td>\n",
       "      <td>41963.0</td>\n",
       "      <td>4891.206367</td>\n",
       "      <td>375.099871</td>\n",
       "      <td>72.256316</td>\n",
       "      <td>0.192632</td>\n",
       "      <td>...</td>\n",
       "      <td>77.761949</td>\n",
       "      <td>7.781328</td>\n",
       "      <td>4.259097</td>\n",
       "      <td>4.750646</td>\n",
       "      <td>3.437344</td>\n",
       "      <td>2178.144266</td>\n",
       "      <td>70.319097</td>\n",
       "      <td>12.253024</td>\n",
       "      <td>20.177949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2151 rows  39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        State  Year  quarter  state_total_pop  state_emp_total  \\\n",
       "0     Alabama  2014        1        4843737.0        1484428.0   \n",
       "1     Alabama  2014        2        4843737.0        1488384.0   \n",
       "2     Alabama  2014        3        4843737.0        1510987.0   \n",
       "3     Alabama  2014        4        4843737.0        1509475.0   \n",
       "4     Alabama  2015        1        4854803.0        1510048.0   \n",
       "...       ...   ...      ...              ...              ...   \n",
       "2146  Wyoming  2023        4         578239.0         215710.0   \n",
       "2147  Wyoming  2024        1         580752.0         208388.0   \n",
       "2148  Wyoming  2024        2         580752.0         208453.0   \n",
       "2149  Wyoming  2024        3         580752.0         222565.0   \n",
       "2150  Wyoming  2024        4         580752.0         217840.0   \n",
       "\n",
       "      state_hires_total  state_avg_earnings  econ_emp_per_1k  \\\n",
       "0              234866.0         3183.492990       306.463377   \n",
       "1              296168.0         3186.477239       307.280102   \n",
       "2              292385.0         3158.744593       311.946540   \n",
       "3              273242.0         3383.957254       311.634385   \n",
       "4              246062.0         3224.297608       311.042075   \n",
       "...                 ...                 ...              ...   \n",
       "2146            45344.0         4717.370878       373.046439   \n",
       "2147            36781.0         4665.328800       358.824421   \n",
       "2148            59642.0         4530.285609       358.936345   \n",
       "2149            51118.0         4574.083715       383.235873   \n",
       "2150            41963.0         4891.206367       375.099871   \n",
       "\n",
       "      econ_hires_per_1k  econ_hire_rate  ...  \\\n",
       "0             48.488595        0.158220  ...   \n",
       "1             61.144525        0.198986  ...   \n",
       "2             60.363517        0.193506  ...   \n",
       "3             56.411403        0.181018  ...   \n",
       "4             50.684240        0.162950  ...   \n",
       "...                 ...             ...  ...   \n",
       "2146          78.417402        0.210208  ...   \n",
       "2147          63.333402        0.176502  ...   \n",
       "2148         102.697881        0.286117  ...   \n",
       "2149          88.020360        0.229677  ...   \n",
       "2150          72.256316        0.192632  ...   \n",
       "\n",
       "      STATE_% With Access to Exercise Opportunities  \\\n",
       "0                                         51.935239   \n",
       "1                                         51.935239   \n",
       "2                                         51.935239   \n",
       "3                                         51.935239   \n",
       "4                                         64.355654   \n",
       "...                                             ...   \n",
       "2146                                      77.638870   \n",
       "2147                                      77.761949   \n",
       "2148                                      77.761949   \n",
       "2149                                      77.761949   \n",
       "2150                                      77.761949   \n",
       "\n",
       "      STATE_Food Environment Index  STATE_Income Ratio  \\\n",
       "0                         6.932836            0.000000   \n",
       "1                         6.932836            0.000000   \n",
       "2                         6.932836            0.000000   \n",
       "3                         6.932836            0.000000   \n",
       "4                         6.694086            5.030240   \n",
       "...                            ...                 ...   \n",
       "2146                      7.779140            4.235807   \n",
       "2147                      7.781328            4.259097   \n",
       "2148                      7.781328            4.259097   \n",
       "2149                      7.781328            4.259097   \n",
       "2150                      7.781328            4.259097   \n",
       "\n",
       "      STATE_Mentally Unhealthy Days  STATE_Physically Unhealthy Days  \\\n",
       "0                          4.254047                         4.289236   \n",
       "1                          4.254047                         4.289236   \n",
       "2                          4.254047                         4.289236   \n",
       "3                          4.254047                         4.289236   \n",
       "4                          4.252503                         4.285823   \n",
       "...                             ...                              ...   \n",
       "2146                       4.019662                         2.849604   \n",
       "2147                       4.750646                         3.437344   \n",
       "2148                       4.750646                         3.437344   \n",
       "2149                       4.750646                         3.437344   \n",
       "2150                       4.750646                         3.437344   \n",
       "\n",
       "      STATE_Preventable Hospitalization Rate  \\\n",
       "0                                   0.000000   \n",
       "1                                   0.000000   \n",
       "2                                   0.000000   \n",
       "3                                   0.000000   \n",
       "4                                  69.088952   \n",
       "...                                      ...   \n",
       "2146                             2324.557970   \n",
       "2147                             2178.144266   \n",
       "2148                             2178.144266   \n",
       "2149                             2178.144266   \n",
       "2150                             2178.144266   \n",
       "\n",
       "      STATE_Primary Care Physicians Rate  STATE_Social Association Rate  \\\n",
       "0                              62.132991                       0.000000   \n",
       "1                              62.132991                       0.000000   \n",
       "2                              62.132991                       0.000000   \n",
       "3                              62.132991                       0.000000   \n",
       "4                              62.883135                      12.483551   \n",
       "...                                  ...                            ...   \n",
       "2146                           70.486996                      11.973542   \n",
       "2147                           70.319097                      12.253024   \n",
       "2148                           70.319097                      12.253024   \n",
       "2149                           70.319097                      12.253024   \n",
       "2150                           70.319097                      12.253024   \n",
       "\n",
       "      STATE_Teen Birth Rate  STATE_Violent Crime Rate  \n",
       "0                 48.186843                410.745302  \n",
       "1                 48.186843                410.745302  \n",
       "2                 48.186843                410.745302  \n",
       "3                 48.186843                410.745302  \n",
       "4                 46.968524                410.093125  \n",
       "...                     ...                       ...  \n",
       "2146              24.269712                  0.000000  \n",
       "2147              20.177949                  0.000000  \n",
       "2148              20.177949                  0.000000  \n",
       "2149              20.177949                  0.000000  \n",
       "2150              20.177949                  0.000000  \n",
       "\n",
       "[2151 rows x 39 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73afd9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r\"C:\\Users\\liuc\\Downloads\\CH_ECON_V3.csv\"  # change if needed\n",
    "#state_df_final.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d9bf732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cadee8a2fc14a8d88df2d5bc2ed29b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Variable:', index=26, layout=Layout(width='450px'), options=('STATE_# Alc"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040f5b55929644f6aa8916b78b0d932a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================\n",
    "# US State Choropleth (Using state_df_final)\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from ipywidgets import widgets, interactive_output\n",
    "from IPython.display import display\n",
    "\n",
    "# 1. Prepare the Data\n",
    "# --------------------------------------------\n",
    "# We group by State/Year and take the MEAN.\n",
    "# - For Health data (Annual): The value is constant across quarters, so Mean returns the value.\n",
    "# - For Econ data (Quarterly): This gives us the \"Average Quarterly\" level for that year.\n",
    "df_map = state_df_final.groupby(['State', 'Year'], as_index=False).mean(numeric_only=True)\n",
    "\n",
    "# 2. Map State Names to USPS Codes\n",
    "# --------------------------------------------\n",
    "state_to_code = {\n",
    "    'Alabama':'AL','Alaska':'AK','Arizona':'AZ','Arkansas':'AR','California':'CA','Colorado':'CO',\n",
    "    'Connecticut':'CT','Delaware':'DE','District of Columbia':'DC','Florida':'FL','Georgia':'GA',\n",
    "    'Hawaii':'HI','Idaho':'ID','Illinois':'IL','Indiana':'IN','Iowa':'IA','Kansas':'KS','Kentucky':'KY',\n",
    "    'Louisiana':'LA','Maine':'ME','Maryland':'MD','Massachusetts':'MA','Michigan':'MI','Minnesota':'MN',\n",
    "    'Mississippi':'MS','Missouri':'MO','Montana':'MT','Nebraska':'NE','Nevada':'NV','New Hampshire':'NH',\n",
    "    'New Jersey':'NJ','New Mexico':'NM','New York':'NY','North Carolina':'NC','North Dakota':'ND',\n",
    "    'Ohio':'OH','Oklahoma':'OK','Oregon':'OR','Pennsylvania':'PA','Rhode Island':'RI','South Carolina':'SC',\n",
    "    'South Dakota':'SD','Tennessee':'TN','Texas':'TX','Utah':'UT','Vermont':'VT','Virginia':'VA',\n",
    "    'Washington':'WA','West Virginia':'WV','Wisconsin':'WI','Wyoming':'WY'\n",
    "}\n",
    "\n",
    "df_map['code'] = df_map['State'].map(state_to_code)\n",
    "\n",
    "# 3. Define Columns for Dropdown\n",
    "# --------------------------------------------\n",
    "# We dynamically pull the columns available in your dataframe\n",
    "# to ensure the dropdown never breaks.\n",
    "exclude_cols = ['State', 'Year', 'quarter', 'code', 'state_total_pop']\n",
    "available_cols = [c for c in df_map.columns if c not in exclude_cols]\n",
    "available_cols.sort()\n",
    "\n",
    "# Set a smart default\n",
    "default_val = 'econ_emp_per_1k' if 'econ_emp_per_1k' in available_cols else available_cols[0]\n",
    "\n",
    "# 4. Widgets\n",
    "# --------------------------------------------\n",
    "var_dd = widgets.Dropdown(\n",
    "    options=available_cols,\n",
    "    value=default_val,\n",
    "    description='Variable:',\n",
    "    layout=widgets.Layout(width='450px')\n",
    ")\n",
    "\n",
    "yrs = sorted(df_map['Year'].unique())\n",
    "year_dd = widgets.Dropdown(\n",
    "    options=yrs,\n",
    "    value=yrs[-1], # Default to latest year\n",
    "    description='Year:',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "# 5. Custom Colorscale (Your Preferred Yellow-Gold)\n",
    "# --------------------------------------------\n",
    "ylw_scale = [\n",
    "    (0.00, \"#fffde7\"),\n",
    "    (0.33, \"#fff59d\"),\n",
    "    (0.66, \"#fdd835\"),\n",
    "    (1.00, \"#f9a825\")\n",
    "]\n",
    "\n",
    "# 6. Plot Function\n",
    "# --------------------------------------------\n",
    "def show_map(variable, year):\n",
    "    # Filter data for specific year\n",
    "    d = df_map[df_map['Year'] == year].copy()\n",
    "\n",
    "    # Dynamic Range calculation for better contrast\n",
    "    # (Avoids 0s or NaNs skewing the scale)\n",
    "    valid_values = d[variable].dropna()\n",
    "    if len(valid_values) > 0:\n",
    "        low = valid_values.quantile(0.05)\n",
    "        high = valid_values.quantile(0.95)\n",
    "        rc = [low, high]\n",
    "    else:\n",
    "        rc = None\n",
    "\n",
    "    fig = px.choropleth(\n",
    "        d,\n",
    "        locations=\"code\",\n",
    "        locationmode=\"USA-states\",\n",
    "        color=variable,\n",
    "        scope=\"usa\",\n",
    "        range_color=rc,\n",
    "        color_continuous_scale=ylw_scale,\n",
    "        hover_name=\"State\",\n",
    "        hover_data={'code':False, 'Year':True, variable:':.2f'},\n",
    "        labels={variable: variable.replace('_',' ').replace('STATE', '').title()}\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f\"US States  {variable.replace('_',' ').title()} ({year})\",\n",
    "            x=0.5,\n",
    "            xanchor='center'\n",
    "        ),\n",
    "        coloraxis_colorbar=dict(title=\"Value\"),\n",
    "        geo=dict(bgcolor='rgba(0,0,0,0)'), # Transparent geo background\n",
    "        width=1050,\n",
    "        height=600,\n",
    "        margin=dict(l=0,r=0,t=60,b=0)\n",
    "    )\n",
    "\n",
    "    fig.update_traces(marker_line_color=\"white\", marker_line_width=0.5)\n",
    "    fig.show()\n",
    "\n",
    "# 7. Display UI\n",
    "# --------------------------------------------\n",
    "ui = widgets.HBox([var_dd, year_dd])\n",
    "out = interactive_output(show_map, {'variable': var_dd, 'year': year_dd})\n",
    "\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a9471c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuc\\AppData\\Local\\Temp\\ipykernel_10888\\2192016559.py:33: DeprecationWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c21f7cc47442a3a7b2dc8f116fc064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='State:', layout=Layout(width='250px'), options=('Alabama',"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cd3162ba6f4b4da0e17e11be4c4e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Interactive State Trend Plot (20142024)\n",
    "# Using pre-aggregated 'state_df_final'\n",
    "# ========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import widgets, interactive_output\n",
    "from IPython.display import display\n",
    "\n",
    "# --------------------------------------\n",
    "# 1) Prepare Annual Data\n",
    "# --------------------------------------\n",
    "# We start with state_df_final (Quarterly) and collapse to Annual.\n",
    "# Logic:\n",
    "# - Health columns are constant for the year, so mean() returns the correct value.\n",
    "# - Econ columns are quarterly, so mean() gives the \"Average Quarterly Level\" for that year.\n",
    "df_trend = state_df_final.groupby(['State', 'Year'], as_index=False).mean(numeric_only=True)\n",
    "\n",
    "# Ensure full 2014-2024 range for every state (handling missing years)\n",
    "def _complete_years(g):\n",
    "    # Create reference index\n",
    "    all_years = pd.DataFrame({'Year': np.arange(2014, 2025)})\n",
    "    # Merge existing data onto it\n",
    "    g = all_years.merge(g, on='Year', how='left')\n",
    "    # Fill State name downwards and upwards\n",
    "    g['State'] = g['State'].ffill().bfill()\n",
    "    return g\n",
    "\n",
    "df_trend = (\n",
    "    df_trend.groupby('State', as_index=False, group_keys=False)\n",
    "    .apply(_complete_years)\n",
    "    .sort_values(['State', 'Year'])\n",
    ")\n",
    "\n",
    "# --------------------------------------\n",
    "# 2) Define Variables for Dropdowns\n",
    "# --------------------------------------\n",
    "exclude_cols = ['State', 'Year', 'quarter', 'state_total_pop']\n",
    "# Get all numeric columns except the excluded ones\n",
    "all_vars = sorted([c for c in df_trend.columns if c not in exclude_cols])\n",
    "\n",
    "state_options = sorted(df_trend['State'].dropna().unique().tolist())\n",
    "\n",
    "# --------------------------------------\n",
    "# 3) Widgets\n",
    "# --------------------------------------\n",
    "state_dd = widgets.Dropdown(\n",
    "    options=state_options,\n",
    "    value=state_options[0] if state_options else None,\n",
    "    description='State:',\n",
    "    layout=widgets.Layout(width='250px')\n",
    ")\n",
    "\n",
    "var1_dd = widgets.Dropdown(\n",
    "    options=all_vars,\n",
    "    value='econ_emp_per_1k' if 'econ_emp_per_1k' in all_vars else all_vars[0],\n",
    "    description='Variable 1:',\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "# Try to find a good default for Var 2 (e.g., Obesity)\n",
    "default_v2 = [v for v in all_vars if 'Obesity' in v]\n",
    "default_v2 = default_v2[0] if default_v2 else (all_vars[1] if len(all_vars) > 1 else all_vars[0])\n",
    "\n",
    "var2_dd = widgets.Dropdown(\n",
    "    options=all_vars,\n",
    "    value=default_v2,\n",
    "    description='Variable 2:',\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "normalize_cb = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Normalize (01)',\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "# --------------------------------------\n",
    "# 4) Plotting Function\n",
    "# --------------------------------------\n",
    "def plot_state_trends(state, var1, var2, normalize):\n",
    "    if not state or not var1 or not var2:\n",
    "        return\n",
    "\n",
    "    # Filter data\n",
    "    sub = df_trend[df_trend['State'] == state].copy()\n",
    "    \n",
    "    x = sub['Year']\n",
    "    y1 = sub[var1]\n",
    "    y2 = sub[var2]\n",
    "\n",
    "    # --- Normalization Logic ---\n",
    "    def _minmax(s):\n",
    "        mn, mx = s.min(), s.max()\n",
    "        if pd.isna(mn) or pd.isna(mx) or mx == mn:\n",
    "            return s\n",
    "        return (s - mn) / (mx - mn)\n",
    "\n",
    "    if normalize:\n",
    "        y1_plot = _minmax(y1)\n",
    "        y2_plot = _minmax(y2)\n",
    "        y1_lbl = f\"{var1} (Scaled)\"\n",
    "        y2_lbl = f\"{var2} (Scaled)\"\n",
    "    else:\n",
    "        y1_plot, y2_plot = y1, y2\n",
    "        y1_lbl = var1\n",
    "        y2_lbl = var2\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    # Style 1\n",
    "    color1 = \"#1f77b4\" # Tab:Blue\n",
    "    line1 = ax1.plot(x, y1_plot, marker='o', linestyle='-', linewidth=2, color=color1, label=y1_lbl)\n",
    "    ax1.set_xlabel(\"Year\", fontsize=10)\n",
    "    ax1.set_ylabel(y1_lbl, color=color1, fontsize=10, fontweight='bold')\n",
    "    ax1.tick_params(axis='y', labelcolor=color1)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Style 2 (Twin Axis)\n",
    "    ax2 = ax1.twinx()\n",
    "    color2 = \"#ff7f0e\" # Tab:Orange\n",
    "    line2 = ax2.plot(x, y2_plot, marker='s', linestyle='--', linewidth=2, color=color2, label=y2_lbl)\n",
    "    ax2.set_ylabel(y2_lbl, color=color2, fontsize=10, fontweight='bold')\n",
    "    ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "    # Title\n",
    "    norm_txt = \" (Normalized Trend)\" if normalize else \"\"\n",
    "    plt.title(f\"{state}: {var1} vs. {var2}{norm_txt}\", fontsize=12)\n",
    "    plt.xticks(np.arange(2014, 2025, 1))\n",
    "\n",
    "    # Unified Legend\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax1.legend(lines, labels, loc='upper left', frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --------------------------------------\n",
    "# 5) Display\n",
    "# --------------------------------------\n",
    "ui = widgets.VBox([\n",
    "    widgets.HBox([state_dd, normalize_cb]),\n",
    "    widgets.HBox([var1_dd, var2_dd])\n",
    "])\n",
    "\n",
    "out = interactive_output(\n",
    "    plot_state_trends,\n",
    "    {'state': state_dd, 'var1': var1_dd, 'var2': var2_dd, 'normalize': normalize_cb}\n",
    ")\n",
    "\n",
    "display(ui, out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
